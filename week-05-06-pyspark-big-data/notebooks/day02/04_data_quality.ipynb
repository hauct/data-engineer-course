{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ DATA QUALITY & VALIDATION WITH PYSPARK\n",
    "\n",
    "---\n",
    "\n",
    "## üìã **OBJECTIVES**\n",
    "\n",
    "1. Define data quality rules\n",
    "2. Implement validation checks\n",
    "3. Create data quality metrics\n",
    "4. Build data quality dashboard\n",
    "5. Automated quality monitoring\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß **SETUP SPARK SESSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark Session Created\n",
      "Spark Version: 3.5.1\n",
      "Master: spark://spark-master:7077\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import builtins\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataQuality\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark Session Created\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä **1. CREATE TEST DATASET**\n",
    "\n",
    "T·∫°o dataset v·ªõi nhi·ªÅu data quality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä TEST DATASET:\n",
      "+--------+-----------+-----------------+---------+------------+--------------+--------------+\n",
      "|order_id|customer_id|email            |amount   |order_date  |status        |country       |\n",
      "+--------+-----------+-----------------+---------+------------+--------------+--------------+\n",
      "|ORD001  |CUST001    |john@email.com   |100.0    |2024-01-01  |completed     |USA           |\n",
      "|ORD002  |CUST002    |jane@email.com   |200.0    |2024-01-02  |completed     |UK            |\n",
      "|ORD003  |CUST003    |bob@email.com    |150.0    |2024-01-03  |completed     |Canada        |\n",
      "|ORD004  |NULL       |alice@email.com  |300.0    |2024-01-04  |pending       |USA           |\n",
      "|ORD005  |CUST005    |NULL             |250.0    |2024-01-05  |completed     |UK            |\n",
      "|ORD006  |CUST006    |charlie@email.com|NULL     |2024-01-06  |completed     |Canada        |\n",
      "|ORD007  |CUST007    |invalid-email    |400.0    |2024-01-07  |completed     |USA           |\n",
      "|ORD008  |CUST008    |david@email.com  |-100.0   |2024-01-08  |completed     |UK            |\n",
      "|ORD009  |CUST009    |eve@email.com    |500.0    |invalid-date|completed     |Canada        |\n",
      "|ORD010  |CUST010    |frank@email.com  |600.0    |2024-01-10  |invalid-status|USA           |\n",
      "|ORD001  |CUST001    |john@email.com   |100.0    |2024-01-01  |completed     |USA           |\n",
      "|ORD011  |CUST011    |grace@email.com  |1000000.0|2024-01-11  |completed     |UK            |\n",
      "|ORD012  |CUST012    |henry@email.com  |700.0    |2025-01-01  |completed     |Canada        |\n",
      "|ORD013  |CUST013    |ivy@email.com    |800.0    |2024-01-13  |completed     |InvalidCountry|\n",
      "|ORD014  |CUST014    |jack@email.com   |180.0    |2024-01-14  |completed     |USA           |\n",
      "|ORD015  |CUST015    |karen@email.com  |220.0    |2024-01-15  |pending       |UK            |\n",
      "+--------+-----------+-----------------+---------+------------+--------------+--------------+\n",
      "\n",
      "\n",
      "Total rows: 16\n"
     ]
    }
   ],
   "source": [
    "# Create test data with quality issues\n",
    "test_data = [\n",
    "    # Valid records\n",
    "    (\"ORD001\", \"CUST001\", \"john@email.com\", 100.0, \"2024-01-01\", \"completed\", \"USA\"),\n",
    "    (\"ORD002\", \"CUST002\", \"jane@email.com\", 200.0, \"2024-01-02\", \"completed\", \"UK\"),\n",
    "    (\"ORD003\", \"CUST003\", \"bob@email.com\", 150.0, \"2024-01-03\", \"completed\", \"Canada\"),\n",
    "    \n",
    "    # Missing values\n",
    "    (\"ORD004\", None, \"alice@email.com\", 300.0, \"2024-01-04\", \"pending\", \"USA\"),\n",
    "    (\"ORD005\", \"CUST005\", None, 250.0, \"2024-01-05\", \"completed\", \"UK\"),\n",
    "    (\"ORD006\", \"CUST006\", \"charlie@email.com\", None, \"2024-01-06\", \"completed\", \"Canada\"),\n",
    "    \n",
    "    # Invalid values\n",
    "    (\"ORD007\", \"CUST007\", \"invalid-email\", 400.0, \"2024-01-07\", \"completed\", \"USA\"),\n",
    "    (\"ORD008\", \"CUST008\", \"david@email.com\", -100.0, \"2024-01-08\", \"completed\", \"UK\"),\n",
    "    (\"ORD009\", \"CUST009\", \"eve@email.com\", 500.0, \"invalid-date\", \"completed\", \"Canada\"),\n",
    "    (\"ORD010\", \"CUST010\", \"frank@email.com\", 600.0, \"2024-01-10\", \"invalid-status\", \"USA\"),\n",
    "    \n",
    "    # Duplicates\n",
    "    (\"ORD001\", \"CUST001\", \"john@email.com\", 100.0, \"2024-01-01\", \"completed\", \"USA\"),\n",
    "    \n",
    "    # Outliers\n",
    "    (\"ORD011\", \"CUST011\", \"grace@email.com\", 1000000.0, \"2024-01-11\", \"completed\", \"UK\"),\n",
    "    \n",
    "    # Future dates\n",
    "    (\"ORD012\", \"CUST012\", \"henry@email.com\", 700.0, \"2025-01-01\", \"completed\", \"Canada\"),\n",
    "    \n",
    "    # Invalid country\n",
    "    (\"ORD013\", \"CUST013\", \"ivy@email.com\", 800.0, \"2024-01-13\", \"completed\", \"InvalidCountry\"),\n",
    "    \n",
    "    # More valid records\n",
    "    (\"ORD014\", \"CUST014\", \"jack@email.com\", 180.0, \"2024-01-14\", \"completed\", \"USA\"),\n",
    "    (\"ORD015\", \"CUST015\", \"karen@email.com\", 220.0, \"2024-01-15\", \"pending\", \"UK\"),\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(test_data, schema)\n",
    "\n",
    "print(\"üìä TEST DATASET:\")\n",
    "df.show(20, truncate=False)\n",
    "print(f\"\\nTotal rows: {df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ **2. DEFINE DATA QUALITY RULES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã DATA QUALITY RULES:\n",
      "\n",
      "COMPLETENESS:\n",
      "  ‚Ä¢ order_id: NOT NULL\n",
      "  ‚Ä¢ customer_id: NOT NULL\n",
      "  ‚Ä¢ email: NOT NULL\n",
      "  ‚Ä¢ amount: NOT NULL\n",
      "  ‚Ä¢ order_date: NOT NULL\n",
      "  ‚Ä¢ status: NOT NULL\n",
      "  ‚Ä¢ country: NOT NULL\n",
      "\n",
      "VALIDITY:\n",
      "  ‚Ä¢ email: REGEX: ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n",
      "  ‚Ä¢ amount: RANGE: 0 to 100000\n",
      "  ‚Ä¢ order_date: DATE FORMAT: yyyy-MM-dd\n",
      "  ‚Ä¢ status: IN: [pending, completed, cancelled]\n",
      "  ‚Ä¢ country: IN: [USA, UK, Canada]\n",
      "\n",
      "UNIQUENESS:\n",
      "  ‚Ä¢ order_id: UNIQUE\n",
      "\n",
      "TIMELINESS:\n",
      "  ‚Ä¢ order_date: NOT FUTURE DATE\n"
     ]
    }
   ],
   "source": [
    "# Define quality rules\n",
    "quality_rules = {\n",
    "    \"completeness\": {\n",
    "        \"order_id\": \"NOT NULL\",\n",
    "        \"customer_id\": \"NOT NULL\",\n",
    "        \"email\": \"NOT NULL\",\n",
    "        \"amount\": \"NOT NULL\",\n",
    "        \"order_date\": \"NOT NULL\",\n",
    "        \"status\": \"NOT NULL\",\n",
    "        \"country\": \"NOT NULL\"\n",
    "    },\n",
    "    \"validity\": {\n",
    "        \"email\": \"REGEX: ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\",\n",
    "        \"amount\": \"RANGE: 0 to 100000\",\n",
    "        \"order_date\": \"DATE FORMAT: yyyy-MM-dd\",\n",
    "        \"status\": \"IN: [pending, completed, cancelled]\",\n",
    "        \"country\": \"IN: [USA, UK, Canada]\"\n",
    "    },\n",
    "    \"uniqueness\": {\n",
    "        \"order_id\": \"UNIQUE\"\n",
    "    },\n",
    "    \"timeliness\": {\n",
    "        \"order_date\": \"NOT FUTURE DATE\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìã DATA QUALITY RULES:\")\n",
    "for category, rules in quality_rules.items():\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    for field, rule in rules.items():\n",
    "        print(f\"  ‚Ä¢ {field}: {rule}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ **3. COMPLETENESS CHECKS**\n",
    "\n",
    "Check for missing/null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ COMPLETENESS CHECK:\n",
      "+-----------+------------+----------+---------------+------+----------+\n",
      "|column     |completeness|null_count|null_percentage|status|total_rows|\n",
      "+-----------+------------+----------+---------------+------+----------+\n",
      "|order_id   |100.0       |0         |0.0            |‚úÖ PASS|16        |\n",
      "|customer_id|93.75       |1         |6.25           |‚ùå FAIL|16        |\n",
      "|email      |93.75       |1         |6.25           |‚ùå FAIL|16        |\n",
      "|amount     |93.75       |1         |6.25           |‚ùå FAIL|16        |\n",
      "|order_date |100.0       |0         |0.0            |‚úÖ PASS|16        |\n",
      "|status     |100.0       |0         |0.0            |‚úÖ PASS|16        |\n",
      "|country    |100.0       |0         |0.0            |‚úÖ PASS|16        |\n",
      "+-----------+------------+----------+---------------+------+----------+\n",
      "\n",
      "\n",
      "üìä Summary: 3 columns failed completeness check\n"
     ]
    }
   ],
   "source": [
    "def check_completeness(df):\n",
    "    \"\"\"\n",
    "    Check completeness (null values) for all columns\n",
    "    \"\"\"\n",
    "    total_rows = df.count()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for col_name in df.columns:\n",
    "        null_count = df.filter(col(col_name).isNull()).count()\n",
    "        null_percentage = (null_count / total_rows) * 100\n",
    "        completeness = 100 - null_percentage\n",
    "        \n",
    "        status = \"‚úÖ PASS\" if null_count == 0 else \"‚ùå FAIL\"\n",
    "        \n",
    "        results.append({\n",
    "            \"column\": col_name,\n",
    "            \"total_rows\": total_rows,\n",
    "            \"null_count\": null_count,\n",
    "            \"null_percentage\": __builtins__.round(null_percentage, 2),\n",
    "            \"completeness\": __builtins__.round(completeness, 2),\n",
    "            \"status\": status\n",
    "        })\n",
    "    \n",
    "    return spark.createDataFrame(results)\n",
    "\n",
    "# Run completeness check\n",
    "print(\"‚úÖ COMPLETENESS CHECK:\")\n",
    "completeness_report = check_completeness(df)\n",
    "completeness_report.show(truncate=False)\n",
    "\n",
    "# Summary\n",
    "failed_checks = completeness_report.filter(col(\"status\") == \"‚ùå FAIL\").count()\n",
    "print(f\"\\nüìä Summary: {failed_checks} columns failed completeness check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ **4. VALIDITY CHECKS**\n",
    "\n",
    "Check if values meet business rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìß EMAIL VALIDITY:\n",
      "Total: 16\n",
      "Valid: 14 (87.50%)\n",
      "Invalid: 2 (12.50%)\n",
      "\n",
      "‚ùå INVALID EMAILS:\n",
      "+--------+-------------+\n",
      "|order_id|email        |\n",
      "+--------+-------------+\n",
      "|ORD007  |invalid-email|\n",
      "+--------+-------------+\n",
      "\n",
      "\n",
      "üí∞ AMOUNT VALIDITY:\n",
      "Total: 16\n",
      "Valid: 13 (81.25%)\n",
      "Invalid: 3 (18.75%)\n",
      "\n",
      "‚ùå INVALID AMOUNTS:\n",
      "+--------+---------+\n",
      "|order_id|amount   |\n",
      "+--------+---------+\n",
      "|ORD008  |-100.0   |\n",
      "|ORD011  |1000000.0|\n",
      "+--------+---------+\n",
      "\n",
      "\n",
      "üìä STATUS VALIDITY:\n",
      "Total: 16\n",
      "Valid: 15 (93.75%)\n",
      "Invalid: 1 (6.25%)\n",
      "\n",
      "‚ùå INVALID STATUSES:\n",
      "+--------+--------------+\n",
      "|order_id|status        |\n",
      "+--------+--------------+\n",
      "|ORD010  |invalid-status|\n",
      "+--------+--------------+\n",
      "\n",
      "\n",
      "üåç COUNTRY VALIDITY:\n",
      "Total: 16\n",
      "Valid: 15 (93.75%)\n",
      "Invalid: 1 (6.25%)\n",
      "\n",
      "‚ùå INVALID COUNTRIES:\n",
      "+--------+--------------+\n",
      "|order_id|country       |\n",
      "+--------+--------------+\n",
      "|ORD013  |InvalidCountry|\n",
      "+--------+--------------+\n",
      "\n",
      "\n",
      "üìÖ DATE VALIDITY:\n",
      "Total: 16\n",
      "Valid: 15 (93.75%)\n",
      "Invalid: 1 (6.25%)\n",
      "\n",
      "‚ùå INVALID DATES:\n",
      "+--------+------------+\n",
      "|order_id|order_date  |\n",
      "+--------+------------+\n",
      "|ORD009  |invalid-date|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Email validation\n",
    "def check_email_validity(df):\n",
    "    email_regex = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
    "    \n",
    "    df_check = df.withColumn(\n",
    "        \"is_valid_email\",\n",
    "        col(\"email\").rlike(email_regex)\n",
    "    )\n",
    "    \n",
    "    total = df_check.count()\n",
    "    valid = df_check.filter(col(\"is_valid_email\") == True).count()\n",
    "    invalid = total - valid\n",
    "    \n",
    "    print(\"üìß EMAIL VALIDITY:\")\n",
    "    print(f\"Total: {total}\")\n",
    "    print(f\"Valid: {valid} ({(valid/total)*100:.2f}%)\")\n",
    "    print(f\"Invalid: {invalid} ({(invalid/total)*100:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n‚ùå INVALID EMAILS:\")\n",
    "    df_check.filter(col(\"is_valid_email\") == False).select(\"order_id\", \"email\").show(truncate=False)\n",
    "    \n",
    "    return df_check\n",
    "\n",
    "df_email_check = check_email_validity(df)\n",
    "\n",
    "# 4.2 Amount validation\n",
    "def check_amount_validity(df):\n",
    "    df_check = df.withColumn(\n",
    "        \"is_valid_amount\",\n",
    "        (col(\"amount\") >= 0) & (col(\"amount\") <= 100000)\n",
    "    )\n",
    "    \n",
    "    total = df_check.count()\n",
    "    valid = df_check.filter(col(\"is_valid_amount\") == True).count()\n",
    "    invalid = total - valid\n",
    "    \n",
    "    print(\"\\nüí∞ AMOUNT VALIDITY:\")\n",
    "    print(f\"Total: {total}\")\n",
    "    print(f\"Valid: {valid} ({(valid/total)*100:.2f}%)\")\n",
    "    print(f\"Invalid: {invalid} ({(invalid/total)*100:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n‚ùå INVALID AMOUNTS:\")\n",
    "    df_check.filter(col(\"is_valid_amount\") == False).select(\"order_id\", \"amount\").show(truncate=False)\n",
    "    \n",
    "    return df_check\n",
    "\n",
    "df_amount_check = check_amount_validity(df)\n",
    "\n",
    "# 4.3 Status validation\n",
    "def check_status_validity(df):\n",
    "    valid_statuses = [\"pending\", \"completed\", \"cancelled\"]\n",
    "    \n",
    "    df_check = df.withColumn(\n",
    "        \"is_valid_status\",\n",
    "        col(\"status\").isin(valid_statuses)\n",
    "    )\n",
    "    \n",
    "    total = df_check.count()\n",
    "    valid = df_check.filter(col(\"is_valid_status\") == True).count()\n",
    "    invalid = total - valid\n",
    "    \n",
    "    print(\"\\nüìä STATUS VALIDITY:\")\n",
    "    print(f\"Total: {total}\")\n",
    "    print(f\"Valid: {valid} ({(valid/total)*100:.2f}%)\")\n",
    "    print(f\"Invalid: {invalid} ({(invalid/total)*100:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n‚ùå INVALID STATUSES:\")\n",
    "    df_check.filter(col(\"is_valid_status\") == False).select(\"order_id\", \"status\").show(truncate=False)\n",
    "    \n",
    "    return df_check\n",
    "\n",
    "df_status_check = check_status_validity(df)\n",
    "\n",
    "# 4.4 Country validation\n",
    "def check_country_validity(df):\n",
    "    valid_countries = [\"USA\", \"UK\", \"Canada\"]\n",
    "    \n",
    "    df_check = df.withColumn(\n",
    "        \"is_valid_country\",\n",
    "        col(\"country\").isin(valid_countries)\n",
    "    )\n",
    "    \n",
    "    total = df_check.count()\n",
    "    valid = df_check.filter(col(\"is_valid_country\") == True).count()\n",
    "    invalid = total - valid\n",
    "    \n",
    "    print(\"\\nüåç COUNTRY VALIDITY:\")\n",
    "    print(f\"Total: {total}\")\n",
    "    print(f\"Valid: {valid} ({(valid/total)*100:.2f}%)\")\n",
    "    print(f\"Invalid: {invalid} ({(invalid/total)*100:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n‚ùå INVALID COUNTRIES:\")\n",
    "    df_check.filter(col(\"is_valid_country\") == False).select(\"order_id\", \"country\").show(truncate=False)\n",
    "    \n",
    "    return df_check\n",
    "\n",
    "df_country_check = check_country_validity(df)\n",
    "\n",
    "# 4.5 Date validation\n",
    "def check_date_validity(df):\n",
    "    df_check = df.withColumn(\n",
    "        \"parsed_date\",\n",
    "        to_date(col(\"order_date\"), \"yyyy-MM-dd\")\n",
    "    ).withColumn(\n",
    "        \"is_valid_date\",\n",
    "        col(\"parsed_date\").isNotNull()\n",
    "    )\n",
    "    \n",
    "    total = df_check.count()\n",
    "    valid = df_check.filter(col(\"is_valid_date\") == True).count()\n",
    "    invalid = total - valid\n",
    "    \n",
    "    print(\"\\nüìÖ DATE VALIDITY:\")\n",
    "    print(f\"Total: {total}\")\n",
    "    print(f\"Valid: {valid} ({(valid/total)*100:.2f}%)\")\n",
    "    print(f\"Invalid: {invalid} ({(invalid/total)*100:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n‚ùå INVALID DATES:\")\n",
    "    df_check.filter(col(\"is_valid_date\") == False).select(\"order_id\", \"order_date\").show(truncate=False)\n",
    "    \n",
    "    return df_check\n",
    "\n",
    "df_date_check = check_date_validity(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ **5. UNIQUENESS CHECKS**\n",
    "\n",
    "Check for duplicate records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë UNIQUENESS CHECK (order_id):\n",
      "Total rows: 16\n",
      "Distinct rows: 15\n",
      "Duplicates: 1\n",
      "Uniqueness: 93.75%\n",
      "\n",
      "‚ùå DUPLICATE RECORDS:\n",
      "+--------+-----------+--------------+------+----------+---------+-------+---------+\n",
      "|order_id|customer_id|email         |amount|order_date|status   |country|dup_count|\n",
      "+--------+-----------+--------------+------+----------+---------+-------+---------+\n",
      "|ORD001  |CUST001    |john@email.com|100.0 |2024-01-01|completed|USA    |2        |\n",
      "|ORD001  |CUST001    |john@email.com|100.0 |2024-01-01|completed|USA    |2        |\n",
      "+--------+-----------+--------------+------+----------+---------+-------+---------+\n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "üîë UNIQUENESS CHECK (customer_id, order_date):\n",
      "Total rows: 16\n",
      "Distinct rows: 15\n",
      "Duplicates: 1\n",
      "Uniqueness: 93.75%\n",
      "\n",
      "‚ùå DUPLICATE RECORDS:\n",
      "+--------+-----------+--------------+------+----------+---------+-------+---------+\n",
      "|order_id|customer_id|email         |amount|order_date|status   |country|dup_count|\n",
      "+--------+-----------+--------------+------+----------+---------+-------+---------+\n",
      "|ORD001  |CUST001    |john@email.com|100.0 |2024-01-01|completed|USA    |2        |\n",
      "|ORD001  |CUST001    |john@email.com|100.0 |2024-01-01|completed|USA    |2        |\n",
      "+--------+-----------+--------------+------+----------+---------+-------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_uniqueness(df, key_columns):\n",
    "    \"\"\"\n",
    "    Check uniqueness of key columns\n",
    "    \"\"\"\n",
    "    total_rows = df.count()\n",
    "    distinct_rows = df.select(key_columns).distinct().count()\n",
    "    duplicates = total_rows - distinct_rows\n",
    "    \n",
    "    print(f\"üîë UNIQUENESS CHECK ({', '.join(key_columns)}):\")\n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    print(f\"Distinct rows: {distinct_rows}\")\n",
    "    print(f\"Duplicates: {duplicates}\")\n",
    "    print(f\"Uniqueness: {(distinct_rows/total_rows)*100:.2f}%\")\n",
    "    \n",
    "    if duplicates > 0:\n",
    "        print(\"\\n‚ùå DUPLICATE RECORDS:\")\n",
    "        \n",
    "        # Find duplicates\n",
    "        windowSpec = Window.partitionBy(key_columns)\n",
    "        df_dup = df.withColumn(\"dup_count\", count(\"*\").over(windowSpec)) \\\n",
    "            .filter(col(\"dup_count\") > 1) \\\n",
    "            .orderBy(key_columns)\n",
    "        \n",
    "        df_dup.show(truncate=False)\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No duplicates found\")\n",
    "    \n",
    "    return duplicates\n",
    "\n",
    "# Check order_id uniqueness\n",
    "check_uniqueness(df, [\"order_id\"])\n",
    "\n",
    "# Check customer_id + order_date uniqueness\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "check_uniqueness(df, [\"customer_id\", \"order_date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ **6. TIMELINESS CHECKS**\n",
    "\n",
    "Check for future dates and data freshness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ TIMELINESS CHECK:\n",
      "Total: 16\n",
      "Future dates: 0 (0.00%)\n",
      "\n",
      "‚úÖ No future dates found\n"
     ]
    }
   ],
   "source": [
    "def check_timeliness(df):\n",
    "    \"\"\"\n",
    "    Check for future dates\n",
    "    \"\"\"\n",
    "    df_check = df.withColumn(\n",
    "        \"parsed_date\",\n",
    "        to_date(col(\"order_date\"), \"yyyy-MM-dd\")\n",
    "    ).withColumn(\n",
    "        \"is_future_date\",\n",
    "        col(\"parsed_date\") > current_date()\n",
    "    )\n",
    "    \n",
    "    total = df_check.count()\n",
    "    future_dates = df_check.filter(col(\"is_future_date\") == True).count()\n",
    "    \n",
    "    print(\"üìÖ TIMELINESS CHECK:\")\n",
    "    print(f\"Total: {total}\")\n",
    "    print(f\"Future dates: {future_dates} ({(future_dates/total)*100:.2f}%)\")\n",
    "    \n",
    "    if future_dates > 0:\n",
    "        print(\"\\n‚ùå FUTURE DATES FOUND:\")\n",
    "        df_check.filter(col(\"is_future_date\") == True) \\\n",
    "            .select(\"order_id\", \"order_date\", \"parsed_date\") \\\n",
    "            .show(truncate=False)\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No future dates found\")\n",
    "    \n",
    "    return df_check\n",
    "\n",
    "df_timeliness_check = check_timeliness(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ **7. CONSISTENCY CHECKS**\n",
    "\n",
    "Check for data consistency across columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó CONSISTENCY CHECK:\n",
      "Rule: Completed orders must have amount > 0\n",
      "Total: 16\n",
      "Inconsistent: 1 (6.25%)\n",
      "\n",
      "‚ùå INCONSISTENT RECORDS:\n",
      "+--------+---------+------+\n",
      "|order_id|status   |amount|\n",
      "+--------+---------+------+\n",
      "|ORD008  |completed|-100.0|\n",
      "+--------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_consistency(df):\n",
    "    \"\"\"\n",
    "    Check business logic consistency\n",
    "    \"\"\"\n",
    "    # Rule: Completed orders must have amount > 0\n",
    "    df_check = df.withColumn(\n",
    "        \"is_consistent\",\n",
    "        when(\n",
    "            (col(\"status\") == \"completed\") & (col(\"amount\") <= 0),\n",
    "            False\n",
    "        ).otherwise(True)\n",
    "    )\n",
    "    \n",
    "    total = df_check.count()\n",
    "    inconsistent = df_check.filter(col(\"is_consistent\") == False).count()\n",
    "    \n",
    "    print(\"üîó CONSISTENCY CHECK:\")\n",
    "    print(f\"Rule: Completed orders must have amount > 0\")\n",
    "    print(f\"Total: {total}\")\n",
    "    print(f\"Inconsistent: {inconsistent} ({(inconsistent/total)*100:.2f}%)\")\n",
    "    \n",
    "    if inconsistent > 0:\n",
    "        print(\"\\n‚ùå INCONSISTENT RECORDS:\")\n",
    "        df_check.filter(col(\"is_consistent\") == False) \\\n",
    "            .select(\"order_id\", \"status\", \"amount\") \\\n",
    "            .show(truncate=False)\n",
    "    else:\n",
    "        print(\"\\n‚úÖ All records are consistent\")\n",
    "    \n",
    "    return df_check\n",
    "\n",
    "df_consistency_check = check_consistency(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä **8. COMPREHENSIVE QUALITY REPORT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìä DATA QUALITY REPORT\n",
      "================================================================================\n",
      "\n",
      "üìà OVERALL METRICS:\n",
      "   Total Records: 16\n",
      "   Quality Pass: 7 (43.75%)\n",
      "   Quality Fail: 9 (56.25%)\n",
      "   Quality Score: 43.75%\n",
      "\n",
      "‚ùå QUALITY ISSUES BREAKDOWN:\n",
      "   Missing Values: 3 (18.75%)\n",
      "   Invalid Email: 1 (6.25%)\n",
      "   Invalid Amount: 2 (12.50%)\n",
      "   Invalid Status: 1 (6.25%)\n",
      "   Invalid Country: 1 (6.25%)\n",
      "   Invalid Date: 1 (6.25%)\n",
      "   Future Date: 0 (0.00%)\n",
      "\n",
      "üéØ QUALITY GRADE:\n",
      "   Grade: F (Fail)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚ùå RECORDS THAT FAILED QUALITY CHECKS:\n",
      "+--------+-----------+-----------------+---------+--------------+--------------+------------+\n",
      "|order_id|customer_id|email            |amount   |status        |country       |order_date  |\n",
      "+--------+-----------+-----------------+---------+--------------+--------------+------------+\n",
      "|ORD004  |NULL       |alice@email.com  |300.0    |pending       |USA           |2024-01-04  |\n",
      "|ORD005  |CUST005    |NULL             |250.0    |completed     |UK            |2024-01-05  |\n",
      "|ORD006  |CUST006    |charlie@email.com|NULL     |completed     |Canada        |2024-01-06  |\n",
      "|ORD007  |CUST007    |invalid-email    |400.0    |completed     |USA           |2024-01-07  |\n",
      "|ORD008  |CUST008    |david@email.com  |-100.0   |completed     |UK            |2024-01-08  |\n",
      "|ORD009  |CUST009    |eve@email.com    |500.0    |completed     |Canada        |invalid-date|\n",
      "|ORD010  |CUST010    |frank@email.com  |600.0    |invalid-status|USA           |2024-01-10  |\n",
      "|ORD011  |CUST011    |grace@email.com  |1000000.0|completed     |UK            |2024-01-11  |\n",
      "|ORD013  |CUST013    |ivy@email.com    |800.0    |completed     |InvalidCountry|2024-01-13  |\n",
      "+--------+-----------+-----------------+---------+--------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_quality_report(df):\n",
    "    \"\"\"\n",
    "    Generate comprehensive data quality report\n",
    "    \"\"\"\n",
    "    total_rows = df.count()\n",
    "    \n",
    "    # Add all validation flags\n",
    "    email_regex = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
    "    valid_statuses = [\"pending\", \"completed\", \"cancelled\"]\n",
    "    valid_countries = [\"USA\", \"UK\", \"Canada\"]\n",
    "    \n",
    "    df_report = df \\\n",
    "        .withColumn(\"has_null\", \n",
    "            col(\"order_id\").isNull() | \n",
    "            col(\"customer_id\").isNull() | \n",
    "            col(\"email\").isNull() | \n",
    "            col(\"amount\").isNull() | \n",
    "            col(\"order_date\").isNull() | \n",
    "            col(\"status\").isNull() | \n",
    "            col(\"country\").isNull()\n",
    "        ) \\\n",
    "        .withColumn(\"is_valid_email\", col(\"email\").rlike(email_regex)) \\\n",
    "        .withColumn(\"is_valid_amount\", (col(\"amount\") >= 0) & (col(\"amount\") <= 100000)) \\\n",
    "        .withColumn(\"is_valid_status\", col(\"status\").isin(valid_statuses)) \\\n",
    "        .withColumn(\"is_valid_country\", col(\"country\").isin(valid_countries)) \\\n",
    "        .withColumn(\"parsed_date\", to_date(col(\"order_date\"), \"yyyy-MM-dd\")) \\\n",
    "        .withColumn(\"is_valid_date\", col(\"parsed_date\").isNotNull()) \\\n",
    "        .withColumn(\"is_future_date\", col(\"parsed_date\") > current_date()) \\\n",
    "        .withColumn(\"is_quality_pass\",\n",
    "            ~col(\"has_null\") &\n",
    "            col(\"is_valid_email\") &\n",
    "            col(\"is_valid_amount\") &\n",
    "            col(\"is_valid_status\") &\n",
    "            col(\"is_valid_country\") &\n",
    "            col(\"is_valid_date\") &\n",
    "            ~col(\"is_future_date\")\n",
    "        )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    quality_pass = df_report.filter(col(\"is_quality_pass\") == True).count()\n",
    "    quality_fail = total_rows - quality_pass\n",
    "    quality_score = (quality_pass / total_rows) * 100\n",
    "    \n",
    "    # Detailed breakdown\n",
    "    has_null = df_report.filter(col(\"has_null\") == True).count()\n",
    "    invalid_email = df_report.filter(col(\"is_valid_email\") == False).count()\n",
    "    invalid_amount = df_report.filter(col(\"is_valid_amount\") == False).count()\n",
    "    invalid_status = df_report.filter(col(\"is_valid_status\") == False).count()\n",
    "    invalid_country = df_report.filter(col(\"is_valid_country\") == False).count()\n",
    "    invalid_date = df_report.filter(col(\"is_valid_date\") == False).count()\n",
    "    future_date = df_report.filter(col(\"is_future_date\") == True).count()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"üìä DATA QUALITY REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nüìà OVERALL METRICS:\")\n",
    "    print(f\"   Total Records: {total_rows}\")\n",
    "    print(f\"   Quality Pass: {quality_pass} ({(quality_pass/total_rows)*100:.2f}%)\")\n",
    "    print(f\"   Quality Fail: {quality_fail} ({(quality_fail/total_rows)*100:.2f}%)\")\n",
    "    print(f\"   Quality Score: {quality_score:.2f}%\")\n",
    "    \n",
    "    print(f\"\\n‚ùå QUALITY ISSUES BREAKDOWN:\")\n",
    "    print(f\"   Missing Values: {has_null} ({(has_null/total_rows)*100:.2f}%)\")\n",
    "    print(f\"   Invalid Email: {invalid_email} ({(invalid_email/total_rows)*100:.2f}%)\")\n",
    "    print(f\"   Invalid Amount: {invalid_amount} ({(invalid_amount/total_rows)*100:.2f}%)\")\n",
    "    print(f\"   Invalid Status: {invalid_status} ({(invalid_status/total_rows)*100:.2f}%)\")\n",
    "    print(f\"   Invalid Country: {invalid_country} ({(invalid_country/total_rows)*100:.2f}%)\")\n",
    "    print(f\"   Invalid Date: {invalid_date} ({(invalid_date/total_rows)*100:.2f}%)\")\n",
    "    print(f\"   Future Date: {future_date} ({(future_date/total_rows)*100:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nüéØ QUALITY GRADE:\")\n",
    "    if quality_score >= 95:\n",
    "        grade = \"A+ (Excellent)\"\n",
    "    elif quality_score >= 90:\n",
    "        grade = \"A (Very Good)\"\n",
    "    elif quality_score >= 80:\n",
    "        grade = \"B (Good)\"\n",
    "    elif quality_score >= 70:\n",
    "        grade = \"C (Fair)\"\n",
    "    elif quality_score >= 60:\n",
    "        grade = \"D (Poor)\"\n",
    "    else:\n",
    "        grade = \"F (Fail)\"\n",
    "    \n",
    "    print(f\"   Grade: {grade}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    return df_report, quality_score\n",
    "\n",
    "# Generate report\n",
    "df_with_quality, quality_score = generate_quality_report(df)\n",
    "\n",
    "# Show failed records\n",
    "print(\"\\n‚ùå RECORDS THAT FAILED QUALITY CHECKS:\")\n",
    "df_with_quality.filter(col(\"is_quality_pass\") == False) \\\n",
    "    .select(\"order_id\", \"customer_id\", \"email\", \"amount\", \"status\", \"country\", \"order_date\") \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß **9. DATA QUALITY FRAMEWORK**\n",
    "\n",
    "Reusable framework for data quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß RUNNING DATA QUALITY FRAMEWORK:\n",
      "\n",
      "üìä QUALITY REPORT:\n",
      "+------+---------+------+-----------------------------------------------+------+-----+\n",
      "|failed|pass_rate|passed|rule                                           |status|total|\n",
      "+------+---------+------+-----------------------------------------------+------+-----+\n",
      "|0     |100.0    |16    |order_id NOT NULL                              |‚úÖ PASS|16   |\n",
      "|1     |93.75    |15    |customer_id NOT NULL                           |‚ùå FAIL|16   |\n",
      "|1     |93.75    |15    |email NOT NULL                                 |‚ùå FAIL|16   |\n",
      "|1     |93.75    |15    |amount NOT NULL                                |‚ùå FAIL|16   |\n",
      "|0     |100.0    |16    |order_date NOT NULL                            |‚úÖ PASS|16   |\n",
      "|0     |100.0    |16    |status NOT NULL                                |‚úÖ PASS|16   |\n",
      "|0     |100.0    |16    |country NOT NULL                               |‚úÖ PASS|16   |\n",
      "|2     |87.5     |14    |email REGEX                                    |‚ùå FAIL|16   |\n",
      "|3     |81.25    |13    |amount RANGE [0, 100000]                       |‚ùå FAIL|16   |\n",
      "|1     |93.75    |15    |status IN ['pending', 'completed', 'cancelled']|‚ùå FAIL|16   |\n",
      "|1     |93.75    |15    |country IN ['USA', 'UK', 'Canada']             |‚ùå FAIL|16   |\n",
      "|1     |93.75    |15    |order_id UNIQUE                                |‚ùå FAIL|16   |\n",
      "+------+---------+------+-----------------------------------------------+------+-----+\n",
      "\n",
      "\n",
      "üìà SUMMARY:\n",
      "Total Checks: 12\n",
      "Passed: 4 (33.33%)\n",
      "Failed: 8 (66.67%)\n"
     ]
    }
   ],
   "source": [
    "class DataQualityChecker:\n",
    "    \"\"\"\n",
    "    Reusable Data Quality Framework\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, rules):\n",
    "        self.df = df\n",
    "        self.rules = rules\n",
    "        self.results = []\n",
    "    \n",
    "    def check_not_null(self, column):\n",
    "        \"\"\"Check for null values\"\"\"\n",
    "        total = self.df.count()\n",
    "        null_count = self.df.filter(col(column).isNull()).count()\n",
    "        pass_rate = ((total - null_count) / total) * 100\n",
    "        \n",
    "        self.results.append({\n",
    "            \"rule\": f\"{column} NOT NULL\",\n",
    "            \"total\": total,\n",
    "            \"passed\": total - null_count,\n",
    "            \"failed\": null_count,\n",
    "            \"pass_rate\": __builtins__.round(pass_rate, 2),\n",
    "            \"status\": \"‚úÖ PASS\" if null_count == 0 else \"‚ùå FAIL\"\n",
    "        })\n",
    "    \n",
    "    def check_regex(self, column, pattern):\n",
    "        \"\"\"Check regex pattern\"\"\"\n",
    "        total = self.df.count()\n",
    "        passed = self.df.filter(col(column).rlike(pattern)).count()\n",
    "        failed = total - passed\n",
    "        pass_rate = (passed / total) * 100\n",
    "        \n",
    "        self.results.append({\n",
    "            \"rule\": f\"{column} REGEX\",\n",
    "            \"total\": total,\n",
    "            \"passed\": passed,\n",
    "            \"failed\": failed,\n",
    "            \"pass_rate\": __builtins__.round(pass_rate, 2),\n",
    "            \"status\": \"‚úÖ PASS\" if failed == 0 else \"‚ùå FAIL\"\n",
    "        })\n",
    "    \n",
    "    def check_range(self, column, min_val, max_val):\n",
    "        \"\"\"Check value range\"\"\"\n",
    "        total = self.df.count()\n",
    "        passed = self.df.filter((col(column) >= min_val) & (col(column) <= max_val)).count()\n",
    "        failed = total - passed\n",
    "        pass_rate = (passed / total) * 100\n",
    "        \n",
    "        self.results.append({\n",
    "            \"rule\": f\"{column} RANGE [{min_val}, {max_val}]\",\n",
    "            \"total\": total,\n",
    "            \"passed\": passed,\n",
    "            \"failed\": failed,\n",
    "            \"pass_rate\": __builtins__.round(pass_rate, 2),\n",
    "            \"status\": \"‚úÖ PASS\" if failed == 0 else \"‚ùå FAIL\"\n",
    "        })\n",
    "    \n",
    "    def check_in_list(self, column, valid_values):\n",
    "        \"\"\"Check if value in list\"\"\"\n",
    "        total = self.df.count()\n",
    "        passed = self.df.filter(col(column).isin(valid_values)).count()\n",
    "        failed = total - passed\n",
    "        pass_rate = (passed / total) * 100\n",
    "        \n",
    "        self.results.append({\n",
    "            \"rule\": f\"{column} IN {valid_values}\",\n",
    "            \"total\": total,\n",
    "            \"passed\": passed,\n",
    "            \"failed\": failed,\n",
    "            \"pass_rate\": __builtins__.round(pass_rate, 2),\n",
    "            \"status\": \"‚úÖ PASS\" if failed == 0 else \"‚ùå FAIL\"\n",
    "        })\n",
    "    \n",
    "    def check_unique(self, columns):\n",
    "        \"\"\"Check uniqueness\"\"\"\n",
    "        total = self.df.count()\n",
    "        distinct = self.df.select(columns).distinct().count()\n",
    "        duplicates = total - distinct\n",
    "        pass_rate = (distinct / total) * 100\n",
    "        \n",
    "        self.results.append({\n",
    "            \"rule\": f\"{', '.join(columns)} UNIQUE\",\n",
    "            \"total\": total,\n",
    "            \"passed\": distinct,\n",
    "            \"failed\": duplicates,\n",
    "            \"pass_rate\": __builtins__.round(pass_rate, 2),\n",
    "            \"status\": \"‚úÖ PASS\" if duplicates == 0 else \"‚ùå FAIL\"\n",
    "        })\n",
    "    \n",
    "    def run_all_checks(self):\n",
    "        \"\"\"Run all defined checks\"\"\"\n",
    "        # Completeness checks\n",
    "        for column in self.df.columns:\n",
    "            self.check_not_null(column)\n",
    "        \n",
    "        # Validity checks\n",
    "        self.check_regex(\"email\", r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\")\n",
    "        self.check_range(\"amount\", 0, 100000)\n",
    "        self.check_in_list(\"status\", [\"pending\", \"completed\", \"cancelled\"])\n",
    "        self.check_in_list(\"country\", [\"USA\", \"UK\", \"Canada\"])\n",
    "        \n",
    "        # Uniqueness checks\n",
    "        self.check_unique([\"order_id\"])\n",
    "        \n",
    "        return self.get_report()\n",
    "    \n",
    "    def get_report(self):\n",
    "        \"\"\"Get quality report as DataFrame\"\"\"\n",
    "        return spark.createDataFrame(self.results)\n",
    "\n",
    "# Use the framework\n",
    "print(\"üîß RUNNING DATA QUALITY FRAMEWORK:\")\n",
    "checker = DataQualityChecker(df, quality_rules)\n",
    "quality_report = checker.run_all_checks()\n",
    "\n",
    "print(\"\\nüìä QUALITY REPORT:\")\n",
    "quality_report.show(truncate=False)\n",
    "\n",
    "# Summary\n",
    "total_checks = quality_report.count()\n",
    "passed_checks = quality_report.filter(col(\"status\") == \"‚úÖ PASS\").count()\n",
    "failed_checks = total_checks - passed_checks\n",
    "\n",
    "print(f\"\\nüìà SUMMARY:\")\n",
    "print(f\"Total Checks: {total_checks}\")\n",
    "print(f\"Passed: {passed_checks} ({(passed_checks/total_checks)*100:.2f}%)\")\n",
    "print(f\"Failed: {failed_checks} ({(failed_checks/total_checks)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üíæ **10. SAVE QUALITY REPORT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/08 15:32:08 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Quality report saved to: s3a://warehouse/quality_reports/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data with quality flags saved to: s3a://warehouse/orders_with_quality/\n"
     ]
    }
   ],
   "source": [
    "# Save quality report to MinIO\n",
    "report_path = \"s3a://warehouse/quality_reports/\"\n",
    "\n",
    "# Add timestamp\n",
    "quality_report_with_ts = quality_report.withColumn(\n",
    "    \"report_timestamp\",\n",
    "    current_timestamp()\n",
    ")\n",
    "\n",
    "quality_report_with_ts.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"status\") \\\n",
    "    .parquet(report_path)\n",
    "\n",
    "print(f\"‚úÖ Quality report saved to: {report_path}\")\n",
    "\n",
    "# Save data with quality flags\n",
    "data_with_quality_path = \"s3a://warehouse/orders_with_quality/\"\n",
    "\n",
    "df_with_quality.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"is_quality_pass\") \\\n",
    "    .parquet(data_with_quality_path)\n",
    "\n",
    "print(f\"‚úÖ Data with quality flags saved to: {data_with_quality_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä **11. QUALITY DASHBOARD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìä DATA QUALITY DASHBOARD\n",
      "================================================================================\n",
      "\n",
      "üìà OVERALL METRICS:\n",
      "   Total Checks: 12\n",
      "   Passed: 4 (33.33%)\n",
      "   Failed: 8 (66.67%)\n",
      "   Average Pass Rate: 94.27%\n",
      "\n",
      "‚ùå FAILED CHECKS:\n",
      "   ‚Ä¢ customer_id NOT NULL: 1 failures (6.25%)\n",
      "   ‚Ä¢ email NOT NULL: 1 failures (6.25%)\n",
      "   ‚Ä¢ amount NOT NULL: 1 failures (6.25%)\n",
      "   ‚Ä¢ email REGEX: 2 failures (12.50%)\n",
      "   ‚Ä¢ amount RANGE [0, 100000]: 3 failures (18.75%)\n",
      "   ‚Ä¢ status IN ['pending', 'completed', 'cancelled']: 1 failures (6.25%)\n",
      "   ‚Ä¢ country IN ['USA', 'UK', 'Canada']: 1 failures (6.25%)\n",
      "   ‚Ä¢ order_id UNIQUE: 1 failures (6.25%)\n",
      "\n",
      "üîù TOP 5 ISSUES:\n",
      "   9. amount RANGE [0, 100000]: 3 failures\n",
      "   8. email REGEX: 2 failures\n",
      "   2. customer_id NOT NULL: 1 failures\n",
      "   3. email NOT NULL: 1 failures\n",
      "   4. amount NOT NULL: 1 failures\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create quality dashboard\n",
    "def create_quality_dashboard(quality_report):\n",
    "    \"\"\"\n",
    "    Create visual quality dashboard\n",
    "    \"\"\"\n",
    "    # Convert to Pandas for visualization\n",
    "    report_pd = quality_report.toPandas()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"üìä DATA QUALITY DASHBOARD\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Overall metrics\n",
    "    total_checks = len(report_pd)\n",
    "    passed = len(report_pd[report_pd['status'] == '‚úÖ PASS'])\n",
    "    failed = total_checks - passed\n",
    "    avg_pass_rate = report_pd['pass_rate'].mean()\n",
    "    \n",
    "    print(f\"\\nüìà OVERALL METRICS:\")\n",
    "    print(f\"   Total Checks: {total_checks}\")\n",
    "    print(f\"   Passed: {passed} ({(passed/total_checks)*100:.2f}%)\")\n",
    "    print(f\"   Failed: {failed} ({(failed/total_checks)*100:.2f}%)\")\n",
    "    print(f\"   Average Pass Rate: {avg_pass_rate:.2f}%\")\n",
    "    \n",
    "    # Failed checks\n",
    "    print(f\"\\n‚ùå FAILED CHECKS:\")\n",
    "    failed_checks = report_pd[report_pd['status'] == '‚ùå FAIL']\n",
    "    if len(failed_checks) > 0:\n",
    "        for _, row in failed_checks.iterrows():\n",
    "            print(f\"   ‚Ä¢ {row['rule']}: {row['failed']} failures ({100-row['pass_rate']:.2f}%)\")\n",
    "    else:\n",
    "        print(\"   None! All checks passed ‚úÖ\")\n",
    "    \n",
    "    # Top issues\n",
    "    print(f\"\\nüîù TOP 5 ISSUES:\")\n",
    "    top_issues = report_pd.nlargest(5, 'failed')\n",
    "    for i, row in top_issues.iterrows():\n",
    "        print(f\"   {i+1}. {row['rule']}: {row['failed']} failures\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "create_quality_dashboard(quality_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì **KEY TAKEAWAYS**\n",
    "\n",
    "### **‚úÖ Data Quality Dimensions:**\n",
    "\n",
    "1. **Completeness** - No missing values\n",
    "2. **Validity** - Values meet business rules\n",
    "3. **Uniqueness** - No duplicates\n",
    "4. **Timeliness** - Data is current\n",
    "5. **Consistency** - Data is logically consistent\n",
    "6. **Accuracy** - Data reflects reality\n",
    "\n",
    "### **üîß Best Practices:**\n",
    "\n",
    "1. **Define clear rules** - Document all quality requirements\n",
    "2. **Automate checks** - Run quality checks in ETL pipeline\n",
    "3. **Track metrics** - Monitor quality over time\n",
    "4. **Flag bad data** - Don't delete, flag for review\n",
    "5. **Create reports** - Share quality metrics with stakeholders\n",
    "6. **Set thresholds** - Define acceptable quality levels\n",
    "7. **Alert on failures** - Notify when quality drops\n",
    "8. **Root cause analysis** - Investigate quality issues\n",
    "\n",
    "### **üöÄ Production Tips:**\n",
    "\n",
    "- Run quality checks BEFORE and AFTER transformations\n",
    "- Store quality reports for auditing\n",
    "- Create quality SLAs (e.g., 95% pass rate)\n",
    "- Integrate with monitoring tools (Grafana, DataDog)\n",
    "- Use Great Expectations or Deequ for advanced checks\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ **CONGRATULATIONS!**\n",
    "\n",
    "B·∫°n ƒë√£ ho√†n th√†nh **DAY 2: DATA I/O & CLEANING**!\n",
    "\n",
    "### **‚úÖ ƒê√£ h·ªçc:**\n",
    "- Reading data from multiple formats\n",
    "- Writing data with partitioning\n",
    "- Data cleaning techniques\n",
    "- Data quality validation\n",
    "- Quality reporting & monitoring\n",
    "\n",
    "### **üöÄ Next: DAY 3**\n",
    "- Transformations & Aggregations\n",
    "- Window Functions\n",
    "- Complex Joins\n",
    "- Performance Optimization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark session stopped\n",
      "\n",
      "üéâ DAY 2 COMPLETED! Ready for DAY 3!\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "spark.stop()\n",
    "print(\"‚úÖ Spark session stopped\")\n",
    "print(\"\\nüéâ DAY 2 COMPLETED! Ready for DAY 3!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
