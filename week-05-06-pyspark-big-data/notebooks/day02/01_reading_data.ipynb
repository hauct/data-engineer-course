{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö DAY 2 - NOTEBOOK 1: READING DATA\n",
    "\n",
    "## üéØ Objectives:\n",
    "- Read data from multiple formats (CSV, JSON, Parquet)\n",
    "- Understand schema inference vs explicit schema\n",
    "- Use read options effectively\n",
    "- Handle corrupted/malformed data\n",
    "- Compare performance of different formats\n",
    "- Work with both local filesystem and MinIO (S3)\n",
    "\n",
    "## üìÇ Data Pipeline:\n",
    "```\n",
    "raw/ ‚Üí staging/ ‚Üí production/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß PART 1: SETUP SPARK SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Importing libraries...\n",
      "‚úÖ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when, expr, count, isnan, isnull\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    DoubleType, DateType, TimestampType\n",
    ")\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"üì¶ Importing libraries...\")\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è  No existing Spark session to stop\n"
     ]
    }
   ],
   "source": [
    "# Stop existing Spark session if any\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"üõë Stopped existing Spark session\")\n",
    "    time.sleep(2)\n",
    "except:\n",
    "    print(\"‚ÑπÔ∏è  No existing Spark session to stop\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Creating Spark Session with MinIO support...\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/03 10:35:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark Session Created Successfully!\n",
      "   App Name: Day2-ReadingData\n",
      "   App ID: app-20260103103547-0000\n",
      "   Master: spark://spark-master:7077\n",
      "   Spark Version: 3.5.1\n",
      "   Python Version: 3.8\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Creating Spark Session with MinIO support...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Day2-ReadingData\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"512m\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.cores.max\", \"2\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark Session Created Successfully!\")\n",
    "print(f\"   App Name: {spark.sparkContext.appName}\")\n",
    "print(f\"   App ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"   Master: {spark.sparkContext.master}\")\n",
    "print(f\"   Spark Version: {spark.version}\")\n",
    "print(f\"   Python Version: {spark.sparkContext.pythonVer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìÇ PART 2: DEFINE DATA PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Available data paths:\n",
      "======================================================================\n",
      "\n",
      "üóÇÔ∏è  LOCAL FILESYSTEM (Shared Volume):\n",
      "   Raw:        /opt/spark-data/raw\n",
      "   Staging:    /opt/spark-data/staging\n",
      "   Production: /opt/spark-data/production\n",
      "\n",
      "‚òÅÔ∏è  MINIO (S3-Compatible):\n",
      "   Raw:        s3a://raw\n",
      "   Staging:    s3a://staging\n",
      "   Production: s3a://production\n",
      "\n",
      "üí° TIP: Use local filesystem for development, MinIO for production\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DEFINE DATA PATHS\n",
    "# ============================================================================\n",
    "\n",
    "# Option 1: Local filesystem (shared volume)\n",
    "DATA_RAW_LOCAL = \"/opt/spark-data/raw\"\n",
    "DATA_STAGING_LOCAL = \"/opt/spark-data/staging\"\n",
    "DATA_PRODUCTION_LOCAL = \"/opt/spark-data/production\"\n",
    "\n",
    "# Option 2: MinIO (S3-compatible)\n",
    "DATA_RAW_S3 = \"s3a://raw\"\n",
    "DATA_STAGING_S3 = \"s3a://staging\"\n",
    "DATA_PRODUCTION_S3 = \"s3a://production\"\n",
    "\n",
    "print(\"üìÇ Available data paths:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüóÇÔ∏è  LOCAL FILESYSTEM (Shared Volume):\")\n",
    "print(f\"   Raw:        {DATA_RAW_LOCAL}\")\n",
    "print(f\"   Staging:    {DATA_STAGING_LOCAL}\")\n",
    "print(f\"   Production: {DATA_PRODUCTION_LOCAL}\")\n",
    "\n",
    "print(\"\\n‚òÅÔ∏è  MINIO (S3-Compatible):\")\n",
    "print(f\"   Raw:        {DATA_RAW_S3}\")\n",
    "print(f\"   Staging:    {DATA_STAGING_S3}\")\n",
    "print(f\"   Production: {DATA_PRODUCTION_S3}\")\n",
    "\n",
    "print(\"\\nüí° TIP: Use local filesystem for development, MinIO for production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìù PART 3: CREATE SAMPLE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Creating sample employee data...\n",
      "======================================================================\n",
      "‚úÖ Created 1000 employee records\n",
      "   Departments: Engineering, Sales, HR, Marketing, Finance\n",
      "   Salary range: $40,000 - $150,000\n",
      "   Age range: 22 - 65\n",
      "   Hire date range: Last 10 years\n"
     ]
    }
   ],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "fake = Faker()\n",
    "Faker.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"üìù Creating sample employee data...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create employees data\n",
    "employees_data = []\n",
    "departments = [\"Engineering\", \"Sales\", \"HR\", \"Marketing\", \"Finance\"]\n",
    "\n",
    "for i in range(1, 1001):\n",
    "    employees_data.append({\n",
    "        \"id\": i,\n",
    "        \"name\": fake.name(),\n",
    "        \"email\": fake.email(),\n",
    "        \"department\": random.choice(departments),\n",
    "        \"salary\": random.randint(40000, 150000),\n",
    "        \"age\": random.randint(22, 65),\n",
    "        \"hire_date\": fake.date_between(start_date='-10y', end_date='today').strftime('%Y-%m-%d')\n",
    "    })\n",
    "\n",
    "print(f\"‚úÖ Created {len(employees_data)} employee records\")\n",
    "print(f\"   Departments: {', '.join(departments)}\")\n",
    "print(f\"   Salary range: $40,000 - $150,000\")\n",
    "print(f\"   Age range: 22 - 65\")\n",
    "print(f\"   Hire date range: Last 10 years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Sample data preview:\n",
      "======================================================================\n",
      " id              name                    email  department  salary  age  hire_date\n",
      "  1      Allison Hill donaldgarcia@example.net Engineering   43278   39 2024-12-05\n",
      "  2    Javier Johnson  jesseguzman@example.net       Sales   69256   30 2017-12-29\n",
      "  3 Kimberly Robinson       lisa02@example.net Engineering  128696   56 2020-03-25\n",
      "  4  Daniel Gallagher   daviscolin@example.com Engineering  117397   49 2022-01-16\n",
      "  5    Monica Herrera      smiller@example.net Engineering   43905   27 2022-03-11\n"
     ]
    }
   ],
   "source": [
    "# Preview sample data\n",
    "print(\"\\nüìä Sample data preview:\")\n",
    "print(\"=\"*70)\n",
    "df_preview = pd.DataFrame(employees_data[:5])\n",
    "print(df_preview.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Creating directory structure...\n",
      "======================================================================\n",
      "‚úÖ Created: /opt/spark-data/raw/employees/\n",
      "\n",
      "üíæ Saving data to multiple formats...\n",
      "‚úÖ CSV saved: /opt/spark-data/raw/employees/employees.csv\n",
      "   Size: 67.83 KB\n",
      "‚úÖ JSON saved: /opt/spark-data/raw/employees/employees.json\n",
      "   Size: 136.14 KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Parquet saved: /opt/spark-data/raw/employees/employees.parquet\n",
      "\n",
      "üìä File size comparison:\n",
      "   CSV:     67.83 KB\n",
      "   JSON:    136.14 KB\n",
      "   Parquet: (columnar format - typically smallest)\n"
     ]
    }
   ],
   "source": [
    "# Create directory structure\n",
    "print(\"\\nüìÅ Creating directory structure...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "os.makedirs(f\"{DATA_RAW_LOCAL}/employees\", exist_ok=True)\n",
    "print(f\"‚úÖ Created: {DATA_RAW_LOCAL}/employees/\")\n",
    "\n",
    "# Save to different formats\n",
    "print(\"\\nüíæ Saving data to multiple formats...\")\n",
    "\n",
    "# 1. CSV\n",
    "df_pandas = pd.DataFrame(employees_data)\n",
    "csv_path = f'{DATA_RAW_LOCAL}/employees/employees.csv'\n",
    "df_pandas.to_csv(csv_path, index=False)\n",
    "csv_size = os.path.getsize(csv_path) / 1024  # KB\n",
    "print(f\"‚úÖ CSV saved: {csv_path}\")\n",
    "print(f\"   Size: {csv_size:.2f} KB\")\n",
    "\n",
    "# 2. JSON (line-delimited)\n",
    "json_path = f'{DATA_RAW_LOCAL}/employees/employees.json'\n",
    "df_pandas.to_json(json_path, orient='records', lines=True)\n",
    "json_size = os.path.getsize(json_path) / 1024  # KB\n",
    "print(f\"‚úÖ JSON saved: {json_path}\")\n",
    "print(f\"   Size: {json_size:.2f} KB\")\n",
    "\n",
    "# 3. Parquet (using Spark)\n",
    "df_spark = spark.createDataFrame(employees_data)\n",
    "parquet_path = f'{DATA_RAW_LOCAL}/employees/employees.parquet'\n",
    "df_spark.write.mode('overwrite').parquet(parquet_path)\n",
    "print(f\"‚úÖ Parquet saved: {parquet_path}\")\n",
    "\n",
    "print(\"\\nüìä File size comparison:\")\n",
    "print(f\"   CSV:     {csv_size:.2f} KB\")\n",
    "print(f\"   JSON:    {json_size:.2f} KB\")\n",
    "print(f\"   Parquet: (columnar format - typically smallest)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìñ PART 4: READING CSV FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå READING CSV - Basic (with schema inference)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CSV loaded successfully\n",
      "   Rows: 1000\n",
      "   Columns: 7\n",
      "   Read time: 4.460 seconds\n",
      "\n",
      "üìã Inferred Schema:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n",
      "\n",
      "üìä Sample data:\n",
      "+---+-----------------+------------------------+-----------+------+---+----------+\n",
      "|id |name             |email                   |department |salary|age|hire_date |\n",
      "+---+-----------------+------------------------+-----------+------+---+----------+\n",
      "|1  |Allison Hill     |donaldgarcia@example.net|Engineering|43278 |39 |2024-12-05|\n",
      "|2  |Javier Johnson   |jesseguzman@example.net |Sales      |69256 |30 |2017-12-29|\n",
      "|3  |Kimberly Robinson|lisa02@example.net      |Engineering|128696|56 |2020-03-25|\n",
      "|4  |Daniel Gallagher |daviscolin@example.com  |Engineering|117397|49 |2022-01-16|\n",
      "|5  |Monica Herrera   |smiller@example.net     |Engineering|43905 |27 |2022-03-11|\n",
      "+---+-----------------+------------------------+-----------+------+---+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üìå READING CSV - Basic (with schema inference)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Read CSV with schema inference\n",
    "start_time = time.time()\n",
    "\n",
    "df_csv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(f\"{DATA_RAW_LOCAL}/employees/employees.csv\")\n",
    "\n",
    "read_time = time.time() - start_time\n",
    "\n",
    "print(\"‚úÖ CSV loaded successfully\")\n",
    "print(f\"   Rows: {df_csv.count()}\")\n",
    "print(f\"   Columns: {len(df_csv.columns)}\")\n",
    "print(f\"   Read time: {read_time:.3f} seconds\")\n",
    "\n",
    "print(\"\\nüìã Inferred Schema:\")\n",
    "df_csv.printSchema()\n",
    "\n",
    "print(\"\\nüìä Sample data:\")\n",
    "df_csv.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå READING CSV - With Explicit Schema (RECOMMENDED)\n",
      "======================================================================\n",
      "üìã Defined Schema:\n",
      "struct<id:int,name:string,email:string,department:string,salary:int,age:int,hire_date:date>\n",
      "\n",
      "‚úÖ CSV loaded with explicit schema\n",
      "   Read time: 0.068 seconds\n",
      "   ‚ö° Faster than schema inference!\n",
      "\n",
      "üìã Schema:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n",
      "\n",
      "üìä Sample data:\n",
      "+---+-----------------+--------------------+-----------+------+---+----------+\n",
      "| id|             name|               email| department|salary|age| hire_date|\n",
      "+---+-----------------+--------------------+-----------+------+---+----------+\n",
      "|  1|     Allison Hill|donaldgarcia@exam...|Engineering| 43278| 39|2024-12-05|\n",
      "|  2|   Javier Johnson|jesseguzman@examp...|      Sales| 69256| 30|2017-12-29|\n",
      "|  3|Kimberly Robinson|  lisa02@example.net|Engineering|128696| 56|2020-03-25|\n",
      "|  4| Daniel Gallagher|daviscolin@exampl...|Engineering|117397| 49|2022-01-16|\n",
      "|  5|   Monica Herrera| smiller@example.net|Engineering| 43905| 27|2022-03-11|\n",
      "+---+-----------------+--------------------+-----------+------+---+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üìå READING CSV - With Explicit Schema (RECOMMENDED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define explicit schema\n",
    "employee_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), False),\n",
    "    StructField(\"salary\", IntegerType(), False),\n",
    "    StructField(\"age\", IntegerType(), False),\n",
    "    StructField(\"hire_date\", DateType(), False)\n",
    "])\n",
    "\n",
    "print(\"üìã Defined Schema:\")\n",
    "print(employee_schema.simpleString())\n",
    "\n",
    "# Read with explicit schema\n",
    "start_time = time.time()\n",
    "\n",
    "df_csv_explicit = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\") \\\n",
    "    .schema(employee_schema) \\\n",
    "    .csv(f\"{DATA_RAW_LOCAL}/employees/employees.csv\")\n",
    "\n",
    "read_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n‚úÖ CSV loaded with explicit schema\")\n",
    "print(f\"   Read time: {read_time:.3f} seconds\")\n",
    "print(\"   ‚ö° Faster than schema inference!\")\n",
    "\n",
    "print(\"\\nüìã Schema:\")\n",
    "df_csv_explicit.printSchema()\n",
    "\n",
    "print(\"\\nüìä Sample data:\")\n",
    "df_csv_explicit.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå CSV READ OPTIONS - Advanced\n",
      "======================================================================\n",
      "‚úÖ CSV loaded with advanced options\n",
      "\n",
      "üìù Options explained:\n",
      "   ‚Ä¢ header: First row is column names\n",
      "   ‚Ä¢ sep: Field delimiter (comma)\n",
      "   ‚Ä¢ quote: Quote character for strings\n",
      "   ‚Ä¢ escape: Escape character\n",
      "   ‚Ä¢ nullValue: String to treat as NULL\n",
      "   ‚Ä¢ dateFormat: Date parsing format\n",
      "   ‚Ä¢ mode: How to handle corrupt records\n",
      "   ‚Ä¢ columnNameOfCorruptRecord: Column for bad records\n",
      "+---+-----------------+--------------------+-----------+------+---+----------+\n",
      "| id|             name|               email| department|salary|age| hire_date|\n",
      "+---+-----------------+--------------------+-----------+------+---+----------+\n",
      "|  1|     Allison Hill|donaldgarcia@exam...|Engineering| 43278| 39|2024-12-05|\n",
      "|  2|   Javier Johnson|jesseguzman@examp...|      Sales| 69256| 30|2017-12-29|\n",
      "|  3|Kimberly Robinson|  lisa02@example.net|Engineering|128696| 56|2020-03-25|\n",
      "|  4| Daniel Gallagher|daviscolin@exampl...|Engineering|117397| 49|2022-01-16|\n",
      "|  5|   Monica Herrera| smiller@example.net|Engineering| 43905| 27|2022-03-11|\n",
      "+---+-----------------+--------------------+-----------+------+---+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üìå CSV READ OPTIONS - Advanced\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Common CSV options\n",
    "df_csv_advanced = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .option(\"quote\", '\"') \\\n",
    "    .option(\"escape\", \"\\\\\") \\\n",
    "    .option(\"nullValue\", \"NULL\") \\\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\") \\\n",
    "    .option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\") \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\\n",
    "    .schema(employee_schema) \\\n",
    "    .csv(f\"{DATA_RAW_LOCAL}/employees/employees.csv\")\n",
    "\n",
    "print(\"‚úÖ CSV loaded with advanced options\")\n",
    "print(\"\\nüìù Options explained:\")\n",
    "print(\"   ‚Ä¢ header: First row is column names\")\n",
    "print(\"   ‚Ä¢ sep: Field delimiter (comma)\")\n",
    "print(\"   ‚Ä¢ quote: Quote character for strings\")\n",
    "print(\"   ‚Ä¢ escape: Escape character\")\n",
    "print(\"   ‚Ä¢ nullValue: String to treat as NULL\")\n",
    "print(\"   ‚Ä¢ dateFormat: Date parsing format\")\n",
    "print(\"   ‚Ä¢ mode: How to handle corrupt records\")\n",
    "print(\"   ‚Ä¢ columnNameOfCorruptRecord: Column for bad records\")\n",
    "\n",
    "df_csv_advanced.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìñ PART 5: READING JSON FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå READING JSON - Line-delimited (JSONL)\n",
      "======================================================================\n",
      "‚úÖ JSON loaded successfully\n",
      "   Rows: 1000\n",
      "   Columns: 7\n",
      "   Read time: 0.403 seconds\n",
      "\n",
      "üìã Schema (auto-inferred):\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "\n",
      "üìä Sample data:\n",
      "+---+-----------+--------------------+----------+---+-----------------+------+\n",
      "|age| department|               email| hire_date| id|             name|salary|\n",
      "+---+-----------+--------------------+----------+---+-----------------+------+\n",
      "| 39|Engineering|donaldgarcia@exam...|2024-12-05|  1|     Allison Hill| 43278|\n",
      "| 30|      Sales|jesseguzman@examp...|2017-12-29|  2|   Javier Johnson| 69256|\n",
      "| 56|Engineering|  lisa02@example.net|2020-03-25|  3|Kimberly Robinson|128696|\n",
      "| 49|Engineering|daviscolin@exampl...|2022-01-16|  4| Daniel Gallagher|117397|\n",
      "| 27|Engineering| smiller@example.net|2022-03-11|  5|   Monica Herrera| 43905|\n",
      "+---+-----------+--------------------+----------+---+-----------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üìå READING JSON - Line-delimited (JSONL)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df_json = spark.read.json(f\"{DATA_RAW_LOCAL}/employees/employees.json\")\n",
    "\n",
    "read_time = time.time() - start_time\n",
    "\n",
    "print(\"‚úÖ JSON loaded successfully\")\n",
    "print(f\"   Rows: {df_json.count()}\")\n",
    "print(f\"   Columns: {len(df_json.columns)}\")\n",
    "print(f\"   Read time: {read_time:.3f} seconds\")\n",
    "\n",
    "print(\"\\nüìã Schema (auto-inferred):\")\n",
    "df_json.printSchema()\n",
    "\n",
    "print(\"\\nüìä Sample data:\")\n",
    "df_json.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå READING JSON - With Explicit Schema\n",
      "======================================================================\n",
      "‚úÖ JSON loaded with explicit schema\n",
      "   Read time: 0.051 seconds\n",
      "   ‚ö° Faster than schema inference!\n",
      "+---+-----------------+--------------------+-----------+------+---+----------+\n",
      "| id|             name|               email| department|salary|age| hire_date|\n",
      "+---+-----------------+--------------------+-----------+------+---+----------+\n",
      "|  1|     Allison Hill|donaldgarcia@exam...|Engineering| 43278| 39|2024-12-05|\n",
      "|  2|   Javier Johnson|jesseguzman@examp...|      Sales| 69256| 30|2017-12-29|\n",
      "|  3|Kimberly Robinson|  lisa02@example.net|Engineering|128696| 56|2020-03-25|\n",
      "|  4| Daniel Gallagher|daviscolin@exampl...|Engineering|117397| 49|2022-01-16|\n",
      "|  5|   Monica Herrera| smiller@example.net|Engineering| 43905| 27|2022-03-11|\n",
      "+---+-----------------+--------------------+-----------+------+---+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üìå READING JSON - With Explicit Schema\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df_json_explicit = spark.read \\\n",
    "    .schema(employee_schema) \\\n",
    "    .json(f\"{DATA_RAW_LOCAL}/employees/employees.json\")\n",
    "\n",
    "read_time = time.time() - start_time\n",
    "\n",
    "print(\"‚úÖ JSON loaded with explicit schema\")\n",
    "print(f\"   Read time: {read_time:.3f} seconds\")\n",
    "print(\"   ‚ö° Faster than schema inference!\")\n",
    "\n",
    "df_json_explicit.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìñ PART 6: READING PARQUET FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå READING PARQUET - Columnar Format\n",
      "======================================================================\n",
      "‚úÖ Parquet loaded successfully\n",
      "   Rows: 1000\n",
      "   Columns: 7\n",
      "   Read time: 0.314 seconds\n",
      "   ‚ö° Fastest format!\n",
      "\n",
      "üìã Schema (embedded in Parquet):\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "\n",
      "üìä Sample data:\n",
      "+---+-----------+--------------------+----------+---+-----------------+------+\n",
      "|age| department|               email| hire_date| id|             name|salary|\n",
      "+---+-----------+--------------------+----------+---+-----------------+------+\n",
      "| 39|Engineering|donaldgarcia@exam...|2024-12-05|  1|     Allison Hill| 43278|\n",
      "| 30|      Sales|jesseguzman@examp...|2017-12-29|  2|   Javier Johnson| 69256|\n",
      "| 56|Engineering|  lisa02@example.net|2020-03-25|  3|Kimberly Robinson|128696|\n",
      "| 49|Engineering|daviscolin@exampl...|2022-01-16|  4| Daniel Gallagher|117397|\n",
      "| 27|Engineering| smiller@example.net|2022-03-11|  5|   Monica Herrera| 43905|\n",
      "+---+-----------+--------------------+----------+---+-----------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "üí° Parquet advantages:\n",
      "   ‚Ä¢ Schema is embedded (no inference needed)\n",
      "   ‚Ä¢ Columnar storage (efficient for analytics)\n",
      "   ‚Ä¢ Compressed by default\n",
      "   ‚Ä¢ Supports predicate pushdown\n",
      "   ‚Ä¢ Best for production workloads\n"
     ]
    }
   ],
   "source": [
    "print(\"üìå READING PARQUET - Columnar Format\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df_parquet = spark.read.parquet(f\"{DATA_RAW_LOCAL}/employees/employees.parquet\")\n",
    "\n",
    "read_time = time.time() - start_time\n",
    "\n",
    "print(\"‚úÖ Parquet loaded successfully\")\n",
    "print(f\"   Rows: {df_parquet.count()}\")\n",
    "print(f\"   Columns: {len(df_parquet.columns)}\")\n",
    "print(f\"   Read time: {read_time:.3f} seconds\")\n",
    "print(\"   ‚ö° Fastest format!\")\n",
    "\n",
    "print(\"\\nüìã Schema (embedded in Parquet):\")\n",
    "df_parquet.printSchema()\n",
    "\n",
    "print(\"\\nüìä Sample data:\")\n",
    "df_parquet.show(5)\n",
    "\n",
    "print(\"\\nüí° Parquet advantages:\")\n",
    "print(\"   ‚Ä¢ Schema is embedded (no inference needed)\")\n",
    "print(\"   ‚Ä¢ Columnar storage (efficient for analytics)\")\n",
    "print(\"   ‚Ä¢ Compressed by default\")\n",
    "print(\"   ‚Ä¢ Supports predicate pushdown\")\n",
    "print(\"   ‚Ä¢ Best for production workloads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä PART 7: PERFORMANCE COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä PERFORMANCE COMPARISON\n",
      "======================================================================\n",
      "\n",
      "üîÑ Testing CSV...\n",
      "   Time: 4.224s\n",
      "\n",
      "üîÑ Testing CSV (explicit schema)...\n",
      "   Time: 0.314s\n",
      "\n",
      "üîÑ Testing JSON...\n",
      "   Time: 0.551s\n",
      "\n",
      "üîÑ Testing Parquet...\n",
      "   Time: 0.574s\n",
      "\n",
      "======================================================================\n",
      "üìä RESULTS SUMMARY\n",
      "======================================================================\n",
      "Format               Time (s)        Rows       Speedup\n",
      "----------------------------------------------------------------------\n",
      "CSV (inferred)       4.224           1000       1.00x\n",
      "CSV (explicit)       0.314           1000       13.46x\n",
      "JSON (inferred)      0.551           1000       7.67x\n",
      "Parquet              0.574           1000       7.36x\n",
      "\n",
      "üèÜ Winner: Parquet (fastest)\n",
      "üí° Recommendation: Use Parquet for production workloads\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import time\n",
    "\n",
    "results = []\n",
    "\n",
    "# Test CSV\n",
    "print(\"\\nüîÑ Testing CSV...\")\n",
    "start = time.time()\n",
    "df_csv_test = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{DATA_RAW_LOCAL}/employees/employees.csv\")\n",
    "count_csv = df_csv_test.count()\n",
    "time_csv = time.time() - start\n",
    "results.append((\"CSV (inferred)\", time_csv, count_csv))\n",
    "print(f\"   Time: {time_csv:.3f}s\")\n",
    "\n",
    "# Test CSV with explicit schema\n",
    "print(\"\\nüîÑ Testing CSV (explicit schema)...\")\n",
    "start = time.time()\n",
    "df_csv_explicit_test = spark.read.option(\"header\", \"true\").schema(employee_schema).csv(f\"{DATA_RAW_LOCAL}/employees/employees.csv\")\n",
    "count_csv_explicit = df_csv_explicit_test.count()\n",
    "time_csv_explicit = time.time() - start\n",
    "results.append((\"CSV (explicit)\", time_csv_explicit, count_csv_explicit))\n",
    "print(f\"   Time: {time_csv_explicit:.3f}s\")\n",
    "\n",
    "# Test JSON\n",
    "print(\"\\nüîÑ Testing JSON...\")\n",
    "start = time.time()\n",
    "df_json_test = spark.read.json(f\"{DATA_RAW_LOCAL}/employees/employees.json\")\n",
    "count_json = df_json_test.count()\n",
    "time_json = time.time() - start\n",
    "results.append((\"JSON (inferred)\", time_json, count_json))\n",
    "print(f\"   Time: {time_json:.3f}s\")\n",
    "\n",
    "# Test Parquet\n",
    "print(\"\\nüîÑ Testing Parquet...\")\n",
    "start = time.time()\n",
    "df_parquet_test = spark.read.parquet(f\"{DATA_RAW_LOCAL}/employees/employees.parquet\")\n",
    "count_parquet = df_parquet_test.count()\n",
    "time_parquet = time.time() - start\n",
    "results.append((\"Parquet\", time_parquet, count_parquet))\n",
    "print(f\"   Time: {time_parquet:.3f}s\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Format':<20} {'Time (s)':<15} {'Rows':<10} {'Speedup'}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "baseline = time_csv\n",
    "for format_name, time_taken, row_count in results:\n",
    "    speedup = baseline / time_taken\n",
    "    print(f\"{format_name:<20} {time_taken:<15.3f} {row_count:<10} {speedup:.2f}x\")\n",
    "\n",
    "print(\"\\nüèÜ Winner: Parquet (fastest)\")\n",
    "print(\"üí° Recommendation: Use Parquet for production workloads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üõ°Ô∏è PART 8: HANDLING CORRUPTED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Creating corrupted CSV file for testing...\n",
      "======================================================================\n",
      "‚úÖ Created corrupted CSV: /opt/spark-data/raw/employees/employees_corrupted.csv\n",
      "   Contains 2 corrupted records out of 6 total\n"
     ]
    }
   ],
   "source": [
    "print(\"üìù Creating corrupted CSV file for testing...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create a CSV with some corrupted records\n",
    "corrupted_csv = f\"{DATA_RAW_LOCAL}/employees/employees_corrupted.csv\"\n",
    "\n",
    "with open(corrupted_csv, 'w') as f:\n",
    "    f.write(\"id,name,email,department,salary,age,hire_date\\n\")\n",
    "    f.write(\"1,John Doe,john@example.com,Engineering,75000,30,2020-01-15\\n\")\n",
    "    f.write(\"2,Jane Smith,jane@example.com,Sales,65000,28,2021-03-20\\n\")\n",
    "    f.write(\"3,Bad Record,missing_fields\\n\")  # ‚ùå Corrupted: missing fields\n",
    "    f.write(\"4,Bob Johnson,bob@example.com,HR,55000,35,2019-06-10\\n\")\n",
    "    f.write(\"5,Alice,alice@example.com,Marketing,INVALID,29,2022-01-05\\n\")  # ‚ùå Corrupted: invalid salary\n",
    "    f.write(\"6,Charlie Brown,charlie@example.com,Finance,70000,32,2020-09-15\\n\")\n",
    "\n",
    "print(f\"‚úÖ Created corrupted CSV: {corrupted_csv}\")\n",
    "print(\"   Contains 2 corrupted records out of 6 total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå MODE: PERMISSIVE (default) - Keep corrupted records\n",
      "======================================================================\n",
      "‚úÖ Data loaded with PERMISSIVE mode\n",
      "   Total rows: 6\n",
      "\n",
      "üìä All records (including corrupted):\n",
      "+---+-------------+-------------------+-----------+------+----+----------+---------------------------------------------------------+\n",
      "|id |name         |email              |department |salary|age |hire_date |_corrupt_record                                          |\n",
      "+---+-------------+-------------------+-----------+------+----+----------+---------------------------------------------------------+\n",
      "|1  |John Doe     |john@example.com   |Engineering|75000 |30  |2020-01-15|NULL                                                     |\n",
      "|2  |Jane Smith   |jane@example.com   |Sales      |65000 |28  |2021-03-20|NULL                                                     |\n",
      "|3  |Bad Record   |missing_fields     |NULL       |NULL  |NULL|NULL      |3,Bad Record,missing_fields                              |\n",
      "|4  |Bob Johnson  |bob@example.com    |HR         |55000 |35  |2019-06-10|NULL                                                     |\n",
      "|5  |Alice        |alice@example.com  |Marketing  |NULL  |29  |2022-01-05|5,Alice,alice@example.com,Marketing,INVALID,29,2022-01-05|\n",
      "|6  |Charlie Brown|charlie@example.com|Finance    |70000 |32  |2020-09-15|NULL                                                     |\n",
      "+---+-------------+-------------------+-----------+------+----+----------+---------------------------------------------------------+\n",
      "\n",
      "\n",
      "üîç Corrupted records only:\n",
      "   Found 0 corrupted records\n",
      "+---+----------+-----------------+----------+------+----+----------+---------------------------------------------------------+\n",
      "|id |name      |email            |department|salary|age |hire_date |_corrupt_record                                          |\n",
      "+---+----------+-----------------+----------+------+----+----------+---------------------------------------------------------+\n",
      "|3  |Bad Record|missing_fields   |NULL      |NULL  |NULL|NULL      |3,Bad Record,missing_fields                              |\n",
      "|5  |Alice     |alice@example.com|Marketing |NULL  |29  |2022-01-05|5,Alice,alice@example.com,Marketing,INVALID,29,2022-01-05|\n",
      "+---+----------+-----------------+----------+------+----+----------+---------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üìå MODE: PERMISSIVE (default) - Keep corrupted records\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df_permissive = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\\n",
    "    .schema(employee_schema.add(\"_corrupt_record\", StringType(), True)) \\\n",
    "    .csv(corrupted_csv)\n",
    "\n",
    "print(\"‚úÖ Data loaded with PERMISSIVE mode\")\n",
    "print(f\"   Total rows: {df_permissive.count()}\")\n",
    "\n",
    "print(\"\\nüìä All records (including corrupted):\")\n",
    "df_permissive.show(10, truncate=False)\n",
    "\n",
    "print(\"\\nüîç Corrupted records only:\")\n",
    "df_corrupted = df_permissive.filter(col(\"_corrupt_record\").isNotNull())\n",
    "print(f\"   Found {df_corrupted.count()} corrupted records\")\n",
    "df_corrupted.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå MODE: DROPMALFORMED - Drop corrupted records\n",
      "======================================================================\n",
      "‚úÖ Data loaded with DROPMALFORMED mode\n",
      "   Total rows: 6\n",
      "   ‚ö†Ô∏è  Corrupted records were dropped\n",
      "\n",
      "üìä Valid records only:\n",
      "+---+-------------+-------------------+-----------+------+---+----------+---------------+\n",
      "|id |name         |email              |department |salary|age|hire_date |_corrupt_record|\n",
      "+---+-------------+-------------------+-----------+------+---+----------+---------------+\n",
      "|1  |John Doe     |john@example.com   |Engineering|75000 |30 |2020-01-15|NULL           |\n",
      "|2  |Jane Smith   |jane@example.com   |Sales      |65000 |28 |2021-03-20|NULL           |\n",
      "|4  |Bob Johnson  |bob@example.com    |HR         |55000 |35 |2019-06-10|NULL           |\n",
      "|6  |Charlie Brown|charlie@example.com|Finance    |70000 |32 |2020-09-15|NULL           |\n",
      "+---+-------------+-------------------+-----------+------+---+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üìå MODE: DROPMALFORMED - Drop corrupted records\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df_dropmalformed = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "    .schema(employee_schema) \\\n",
    "    .csv(corrupted_csv)\n",
    "\n",
    "print(\"‚úÖ Data loaded with DROPMALFORMED mode\")\n",
    "print(f\"   Total rows: {df_dropmalformed.count()}\")\n",
    "print(\"   ‚ö†Ô∏è  Corrupted records were dropped\")\n",
    "\n",
    "print(\"\\nüìä Valid records only:\")\n",
    "df_dropmalformed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå MODE: FAILFAST - Fail on corrupted records\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/03 10:36:16 WARN TaskSetManager: Lost task 0.0 in stage 48.0 (TID 41) (172.18.0.7 executor 1): org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [3,Bad Record,missing_fields,null,null,null,null,3,Bad Record,missing_fields].\n",
      "Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1610)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 3,Bad Record,missing_fields\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n",
      "\t... 26 more\n",
      "Caused by: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 3,Bad Record,missing_fields\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1393)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:332)\n",
      "\t... 29 more\n",
      "\n",
      "26/01/03 10:36:16 ERROR TaskSetManager: Task 0 in stage 48.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå FAILFAST mode detected corrupted data!\n",
      "   Error: An error occurred while calling o199.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 48.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4...\n",
      "\n",
      "üí° Use FAILFAST in production to catch data quality issues early\n"
     ]
    }
   ],
   "source": [
    "print(\"üìå MODE: FAILFAST - Fail on corrupted records\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    df_failfast = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"mode\", \"FAILFAST\") \\\n",
    "        .schema(employee_schema) \\\n",
    "        .csv(corrupted_csv)\n",
    "    \n",
    "    # This will trigger the error\n",
    "    df_failfast.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"‚ùå FAILFAST mode detected corrupted data!\")\n",
    "    print(f\"   Error: {str(e)[:200]}...\")\n",
    "    print(\"\\nüí° Use FAILFAST in production to catch data quality issues early\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üì§ PART 9: READING MULTIPLE FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Creating multiple CSV files...\n",
      "======================================================================\n",
      "‚úÖ Created: employees_part_1.csv (333 records)\n",
      "‚úÖ Created: employees_part_2.csv (333 records)\n",
      "‚úÖ Created: employees_part_3.csv (334 records)\n",
      "\n",
      "üìÅ Directory: /opt/spark-data/raw/employees_multi\n"
     ]
    }
   ],
   "source": [
    "print(\"üìù Creating multiple CSV files...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create directory for multiple files\n",
    "multi_dir = f\"{DATA_RAW_LOCAL}/employees_multi\"\n",
    "os.makedirs(multi_dir, exist_ok=True)\n",
    "\n",
    "# Split data into 3 files\n",
    "chunk_size = len(employees_data) // 3\n",
    "\n",
    "for i in range(3):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = start_idx + chunk_size if i < 2 else len(employees_data)\n",
    "    chunk_data = employees_data[start_idx:end_idx]\n",
    "    \n",
    "    df_chunk = pd.DataFrame(chunk_data)\n",
    "    df_chunk.to_csv(f\"{multi_dir}/employees_part_{i+1}.csv\", index=False)\n",
    "    print(f\"‚úÖ Created: employees_part_{i+1}.csv ({len(chunk_data)} records)\")\n",
    "\n",
    "print(f\"\\nüìÅ Directory: {multi_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Reading multiple CSV files at once\n",
      "======================================================================\n",
      "‚úÖ All files loaded successfully\n",
      "   Total rows: 1000\n",
      "   Files read: 3\n",
      "\n",
      "üìä Sample from combined data:\n",
      "+---+-----------------+--------------------+-----------+------+---+----------+---------------+\n",
      "| id|             name|               email| department|salary|age| hire_date|_corrupt_record|\n",
      "+---+-----------------+--------------------+-----------+------+---+----------+---------------+\n",
      "|667|      Susan Myers|kristina57@exampl...|    Finance| 71711| 31|2016-04-22|           NULL|\n",
      "|668|   Gregory Murray|justinbailey@exam...|         HR| 96261| 22|2017-04-12|           NULL|\n",
      "|669|    Robert Robles|angela14@example.net|    Finance| 86244| 37|2020-08-12|           NULL|\n",
      "|670|  Cynthia Sanchez|terrireyes@exampl...|    Finance| 94600| 33|2018-02-01|           NULL|\n",
      "|671|    Jessica Baird|   amy32@example.net|Engineering|108614| 45|2022-09-14|           NULL|\n",
      "|672|Michelle Copeland|vargassteven@exam...|Engineering|108951| 56|2016-12-24|           NULL|\n",
      "|673|    Eric Sullivan|jenningscandace@e...|    Finance|143076| 54|2019-04-13|           NULL|\n",
      "|674|     Amy Robinson|paigeberger@examp...|    Finance| 42665| 46|2024-02-08|           NULL|\n",
      "|675|      Mary Chavez| peter97@example.org|  Marketing| 45706| 62|2018-01-23|           NULL|\n",
      "|676|  Elizabeth Bauer|samantha65@exampl...|  Marketing| 88929| 38|2021-10-31|           NULL|\n",
      "+---+-----------------+--------------------+-----------+------+---+----------+---------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "üí° Spark automatically combines all matching files!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"üìå Reading multiple CSV files at once\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Read all CSV files in directory\n",
    "df_multi = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(employee_schema) \\\n",
    "    .csv(f\"{multi_dir}/*.csv\")\n",
    "\n",
    "print(\"‚úÖ All files loaded successfully\")\n",
    "print(f\"   Total rows: {df_multi.count()}\")\n",
    "print(f\"   Files read: 3\")\n",
    "\n",
    "print(\"\\nüìä Sample from combined data:\")\n",
    "df_multi.show(10)\n",
    "\n",
    "print(\"\\nüí° Spark automatically combines all matching files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚òÅÔ∏è PART 10: READING FROM MINIO (S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Writing data to MinIO (S3)...\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/03 10:36:17 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data written to MinIO: s3a://raw/employees/employees.parquet\n"
     ]
    }
   ],
   "source": [
    "print(\"üì§ Writing data to MinIO (S3)...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    # Write Parquet to MinIO\n",
    "    s3_path = f\"{DATA_RAW_S3}/employees/employees.parquet\"\n",
    "    \n",
    "    df_parquet.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(s3_path)\n",
    "    \n",
    "    print(f\"‚úÖ Data written to MinIO: {s3_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to write to MinIO: {str(e)[:200]}\")\n",
    "    print(\"\\nüí° Make sure:\")\n",
    "    print(\"   1. MinIO is running (docker-compose ps)\")\n",
    "    print(\"   2. Buckets are created (check MinIO console)\")\n",
    "    print(\"   3. Credentials are correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Reading data from MinIO (S3)...\n",
      "======================================================================\n",
      "‚úÖ Data read from MinIO successfully\n",
      "   Rows: 1000\n",
      "   Columns: 7\n",
      "\n",
      "üìä Sample data from S3:\n",
      "+---+-----------+--------------------+----------+---+-----------------+------+\n",
      "|age| department|               email| hire_date| id|             name|salary|\n",
      "+---+-----------+--------------------+----------+---+-----------------+------+\n",
      "| 39|Engineering|donaldgarcia@exam...|2024-12-05|  1|     Allison Hill| 43278|\n",
      "| 30|      Sales|jesseguzman@examp...|2017-12-29|  2|   Javier Johnson| 69256|\n",
      "| 56|Engineering|  lisa02@example.net|2020-03-25|  3|Kimberly Robinson|128696|\n",
      "| 49|Engineering|daviscolin@exampl...|2022-01-16|  4| Daniel Gallagher|117397|\n",
      "| 27|Engineering| smiller@example.net|2022-03-11|  5|   Monica Herrera| 43905|\n",
      "+---+-----------+--------------------+----------+---+-----------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "‚úÖ MinIO (S3) integration working!\n"
     ]
    }
   ],
   "source": [
    "print(\"üì• Reading data from MinIO (S3)...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    # Read from MinIO\n",
    "    df_from_s3 = spark.read.parquet(f\"{DATA_RAW_S3}/employees/employees.parquet\")\n",
    "    \n",
    "    print(\"‚úÖ Data read from MinIO successfully\")\n",
    "    print(f\"   Rows: {df_from_s3.count()}\")\n",
    "    print(f\"   Columns: {len(df_from_s3.columns)}\")\n",
    "    \n",
    "    print(\"\\nüìä Sample data from S3:\")\n",
    "    df_from_s3.show(5)\n",
    "    \n",
    "    print(\"\\n‚úÖ MinIO (S3) integration working!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to read from MinIO: {str(e)[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä PART 11: DATA QUALITY CHECKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DATA QUALITY CHECKS\n",
      "======================================================================\n",
      "\n",
      "1Ô∏è‚É£ Basic Statistics:\n",
      "   Total rows: 1000\n",
      "   Total columns: 7\n",
      "\n",
      "2Ô∏è‚É£ Column Names:\n",
      "   age, department, email, hire_date, id, name, salary\n",
      "\n",
      "3Ô∏è‚É£ Data Types:\n",
      "   age             LongType()           nullable=True\n",
      "   department      StringType()         nullable=True\n",
      "   email           StringType()         nullable=True\n",
      "   hire_date       StringType()         nullable=True\n",
      "   id              LongType()           nullable=True\n",
      "   name            StringType()         nullable=True\n",
      "   salary          LongType()           nullable=True\n",
      "\n",
      "4Ô∏è‚É£ Null Counts:\n",
      "+---+----------+-----+---------+---+----+------+\n",
      "|age|department|email|hire_date| id|name|salary|\n",
      "+---+----------+-----+---------+---+----+------+\n",
      "|  0|         0|    0|        0|  0|   0|     0|\n",
      "+---+----------+-----+---------+---+----+------+\n",
      "\n",
      "\n",
      "5Ô∏è‚É£ Numeric Statistics:\n",
      "+-------+------------------+------------------+\n",
      "|summary|            salary|               age|\n",
      "+-------+------------------+------------------+\n",
      "|  count|              1000|              1000|\n",
      "|   mean|         95062.939|            43.882|\n",
      "| stddev|31780.593327493538|12.599079839925327|\n",
      "|    min|             40371|                22|\n",
      "|    max|            149579|                65|\n",
      "+-------+------------------+------------------+\n",
      "\n",
      "\n",
      "6Ô∏è‚É£ Department Distribution:\n",
      "+-----------+-----+\n",
      "| department|count|\n",
      "+-----------+-----+\n",
      "|  Marketing|  218|\n",
      "|    Finance|  216|\n",
      "|         HR|  204|\n",
      "|Engineering|  186|\n",
      "|      Sales|  176|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç DATA QUALITY CHECKS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Use the clean parquet data\n",
    "df = df_parquet\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ Basic Statistics:\")\n",
    "print(f\"   Total rows: {df.count()}\")\n",
    "print(f\"   Total columns: {len(df.columns)}\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ Column Names:\")\n",
    "print(f\"   {', '.join(df.columns)}\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ Data Types:\")\n",
    "for field in df.schema.fields:\n",
    "    print(f\"   {field.name:<15} {str(field.dataType):<20} nullable={field.nullable}\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ Null Counts:\")\n",
    "null_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "null_counts.show()\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£ Numeric Statistics:\")\n",
    "df.select(\"salary\", \"age\").describe().show()\n",
    "\n",
    "print(\"\\n6Ô∏è‚É£ Department Distribution:\")\n",
    "df.groupBy(\"department\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìù PART 12: BEST PRACTICES SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö BEST PRACTICES FOR READING DATA IN SPARK\n",
      "======================================================================\n",
      "\n",
      "‚úÖ DO:\n",
      "1. Always use explicit schema (avoid inferSchema in production)\n",
      "2. Use Parquet format for production workloads\n",
      "3. Use PERMISSIVE mode with _corrupt_record column to track bad data\n",
      "4. Partition large datasets appropriately\n",
      "5. Use MinIO/S3 for scalable storage\n",
      "6. Set appropriate read options (dateFormat, nullValue, etc.)\n",
      "7. Validate data quality after reading\n",
      "8. Use columnar formats (Parquet, ORC) for analytics\n",
      "\n",
      "‚ùå DON'T:\n",
      "1. Don't use inferSchema on large datasets (slow!)\n",
      "2. Don't use CSV/JSON in production (use Parquet)\n",
      "3. Don't ignore corrupted records (use PERMISSIVE to track them)\n",
      "4. Don't read all columns if you only need a few (use select)\n",
      "5. Don't use FAILFAST without proper error handling\n",
      "6. Don't store data on local filesystem in production (use S3/HDFS)\n",
      "\n",
      "üéØ PERFORMANCE TIPS:\n",
      "1. Parquet > ORC > JSON > CSV (in terms of speed)\n",
      "2. Explicit schema > Schema inference (2-3x faster)\n",
      "3. Predicate pushdown works best with Parquet\n",
      "4. Use partitioning for large datasets\n",
      "5. Coalesce/repartition after reading if needed\n",
      "\n",
      "üìä FORMAT RECOMMENDATIONS:\n",
      "‚Ä¢ Development/Testing: CSV (easy to inspect)\n",
      "‚Ä¢ Production: Parquet (fast, compressed, schema-embedded)\n",
      "‚Ä¢ Streaming: JSON (flexible schema)\n",
      "‚Ä¢ Data Lake: Parquet with partitioning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üìö BEST PRACTICES FOR READING DATA IN SPARK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_practices = \"\"\"\n",
    "‚úÖ DO:\n",
    "1. Always use explicit schema (avoid inferSchema in production)\n",
    "2. Use Parquet format for production workloads\n",
    "3. Use PERMISSIVE mode with _corrupt_record column to track bad data\n",
    "4. Partition large datasets appropriately\n",
    "5. Use MinIO/S3 for scalable storage\n",
    "6. Set appropriate read options (dateFormat, nullValue, etc.)\n",
    "7. Validate data quality after reading\n",
    "8. Use columnar formats (Parquet, ORC) for analytics\n",
    "\n",
    "‚ùå DON'T:\n",
    "1. Don't use inferSchema on large datasets (slow!)\n",
    "2. Don't use CSV/JSON in production (use Parquet)\n",
    "3. Don't ignore corrupted records (use PERMISSIVE to track them)\n",
    "4. Don't read all columns if you only need a few (use select)\n",
    "5. Don't use FAILFAST without proper error handling\n",
    "6. Don't store data on local filesystem in production (use S3/HDFS)\n",
    "\n",
    "üéØ PERFORMANCE TIPS:\n",
    "1. Parquet > ORC > JSON > CSV (in terms of speed)\n",
    "2. Explicit schema > Schema inference (2-3x faster)\n",
    "3. Predicate pushdown works best with Parquet\n",
    "4. Use partitioning for large datasets\n",
    "5. Coalesce/repartition after reading if needed\n",
    "\n",
    "üìä FORMAT RECOMMENDATIONS:\n",
    "‚Ä¢ Development/Testing: CSV (easy to inspect)\n",
    "‚Ä¢ Production: Parquet (fast, compressed, schema-embedded)\n",
    "‚Ä¢ Streaming: JSON (flexible schema)\n",
    "‚Ä¢ Data Lake: Parquet with partitioning\n",
    "\"\"\"\n",
    "\n",
    "print(best_practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéì PART 13: EXERCISES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéì EXERCISES\n",
      "======================================================================\n",
      "\n",
      "üìù Exercise 1: Schema Definition\n",
      "Define an explicit schema for a transactions dataset with:\n",
      "- transaction_id (integer)\n",
      "- customer_id (integer)\n",
      "- amount (double)\n",
      "- transaction_date (date)\n",
      "- status (string)\n",
      "\n",
      "üìù Exercise 2: Read with Options\n",
      "Read a CSV file with:\n",
      "- Custom delimiter (|)\n",
      "- Custom null value (\"N/A\")\n",
      "- Custom date format (\"dd/MM/yyyy\")\n",
      "- DROPMALFORMED mode\n",
      "\n",
      "üìù Exercise 3: Performance Test\n",
      "Compare read performance of:\n",
      "- CSV with inferSchema\n",
      "- CSV with explicit schema\n",
      "- Parquet\n",
      "Measure time for reading and counting rows.\n",
      "\n",
      "üìù Exercise 4: Corrupted Data Handling\n",
      "Create a CSV with intentional errors and:\n",
      "- Read with PERMISSIVE mode\n",
      "- Identify corrupted records\n",
      "- Save clean records to staging\n",
      "- Save corrupted records to error log\n",
      "\n",
      "üìù Exercise 5: Multi-file Reading\n",
      "Create 5 CSV files with different date ranges and:\n",
      "- Read all files at once\n",
      "- Filter by date range\n",
      "- Aggregate by month\n",
      "- Write result to Parquet\n",
      "\n",
      "üìù Exercise 6: S3 Integration\n",
      "- Write data to MinIO in Parquet format\n",
      "- Read it back\n",
      "- Perform transformations\n",
      "- Write result to different S3 bucket\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üéì EXERCISES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "exercises = \"\"\"\n",
    "üìù Exercise 1: Schema Definition\n",
    "Define an explicit schema for a transactions dataset with:\n",
    "- transaction_id (integer)\n",
    "- customer_id (integer)\n",
    "- amount (double)\n",
    "- transaction_date (date)\n",
    "- status (string)\n",
    "\n",
    "üìù Exercise 2: Read with Options\n",
    "Read a CSV file with:\n",
    "- Custom delimiter (|)\n",
    "- Custom null value (\"N/A\")\n",
    "- Custom date format (\"dd/MM/yyyy\")\n",
    "- DROPMALFORMED mode\n",
    "\n",
    "üìù Exercise 3: Performance Test\n",
    "Compare read performance of:\n",
    "- CSV with inferSchema\n",
    "- CSV with explicit schema\n",
    "- Parquet\n",
    "Measure time for reading and counting rows.\n",
    "\n",
    "üìù Exercise 4: Corrupted Data Handling\n",
    "Create a CSV with intentional errors and:\n",
    "- Read with PERMISSIVE mode\n",
    "- Identify corrupted records\n",
    "- Save clean records to staging\n",
    "- Save corrupted records to error log\n",
    "\n",
    "üìù Exercise 5: Multi-file Reading\n",
    "Create 5 CSV files with different date ranges and:\n",
    "- Read all files at once\n",
    "- Filter by date range\n",
    "- Aggregate by month\n",
    "- Write result to Parquet\n",
    "\n",
    "üìù Exercise 6: S3 Integration\n",
    "- Write data to MinIO in Parquet format\n",
    "- Read it back\n",
    "- Perform transformations\n",
    "- Write result to different S3 bucket\n",
    "\"\"\"\n",
    "\n",
    "print(exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üßπ CLEANUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleanup (optional)\n",
      "======================================================================\n",
      "\n",
      "To clean up test data, uncomment and run:\n",
      "\n",
      "# import shutil\n",
      "# shutil.rmtree(f\"{DATA_RAW_LOCAL}/employees\", ignore_errors=True)\n",
      "# shutil.rmtree(f\"{DATA_RAW_LOCAL}/employees_multi\", ignore_errors=True)\n",
      "# print(\"‚úÖ Test data cleaned up\")\n",
      "\n",
      "\n",
      "‚úÖ Notebook completed successfully!\n",
      "\n",
      "üìö Next: Notebook 2 - Writing Data\n"
     ]
    }
   ],
   "source": [
    "print(\"üßπ Cleanup (optional)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nTo clean up test data, uncomment and run:\")\n",
    "print(\"\"\"\\n# import shutil\n",
    "# shutil.rmtree(f\"{DATA_RAW_LOCAL}/employees\", ignore_errors=True)\n",
    "# shutil.rmtree(f\"{DATA_RAW_LOCAL}/employees_multi\", ignore_errors=True)\n",
    "# print(\"‚úÖ Test data cleaned up\")\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úÖ Notebook completed successfully!\")\n",
    "print(\"\\nüìö Next: Notebook 2 - Writing Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
