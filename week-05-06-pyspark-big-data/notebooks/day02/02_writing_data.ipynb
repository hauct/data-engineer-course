{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò NOTEBOOK 2: WRITING DATA IN PYSPARK\n",
    "\n",
    "## Topics Covered:\n",
    "- Writing to different formats (CSV, JSON, Parquet, ORC)\n",
    "- Write modes (overwrite, append, ignore, error)\n",
    "- Partitioning strategies\n",
    "- Compression options\n",
    "- Performance optimization\n",
    "- Best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Spark Session with MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/03 14:40:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark Session Created\n",
      "   Spark Version: 3.5.1\n",
      "   Application ID: local-1767451218565\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "import builtins\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"02-Writing-Data\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark Session Created\")\n",
    "print(f\"   Spark Version: {spark.version}\")\n",
    "print(f\"   Application ID: {spark.sparkContext.applicationId}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìä CREATING SAMPLE DATASET\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created DataFrame with 10,000 rows\n",
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      "\n",
      "+--------+--------+-----------+----------+---------+------+------------+--------+---------+------------+----+-----+---+\n",
      "|category|    city|customer_id|order_date| order_id| price|product_name|quantity|   status|total_amount|year|month|day|\n",
      "+--------+--------+-----------+----------+---------+------+------------+--------+---------+------------+----+-----+---+\n",
      "|    Toys|Haiphong|   CUST0406|2024-09-12|ORD000000|196.28|  Product_54|       4|  pending|      785.12|2024|    9| 12|\n",
      "|    Toys|     HCM|   CUST0841|2024-07-14|ORD000001|201.43|  Product_98|       7|  pending|     1410.01|2024|    7| 14|\n",
      "|   Books|  Cantho|   CUST0311|2024-06-29|ORD000002|  83.0|  Product_85|       1|  pending|        83.0|2024|    6| 29|\n",
      "|    Food|Haiphong|   CUST0738|2024-01-24|ORD000003|789.33|  Product_89|      10|cancelled|      7893.3|2024|    1| 24|\n",
      "|    Food|  Cantho|   CUST0386|2024-02-12|ORD000004|390.76|  Product_17|       8|  pending|     3126.08|2024|    2| 12|\n",
      "+--------+--------+-----------+----------+---------+------+------------+--------+---------+------------+----+-----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üìä CREATING SAMPLE DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Generate sample data\n",
    "data = []\n",
    "start_date = datetime(2024, 1, 1)\n",
    "categories = [\"Electronics\", \"Clothing\", \"Food\", \"Books\", \"Toys\"]\n",
    "cities = [\"Hanoi\", \"HCM\", \"Danang\", \"Haiphong\", \"Cantho\"]\n",
    "\n",
    "for i in range(10000):\n",
    "    data.append({\n",
    "        \"order_id\": f\"ORD{i:06d}\",\n",
    "        \"customer_id\": f\"CUST{random.randint(1, 1000):04d}\",\n",
    "        \"product_name\": f\"Product_{random.randint(1, 100)}\",\n",
    "        \"category\": random.choice(categories),\n",
    "        \"quantity\": random.randint(1, 10),\n",
    "        \"price\": builtins.round(random.uniform(10, 1000), 2),\n",
    "        \"city\": random.choice(cities),\n",
    "        \"order_date\": (start_date + timedelta(days=random.randint(0, 365))).strftime(\"%Y-%m-%d\"),\n",
    "        \"status\": random.choice([\"completed\", \"pending\", \"cancelled\"])\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Add calculated columns\n",
    "df = df.withColumn(\"total_amount\", col(\"quantity\") * col(\"price\")) \\\n",
    "       .withColumn(\"year\", year(col(\"order_date\"))) \\\n",
    "       .withColumn(\"month\", month(col(\"order_date\"))) \\\n",
    "       .withColumn(\"day\", dayofmonth(col(\"order_date\")))\n",
    "\n",
    "print(f\"‚úÖ Created DataFrame with {df.count():,} rows\")\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Writing to Different Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üíæ WRITING TO DIFFERENT FORMATS\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üíæ WRITING TO DIFFERENT FORMATS\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Writing to CSV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ CSV written in 6.38s\n",
      "   üìä Rows: 10,000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìù Writing to CSV...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .option(\"quote\", '\"') \\\n",
    "    .option(\"escape\", \"\\\\\") \\\n",
    "    .option(\"nullValue\", \"NULL\") \\\n",
    "    .csv(\"/opt/spark-data/staging/orders_csv\")\n",
    "\n",
    "csv_time = time.time() - start_time\n",
    "print(f\"   ‚úÖ CSV written in {csv_time:.2f}s\")\n",
    "\n",
    "# Verify\n",
    "csv_df = spark.read.option(\"header\", \"true\").csv(\"/opt/spark-data/staging/orders_csv\")\n",
    "print(f\"   üìä Rows: {csv_df.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Write to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Writing to JSON...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ JSON written in 2.17s\n",
      "   üìä Rows: 10,000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìù Writing to JSON...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"compression\", \"none\") \\\n",
    "    .json(\"/opt/spark-data/staging/orders_json\")\n",
    "\n",
    "json_time = time.time() - start_time\n",
    "print(f\"   ‚úÖ JSON written in {json_time:.2f}s\")\n",
    "\n",
    "# Verify\n",
    "json_df = spark.read.json(\"/opt/spark-data/staging/orders_json\")\n",
    "print(f\"   üìä Rows: {json_df.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Write to Parquet (RECOMMENDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Writing to Parquet...\n",
      "   ‚úÖ Parquet written in 1.01s\n",
      "   üìä Rows: 10,000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìù Writing to Parquet...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df.coalesce(4)\\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .parquet(\"/opt/spark-data/staging/orders_parquet\")\n",
    "\n",
    "parquet_time = time.time() - start_time\n",
    "print(f\"   ‚úÖ Parquet written in {parquet_time:.2f}s\")\n",
    "\n",
    "# Verify\n",
    "parquet_df = spark.read.parquet(\"/opt/spark-data/staging/orders_parquet\")\n",
    "print(f\"   üìä Rows: {parquet_df.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Write to ORC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Writing to ORC...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ ORC written in 2.95s\n",
      "   üìä Rows: 10,000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìù Writing to ORC...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .orc(\"/opt/spark-data/staging/orders_orc\")\n",
    "\n",
    "orc_time = time.time() - start_time\n",
    "print(f\"   ‚úÖ ORC written in {orc_time:.2f}s\")\n",
    "\n",
    "# Verify\n",
    "orc_df = spark.read.orc(\"/opt/spark-data/staging/orders_orc\")\n",
    "print(f\"   üìä Rows: {orc_df.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚ö° WRITE PERFORMANCE COMPARISON\n",
      "================================================================================\n",
      "Format     Time (s)     Size (MB)    Speed     \n",
      "--------------------------------------------------\n",
      "CSV        6.38         0.89         üêå\n",
      "JSON       2.17         2.34         üêå\n",
      "Parquet    1.01         0.24         ‚ö°\n",
      "ORC        2.95         0.32         üêå\n",
      "\n",
      "üí° Key Insights:\n",
      "   - Parquet: Fastest + Smallest (columnar, compressed)\n",
      "   - ORC: Similar to Parquet (optimized for Hive)\n",
      "   - JSON: Slowest + Largest (human-readable)\n",
      "   - CSV: Medium speed, no schema preservation\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚ö° WRITE PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import os\n",
    "\n",
    "def get_dir_size(path):\n",
    "    \"\"\"Get directory size in MB\"\"\"\n",
    "    total = 0\n",
    "    for entry in os.scandir(path):\n",
    "        if entry.is_file():\n",
    "            total += entry.stat().st_size\n",
    "        elif entry.is_dir():\n",
    "            total += get_dir_size(entry.path)\n",
    "    return total / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "formats = {\n",
    "    \"CSV\": (\"/opt/spark-data/staging/orders_csv\", csv_time),\n",
    "    \"JSON\": (\"/opt/spark-data/staging/orders_json\", json_time),\n",
    "    \"Parquet\": (\"/opt/spark-data/staging/orders_parquet\", parquet_time),\n",
    "    \"ORC\": (\"/opt/spark-data/staging/orders_orc\", orc_time)\n",
    "}\n",
    "\n",
    "print(f\"{'Format':<10} {'Time (s)':<12} {'Size (MB)':<12} {'Speed':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for fmt, (path, write_time) in formats.items():\n",
    "    size = get_dir_size(path)\n",
    "    print(f\"{fmt:<10} {write_time:<12.2f} {size:<12.2f} {'‚ö°' if write_time < 2 else 'üêå'}\")\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"   - Parquet: Fastest + Smallest (columnar, compressed)\")\n",
    "print(\"   - ORC: Similar to Parquet (optimized for Hive)\")\n",
    "print(\"   - JSON: Slowest + Largest (human-readable)\")\n",
    "print(\"   - CSV: Medium speed, no schema preservation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Write Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîÑ WRITE MODES\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîÑ WRITE MODES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a small test dataset\n",
    "test_df = spark.createDataFrame([\n",
    "    (1, \"Alice\", 25),\n",
    "    (2, \"Bob\", 30),\n",
    "    (3, \"Charlie\", 35)\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "test_path = \"/opt/spark-data/staging/test_modes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Mode: OVERWRITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1Ô∏è‚É£ Mode: OVERWRITE\n",
      "   - Deletes existing data and writes new data\n",
      "   ‚úÖ First write: 3 rows\n",
      "   ‚úÖ After overwrite: 2 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1Ô∏è‚É£ Mode: OVERWRITE\")\n",
    "print(\"   - Deletes existing data and writes new data\")\n",
    "\n",
    "# First write\n",
    "test_df.write.mode(\"overwrite\").parquet(test_path)\n",
    "print(f\"   ‚úÖ First write: {spark.read.parquet(test_path).count()} rows\")\n",
    "\n",
    "# Second write (overwrites)\n",
    "test_df.limit(2).write.mode(\"overwrite\").parquet(test_path)\n",
    "print(f\"   ‚úÖ After overwrite: {spark.read.parquet(test_path).count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Mode: APPEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2Ô∏è‚É£ Mode: APPEND\n",
      "   - Adds new data to existing data\n",
      "   ‚úÖ After append: 4 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2Ô∏è‚É£ Mode: APPEND\")\n",
    "print(\"   - Adds new data to existing data\")\n",
    "\n",
    "# Append more data\n",
    "new_df = spark.createDataFrame([\n",
    "    (4, \"David\", 40),\n",
    "    (5, \"Eve\", 45)\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "new_df.write.mode(\"append\").parquet(test_path)\n",
    "print(f\"   ‚úÖ After append: {spark.read.parquet(test_path).count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Mode: IGNORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3Ô∏è‚É£ Mode: IGNORE\n",
      "   - Does nothing if data already exists\n",
      "   ‚úÖ After ignore: 4 rows (unchanged)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n3Ô∏è‚É£ Mode: IGNORE\")\n",
    "print(\"   - Does nothing if data already exists\")\n",
    "\n",
    "# Try to write (will be ignored)\n",
    "test_df.write.mode(\"ignore\").parquet(test_path)\n",
    "print(f\"   ‚úÖ After ignore: {spark.read.parquet(test_path).count()} rows (unchanged)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Mode: ERROR (Default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4Ô∏è‚É£ Mode: ERROR\n",
      "   - Throws error if data already exists\n",
      "   ‚úÖ Expected error: [PATH_ALREADY_EXISTS] Path file:/opt/spark-data/st...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n4Ô∏è‚É£ Mode: ERROR\")\n",
    "print(\"   - Throws error if data already exists\")\n",
    "\n",
    "try:\n",
    "    test_df.write.mode(\"error\").parquet(test_path)\n",
    "    print(\"   ‚ùå Should have thrown error!\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚úÖ Expected error: {str(e)[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Partitioning Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üóÇÔ∏è  PARTITIONING STRATEGIES\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üóÇÔ∏è  PARTITIONING STRATEGIES\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Partition by Single Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1Ô∏è‚É£ Partition by Single Column (category)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Partitioned by category\n",
      "   üìÅ Directory structure:\n",
      "      orders_partitioned_category/\n",
      "      ‚îú‚îÄ‚îÄ category=Electronics/\n",
      "      ‚îú‚îÄ‚îÄ category=Clothing/\n",
      "      ‚îú‚îÄ‚îÄ category=Food/\n",
      "      ‚îî‚îÄ‚îÄ ...\n",
      "   üìä Total rows: 10,000\n",
      "   üìä Partitions: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1Ô∏è‚É£ Partition by Single Column (category)\")\n",
    "\n",
    "df.coalesce(4)\\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"category\") \\\n",
    "    .parquet(\"/opt/spark-data/staging/orders_partitioned_category\")\n",
    "\n",
    "print(\"   ‚úÖ Partitioned by category\")\n",
    "print(\"   üìÅ Directory structure:\")\n",
    "print(\"      orders_partitioned_category/\")\n",
    "print(\"      ‚îú‚îÄ‚îÄ category=Electronics/\")\n",
    "print(\"      ‚îú‚îÄ‚îÄ category=Clothing/\")\n",
    "print(\"      ‚îú‚îÄ‚îÄ category=Food/\")\n",
    "print(\"      ‚îî‚îÄ‚îÄ ...\")\n",
    "\n",
    "# Verify partitions\n",
    "partitioned_df = spark.read.parquet(\"/opt/spark-data/staging/orders_partitioned_category\")\n",
    "print(f\"   üìä Total rows: {partitioned_df.count():,}\")\n",
    "print(f\"   üìä Partitions: {partitioned_df.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Partition by Multiple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2Ô∏è‚É£ Partition by Multiple Columns (year, month)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Partitioned by year and month\n",
      "   üìÅ Directory structure:\n",
      "      orders_partitioned_date/\n",
      "      ‚îú‚îÄ‚îÄ year=2024/\n",
      "      ‚îÇ   ‚îú‚îÄ‚îÄ month=1/\n",
      "      ‚îÇ   ‚îú‚îÄ‚îÄ month=2/\n",
      "      ‚îÇ   ‚îî‚îÄ‚îÄ ...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2Ô∏è‚É£ Partition by Multiple Columns (year, month)\")\n",
    "\n",
    "df.coalesce(4)\\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .parquet(\"/opt/spark-data/staging/orders_partitioned_date\")\n",
    "\n",
    "print(\"   ‚úÖ Partitioned by year and month\")\n",
    "print(\"   üìÅ Directory structure:\")\n",
    "print(\"      orders_partitioned_date/\")\n",
    "print(\"      ‚îú‚îÄ‚îÄ year=2024/\")\n",
    "print(\"      ‚îÇ   ‚îú‚îÄ‚îÄ month=1/\")\n",
    "print(\"      ‚îÇ   ‚îú‚îÄ‚îÄ month=2/\")\n",
    "print(\"      ‚îÇ   ‚îî‚îÄ‚îÄ ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Partition Pruning (Query Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3Ô∏è‚É£ Partition Pruning\n",
      "   ‚úÖ Query with partition filter: 896 rows in 1.508s\n",
      "   ‚úÖ Query without partition: 896 rows in 0.501s\n",
      "   ‚ö° Speedup: 0.33x faster with partitioning!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n3Ô∏è‚É£ Partition Pruning\")\n",
    "\n",
    "# Query with partition filter\n",
    "start_time = time.time()\n",
    "result = spark.read.parquet(\"/opt/spark-data/staging/orders_partitioned_date\") \\\n",
    "    .filter((col(\"year\") == 2024) & (col(\"month\") == 1)) \\\n",
    "    .count()\n",
    "pruned_time = time.time() - start_time\n",
    "\n",
    "print(f\"   ‚úÖ Query with partition filter: {result:,} rows in {pruned_time:.3f}s\")\n",
    "\n",
    "# Query without partition filter\n",
    "start_time = time.time()\n",
    "result = spark.read.parquet(\"/opt/spark-data/staging/orders_parquet\") \\\n",
    "    .filter((year(col(\"order_date\")) == 2024) & (month(col(\"order_date\")) == 1)) \\\n",
    "    .count()\n",
    "full_scan_time = time.time() - start_time\n",
    "\n",
    "print(f\"   ‚úÖ Query without partition: {result:,} rows in {full_scan_time:.3f}s\")\n",
    "print(f\"   ‚ö° Speedup: {full_scan_time/pruned_time:.2f}x faster with partitioning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compression Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üóúÔ∏è  COMPRESSION OPTIONS\n",
      "================================================================================\n",
      "\n",
      "üì¶ Testing NONE compression...\n",
      "   ‚úÖ Write time: 1.88s\n",
      "   üìä Size: 0.57 MB\n",
      "\n",
      "üì¶ Testing SNAPPY compression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/03 11:36:39 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "26/01/03 11:36:39 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "26/01/03 11:36:39 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "26/01/03 11:36:39 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "26/01/03 11:36:39 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "26/01/03 11:36:39 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "26/01/03 11:36:39 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Write time: 1.72s\n",
      "   üìä Size: 0.33 MB\n",
      "\n",
      "üì¶ Testing GZIP compression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/03 11:36:40 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "26/01/03 11:36:41 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Write time: 5.31s\n",
      "   üìä Size: 0.25 MB\n",
      "\n",
      "üì¶ Testing LZ4 compression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/03 11:36:46 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "26/01/03 11:36:46 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Write time: 1.86s\n",
      "   üìä Size: 0.32 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üóúÔ∏è  COMPRESSION OPTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test different compression codecs\n",
    "compressions = [\"none\", \"snappy\", \"gzip\", \"lz4\"]\n",
    "compression_results = {}\n",
    "\n",
    "for codec in compressions:\n",
    "    print(f\"\\nüì¶ Testing {codec.upper()} compression...\")\n",
    "    \n",
    "    path = f\"/opt/spark-data/staging/orders_compressed_{codec}\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    df.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"compression\", codec) \\\n",
    "        .parquet(path)\n",
    "    write_time = time.time() - start_time\n",
    "    \n",
    "    size = get_dir_size(path)\n",
    "    compression_results[codec] = (write_time, size)\n",
    "    \n",
    "    print(f\"   ‚úÖ Write time: {write_time:.2f}s\")\n",
    "    print(f\"   üìä Size: {size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Compression Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä COMPRESSION COMPARISON\n",
      "================================================================================\n",
      "Codec      Time (s)     Size (MB)    Ratio     \n",
      "--------------------------------------------------\n",
      "none       1.88         0.57         1.00      x\n",
      "snappy     1.72         0.33         1.73      x\n",
      "gzip       5.31         0.25         2.29      x\n",
      "lz4        1.86         0.32         1.78      x\n",
      "\n",
      "üí° Compression Recommendations:\n",
      "   - snappy: Best balance (fast + good compression) ‚≠ê\n",
      "   - gzip: Best compression (slower write)\n",
      "   - lz4: Fastest (moderate compression)\n",
      "   - none: No compression (largest size)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä COMPRESSION COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Codec':<10} {'Time (s)':<12} {'Size (MB)':<12} {'Ratio':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "baseline_size = compression_results[\"none\"][1]\n",
    "for codec, (write_time, size) in compression_results.items():\n",
    "    ratio = baseline_size / size if size > 0 else 0\n",
    "    print(f\"{codec:<10} {write_time:<12.2f} {size:<12.2f} {ratio:<10.2f}x\")\n",
    "\n",
    "print(\"\\nüí° Compression Recommendations:\")\n",
    "print(\"   - snappy: Best balance (fast + good compression) ‚≠ê\")\n",
    "print(\"   - gzip: Best compression (slower write)\")\n",
    "print(\"   - lz4: Fastest (moderate compression)\")\n",
    "print(\"   - none: No compression (largest size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚ö° PERFORMANCE OPTIMIZATION\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚ö° PERFORMANCE OPTIMIZATION\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Coalesce - Reduce Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1Ô∏è‚É£ Coalesce (Reduce Partitions)\n",
      "   Original partitions: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/03 11:36:48 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ùå Without coalesce: 16 files in 1.72s\n",
      "   ‚úÖ With coalesce(4): 4 files in 0.66s\n",
      "   üí° Fewer files = Faster reads!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1Ô∏è‚É£ Coalesce (Reduce Partitions)\")\n",
    "\n",
    "print(f\"   Original partitions: {df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Write with many small files (bad)\n",
    "start_time = time.time()\n",
    "df.write.mode(\"overwrite\").parquet(\"/opt/spark-data/staging/orders_many_files\")\n",
    "many_files_time = time.time() - start_time\n",
    "many_files_count = len([f for f in os.listdir(\"/opt/spark-data/staging/orders_many_files\") if f.endswith(\".parquet\")])\n",
    "\n",
    "print(f\"   ‚ùå Without coalesce: {many_files_count} files in {many_files_time:.2f}s\")\n",
    "\n",
    "# Write with fewer large files (good)\n",
    "start_time = time.time()\n",
    "df.coalesce(4).write.mode(\"overwrite\").parquet(\"/opt/spark-data/staging/orders_few_files\")\n",
    "few_files_time = time.time() - start_time\n",
    "few_files_count = len([f for f in os.listdir(\"/opt/spark-data/staging/orders_few_files\") if f.endswith(\".parquet\")])\n",
    "\n",
    "print(f\"   ‚úÖ With coalesce(4): {few_files_count} files in {few_files_time:.2f}s\")\n",
    "print(f\"   üí° Fewer files = Faster reads!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Repartition - Increase Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2Ô∏è‚É£ Repartition (Increase Partitions)\n",
      "   ‚úÖ Repartitioned to 16 partitions\n",
      "   üí° More partitions = More parallelism (for large data)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2Ô∏è‚É£ Repartition (Increase Partitions)\")\n",
    "\n",
    "# For large datasets, increase partitions for parallel processing\n",
    "large_df = df.repartition(16)\n",
    "print(f\"   ‚úÖ Repartitioned to {large_df.rdd.getNumPartitions()} partitions\")\n",
    "print(f\"   üí° More partitions = More parallelism (for large data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Bucketing (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3Ô∏è‚É£ Bucketing (Advanced)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Data bucketed by customer_id (10 buckets)\n",
      "   üí° Bucketing optimizes joins and aggregations\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n3Ô∏è‚É£ Bucketing (Advanced)\")\n",
    "\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .bucketBy(10, \"customer_id\") \\\n",
    "    .sortBy(\"order_date\") \\\n",
    "    .saveAsTable(\"orders_bucketed\")\n",
    "\n",
    "print(\"   ‚úÖ Data bucketed by customer_id (10 buckets)\")\n",
    "print(\"   üí° Bucketing optimizes joins and aggregations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Writing to MinIO (S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚òÅÔ∏è  WRITING TO MINIO (S3)\n",
      "================================================================================\n",
      "\n",
      "üì§ Writing to MinIO...\n",
      "   ‚úÖ Successfully written to s3a://staging/orders/\n",
      "   üìä Rows in MinIO: 10,000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚òÅÔ∏è  WRITING TO MINIO (S3)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Write to MinIO staging bucket\n",
    "print(\"\\nüì§ Writing to MinIO...\")\n",
    "\n",
    "try:\n",
    "    df.coalesce(4)\\\n",
    "        .write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy(\"category\") \\\n",
    "        .option(\"compression\", \"snappy\") \\\n",
    "        .parquet(\"s3a://staging/orders/\")\n",
    "    \n",
    "    print(\"   ‚úÖ Successfully written to s3a://staging/orders/\")\n",
    "    \n",
    "    # Verify\n",
    "    minio_df = spark.read.parquet(\"s3a://staging/orders/\")\n",
    "    print(f\"   üìä Rows in MinIO: {minio_df.count():,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error writing to MinIO: {str(e)}\")\n",
    "    print(\"   üí° Make sure MinIO is running and buckets are created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚úÖ BEST PRACTICES FOR WRITING DATA\n",
      "================================================================================\n",
      "\n",
      "1. üìÅ FORMAT SELECTION:\n",
      "   - Use Parquet for production (best compression + performance)\n",
      "   - Use CSV only for human-readable exports\n",
      "   - Use JSON for semi-structured data\n",
      "   - Use ORC if integrating with Hive\n",
      "\n",
      "2. üîÑ WRITE MODES:\n",
      "   - Use 'overwrite' for full refreshes\n",
      "   - Use 'append' for incremental loads\n",
      "   - Use 'ignore' for idempotent writes\n",
      "   - Avoid 'error' mode in production\n",
      "\n",
      "3. üóÇÔ∏è  PARTITIONING:\n",
      "   - Partition by low-cardinality columns (date, category, region)\n",
      "   - Avoid over-partitioning (< 1GB per partition)\n",
      "   - Use partition pruning in queries\n",
      "   - Consider bucketing for joins\n",
      "\n",
      "4. üóúÔ∏è  COMPRESSION:\n",
      "   - Use snappy for balanced performance (default)\n",
      "   - Use gzip for maximum compression\n",
      "   - Use lz4 for fastest writes\n",
      "   - Avoid 'none' in production\n",
      "\n",
      "5. ‚ö° PERFORMANCE:\n",
      "   - Use coalesce() to reduce small files\n",
      "   - Use repartition() for even distribution\n",
      "   - Aim for 128MB - 1GB per file\n",
      "   - Monitor partition count\n",
      "\n",
      "6. üèóÔ∏è  DATA ORGANIZATION:\n",
      "   - Use consistent naming conventions\n",
      "   - Organize by environment (raw/staging/production)\n",
      "   - Use date-based partitions for time-series\n",
      "   - Document partition strategy\n",
      "\n",
      "7. üîí DATA QUALITY:\n",
      "   - Validate before writing\n",
      "   - Use explicit schemas\n",
      "   - Handle nulls appropriately\n",
      "   - Add metadata (write timestamp, source, etc.)\n",
      "\n",
      "8. üìä MONITORING:\n",
      "   - Track write times\n",
      "   - Monitor file sizes\n",
      "   - Check partition distribution\n",
      "   - Log write operations\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ BEST PRACTICES FOR WRITING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_practices = \"\"\"\n",
    "1. üìÅ FORMAT SELECTION:\n",
    "   - Use Parquet for production (best compression + performance)\n",
    "   - Use CSV only for human-readable exports\n",
    "   - Use JSON for semi-structured data\n",
    "   - Use ORC if integrating with Hive\n",
    "\n",
    "2. üîÑ WRITE MODES:\n",
    "   - Use 'overwrite' for full refreshes\n",
    "   - Use 'append' for incremental loads\n",
    "   - Use 'ignore' for idempotent writes\n",
    "   - Avoid 'error' mode in production\n",
    "\n",
    "3. üóÇÔ∏è  PARTITIONING:\n",
    "   - Partition by low-cardinality columns (date, category, region)\n",
    "   - Avoid over-partitioning (< 1GB per partition)\n",
    "   - Use partition pruning in queries\n",
    "   - Consider bucketing for joins\n",
    "\n",
    "4. üóúÔ∏è  COMPRESSION:\n",
    "   - Use snappy for balanced performance (default)\n",
    "   - Use gzip for maximum compression\n",
    "   - Use lz4 for fastest writes\n",
    "   - Avoid 'none' in production\n",
    "\n",
    "5. ‚ö° PERFORMANCE:\n",
    "   - Use coalesce() to reduce small files\n",
    "   - Use repartition() for even distribution\n",
    "   - Aim for 128MB - 1GB per file\n",
    "   - Monitor partition count\n",
    "\n",
    "6. üèóÔ∏è  DATA ORGANIZATION:\n",
    "   - Use consistent naming conventions\n",
    "   - Organize by environment (raw/staging/production)\n",
    "   - Use date-based partitions for time-series\n",
    "   - Document partition strategy\n",
    "\n",
    "7. üîí DATA QUALITY:\n",
    "   - Validate before writing\n",
    "   - Use explicit schemas\n",
    "   - Handle nulls appropriately\n",
    "   - Add metadata (write timestamp, source, etc.)\n",
    "\n",
    "8. üìä MONITORING:\n",
    "   - Track write times\n",
    "   - Monitor file sizes\n",
    "   - Check partition distribution\n",
    "   - Log write operations\n",
    "\"\"\"\n",
    "\n",
    "print(best_practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Practical Example: ETL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîß PRACTICAL EXAMPLE: ETL PIPELINE\n",
      "================================================================================\n",
      "\n",
      "üìù Writing orders to production...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Written 10,000 rows to 0 files\n",
      "   üìÅ Location: /opt/spark-data/production/orders\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîß PRACTICAL EXAMPLE: ETL PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def write_to_production(df, table_name, partition_cols=None):\n",
    "    \"\"\"\n",
    "    Write DataFrame to production with best practices\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìù Writing {table_name} to production...\")\n",
    "    \n",
    "    # Add metadata\n",
    "    df_with_metadata = df \\\n",
    "        .withColumn(\"write_timestamp\", current_timestamp()) \\\n",
    "        .withColumn(\"write_date\", current_date())\n",
    "    \n",
    "    # Optimize partitions\n",
    "    optimal_partitions = builtins.max(4, df.rdd.getNumPartitions() // 4)\n",
    "    df_optimized = df_with_metadata.coalesce(optimal_partitions)\n",
    "    \n",
    "    # Write with best practices\n",
    "    writer = df_optimized.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"compression\", \"snappy\")\n",
    "    \n",
    "    if partition_cols:\n",
    "        writer = writer.partitionBy(*partition_cols)\n",
    "    \n",
    "    path = f\"/opt/spark-data/production/{table_name}\"\n",
    "    writer.parquet(path)\n",
    "    \n",
    "    # Verify\n",
    "    result_df = spark.read.parquet(path)\n",
    "    row_count = result_df.count()\n",
    "    file_count = len([f for f in os.listdir(path) if f.endswith(\".parquet\")])\n",
    "    \n",
    "    print(f\"   ‚úÖ Written {row_count:,} rows to {file_count} files\")\n",
    "    print(f\"   üìÅ Location: {path}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Example: Write orders to production\n",
    "production_df = write_to_production(\n",
    "    df, \n",
    "    table_name=\"orders\",\n",
    "    partition_cols=[\"year\", \"month\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìù EXERCISES\n",
      "================================================================================\n",
      "\n",
      "EXERCISE 1: Format Comparison\n",
      "- Create a DataFrame with 50,000 rows\n",
      "- Write to CSV, JSON, Parquet, ORC\n",
      "- Compare write times and file sizes\n",
      "- Which format is best for your use case?\n",
      "\n",
      "EXERCISE 2: Partition Optimization\n",
      "- Load the orders dataset\n",
      "- Partition by different columns (category, city, date)\n",
      "- Measure query performance with partition pruning\n",
      "- Find the optimal partitioning strategy\n",
      "\n",
      "EXERCISE 3: Compression Testing\n",
      "- Write the same data with different compression codecs\n",
      "- Measure write time, file size, and read time\n",
      "- Calculate compression ratio\n",
      "- Recommend best codec for your scenario\n",
      "\n",
      "EXERCISE 4: ETL Pipeline\n",
      "- Read data from raw layer\n",
      "- Apply transformations (filter, aggregate, join)\n",
      "- Write to staging with partitioning\n",
      "- Write to production with optimization\n",
      "- Add data quality checks\n",
      "\n",
      "EXERCISE 5: MinIO Integration\n",
      "- Write data to MinIO staging bucket\n",
      "- Partition by date\n",
      "- Read back and verify\n",
      "- Implement incremental load (append mode)\n",
      "\n",
      "BONUS: Performance Tuning\n",
      "- Create a large dataset (1M+ rows)\n",
      "- Test different partition counts\n",
      "- Measure impact of coalesce/repartition\n",
      "- Find optimal file size (128MB - 1GB)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìù EXERCISES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "exercises = \"\"\"\n",
    "EXERCISE 1: Format Comparison\n",
    "- Create a DataFrame with 50,000 rows\n",
    "- Write to CSV, JSON, Parquet, ORC\n",
    "- Compare write times and file sizes\n",
    "- Which format is best for your use case?\n",
    "\n",
    "EXERCISE 2: Partition Optimization\n",
    "- Load the orders dataset\n",
    "- Partition by different columns (category, city, date)\n",
    "- Measure query performance with partition pruning\n",
    "- Find the optimal partitioning strategy\n",
    "\n",
    "EXERCISE 3: Compression Testing\n",
    "- Write the same data with different compression codecs\n",
    "- Measure write time, file size, and read time\n",
    "- Calculate compression ratio\n",
    "- Recommend best codec for your scenario\n",
    "\n",
    "EXERCISE 4: ETL Pipeline\n",
    "- Read data from raw layer\n",
    "- Apply transformations (filter, aggregate, join)\n",
    "- Write to staging with partitioning\n",
    "- Write to production with optimization\n",
    "- Add data quality checks\n",
    "\n",
    "EXERCISE 5: MinIO Integration\n",
    "- Write data to MinIO staging bucket\n",
    "- Partition by date\n",
    "- Read back and verify\n",
    "- Implement incremental load (append mode)\n",
    "\n",
    "BONUS: Performance Tuning\n",
    "- Create a large dataset (1M+ rows)\n",
    "- Test different partition counts\n",
    "- Measure impact of coalesce/repartition\n",
    "- Find optimal file size (128MB - 1GB)\n",
    "\"\"\"\n",
    "\n",
    "print(exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üßπ CLEANUP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Optional: Clean up test data\n",
    "# import shutil\n",
    "# shutil.rmtree(\"/opt/spark-data/staging/test_modes\", ignore_errors=True)\n",
    "\n",
    "print(\"‚úÖ Notebook completed successfully!\")\n",
    "print(\"\\nüí° Next Steps:\")\n",
    "print(\"   - Try the exercises above\")\n",
    "print(\"   - Experiment with different formats and options\")\n",
    "print(\"   - Move to Notebook 3: Data Cleaning\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Bucketing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP & IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports completed\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "import builtins\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"‚úÖ Imports completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE SPARK SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/03 17:14:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SPARK CONFIGURATION\n",
      "================================================================================\n",
      "Master: spark://spark-master:7077\n",
      "Warehouse: file:/opt/spark-data/warehouse\n",
      "Bucketing: true\n",
      "S3 Endpoint: http://minio:9000\n",
      "Spark UI: http://localhost:4040\n"
     ]
    }
   ],
   "source": [
    "# Stop existing session if any\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"üõë Stopped existing Spark session\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create new session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Bucketing-Performance-Test\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/opt/spark-data/warehouse\") \\\n",
    "    .config(\"spark.sql.sources.bucketing.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SPARK CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Master: {spark.sparkContext.master}\")\n",
    "print(f\"Warehouse: {spark.conf.get('spark.sql.warehouse.dir')}\")\n",
    "print(f\"Bucketing: {spark.conf.get('spark.sql.sources.bucketing.enabled')}\")\n",
    "print(f\"S3 Endpoint: {spark.conf.get('spark.hadoop.fs.s3a.endpoint')}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE LARGE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìä CREATING LARGE DATASET (TARGET: ~2GB)\n",
      "================================================================================\n",
      "\n",
      "üìã Configuration:\n",
      "   Customers:  100,000\n",
      "   Orders:     500,000\n",
      "   Products:   10,000\n",
      "   Categories: 100\n",
      "\n",
      "‚úÖ Helper functions defined\n",
      "\n",
      "================================================================================\n",
      "1Ô∏è‚É£  CREATING CUSTOMERS (100K rows)\n",
      "================================================================================\n",
      "üìù Generating customer data...\n",
      "   Progress: 10,000 / 100,000 (10.0%)\n",
      "   Progress: 20,000 / 100,000 (20.0%)\n",
      "   Progress: 30,000 / 100,000 (30.0%)\n",
      "   Progress: 40,000 / 100,000 (40.0%)\n",
      "   Progress: 50,000 / 100,000 (50.0%)\n",
      "   Progress: 60,000 / 100,000 (60.0%)\n",
      "   Progress: 70,000 / 100,000 (70.0%)\n",
      "   Progress: 80,000 / 100,000 (80.0%)\n",
      "   Progress: 90,000 / 100,000 (90.0%)\n",
      "   Progress: 100,000 / 100,000 (100.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/03 17:14:53 WARN TaskSetManager: Stage 0 contains a task of very large size (2803 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Customers created: 100,000 rows\n",
      "   Estimated size: ~20 MB\n",
      "\n",
      "================================================================================\n",
      "2Ô∏è‚É£  CREATING PRODUCTS (10K rows)\n",
      "================================================================================\n",
      "üìù Generating product data...\n",
      "   Progress: 1,000 / 10,000 (10.0%)\n",
      "   Progress: 2,000 / 10,000 (20.0%)\n",
      "   Progress: 3,000 / 10,000 (30.0%)\n",
      "   Progress: 4,000 / 10,000 (40.0%)\n",
      "   Progress: 5,000 / 10,000 (50.0%)\n",
      "   Progress: 6,000 / 10,000 (60.0%)\n",
      "   Progress: 7,000 / 10,000 (70.0%)\n",
      "   Progress: 8,000 / 10,000 (80.0%)\n",
      "   Progress: 9,000 / 10,000 (90.0%)\n",
      "   Progress: 10,000 / 10,000 (100.0%)\n",
      "\n",
      "‚úÖ Products created: 10,000 rows\n",
      "   Estimated size: ~5 MB\n",
      "\n",
      "================================================================================\n",
      "3Ô∏è‚É£  CREATING ORDERS (5M rows - THIS WILL TAKE A WHILE)\n",
      "================================================================================\n",
      "üìù Generating 500,000 orders in 5 batches...\n",
      "   Batch size: 100,000\n",
      "   ‚úÖ Batch 1/5 completed (100,000 orders)\n",
      "   ‚úÖ Batch 2/5 completed (200,000 orders)\n",
      "   ‚úÖ Batch 3/5 completed (300,000 orders)\n",
      "   ‚úÖ Batch 4/5 completed (400,000 orders)\n",
      "   ‚úÖ Batch 5/5 completed (500,000 orders)\n",
      "\n",
      "üì¶ Combining all batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/03 17:16:04 WARN TaskSetManager: Stage 6 contains a task of very large size (4737 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/01/03 17:16:08 WARN TaskSetManager: Stage 9 contains a task of very large size (2803 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Orders created: 500,000 rows\n",
      "   Estimated size: ~2 GB\n",
      "\n",
      "================================================================================\n",
      "üìä DATASET SUMMARY\n",
      "================================================================================\n",
      "\n",
      "‚úÖ All data created successfully!\n",
      "\n",
      "üìã Dataset Details:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Customers:  100,000 rows (~20 MB)\n",
      "   Products:   10,000 rows (~5 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/03 17:16:09 WARN TaskSetManager: Stage 15 contains a task of very large size (4737 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Orders:     500,000 rows (~2 GB)\n",
      "   Total:      ~2.025 GB\n",
      "\n",
      "üìä Sample Data:\n",
      "\n",
      "Customers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/03 17:16:13 WARN TaskSetManager: Stage 18 contains a task of very large size (2803 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+------------+---------------------+----------+------+---------+----------+----------+--------------+------------+-------+\n",
      "|age|city     |country  |customer_id |email                |first_name|gender|is_active|join_date |last_name |lifetime_value|phone       |segment|\n",
      "+---+---------+---------+------------+---------------------+----------+------+---------+----------+----------+--------------+------------+-------+\n",
      "|26 |Nha Trang|Japan    |CUST00000001|customer1@outlook.com|BCtpAIod  |O     |false    |2021-10-28|uXIAFbyicW|23511.43      |+84981134396|Premium|\n",
      "|51 |Vung Tau |Japan    |CUST00000002|customer2@outlook.com|gQynvwCm  |O     |false    |2024-12-17|lhelGHdgCY|23344.92      |+84916200999|Bronze |\n",
      "|46 |Cantho   |USA      |CUST00000003|customer3@gmail.com  |IWYgaboG  |M     |true     |2020-11-13|qxYprndYvt|43291.44      |+84977613175|Premium|\n",
      "|19 |Nha Trang|Thailand |CUST00000004|customer4@gmail.com  |RzPFPghz  |F     |true     |2023-09-09|vAzXvzdodM|39247.82      |+84975102359|Premium|\n",
      "|66 |Vung Tau |Singapore|CUST00000005|customer5@outlook.com|YQCclWky  |O     |true     |2024-07-25|oavTxAPjph|4898.81       |+84919883904|Premium|\n",
      "+---+---------+---------+------------+---------------------+----------+------+---------+----------+----------+--------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Products:\n",
      "+---------+-----------+----------+------------+------------+-----------------------+------+------------+--------------+-----------+------------+----------+---------+\n",
      "|brand    |category   |cost_price|is_available|product_id  |product_name           |rating|review_count|stock_quantity|subcategory|supplier    |unit_price|weight_kg|\n",
      "+---------+-----------+----------+------------+------------+-----------------------+------+------------+--------------+-----------+------------+----------+---------+\n",
      "|Brand_359|Category_36|156.81    |false       |PROD00000001|Product VheUXtjqntWbGxW|3.0   |4403        |6153          |SubCat_46  |Supplier_27 |2928.1    |45.15    |\n",
      "|Brand_270|Category_86|2605.94   |false       |PROD00000002|Product TKAHvYPogMkFdNA|4.5   |4590        |6889          |SubCat_292 |Supplier_77 |322.32    |35.8     |\n",
      "|Brand_59 |Category_2 |2691.19   |false       |PROD00000003|Product USGUOzHUbDKLqzL|4.6   |4004        |5469          |SubCat_385 |Supplier_51 |2373.78   |20.74    |\n",
      "|Brand_200|Category_23|1397.8    |false       |PROD00000004|Product MCfFmJHhaHGUGUW|1.7   |3802        |8205          |SubCat_371 |Supplier_151|3036.66   |40.58    |\n",
      "|Brand_258|Category_64|1971.85   |false       |PROD00000005|Product DCgQjRWBQErGnVs|2.6   |5202        |3389          |SubCat_112 |Supplier_118|1004.99   |1.18     |\n",
      "+---------+-----------+----------+------------+------------+-----------------------+------+------------+--------------+-----------+------------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Orders:\n",
      "+------------+---------------+----------------+--------------------------------------------------+----------+-------------+-------------------+--------------+------------+--------+-------------+---------------+---------+--------+----------+-----------+------------+---------------+----------+\n",
      "|customer_id |discount_amount|discount_percent|notes                                             |order_date|order_id     |order_timestamp    |payment_method|product_id  |quantity|shipping_cost|shipping_method|status   |subtotal|tax_amount|tax_percent|total_amount|tracking_number|unit_price|\n",
      "+------------+---------------+----------------+--------------------------------------------------+----------+-------------+-------------------+--------------+------------+--------+-------------+---------------+---------+--------+----------+-----------+------------+---------------+----------+\n",
      "|CUST00068133|226.19         |0.15            |NULL                                              |2024-01-18|ORD0000000001|2024-03-23 01:24:52|cash          |PROD00004162|7       |22.32        |overnight      |completed|1507.94 |128.17    |0.1        |1409.92     |TRK4441946795  |215.42    |\n",
      "|CUST00096235|464.24         |0.13            |NULL                                              |2024-06-15|ORD0000000002|2024-08-30 08:47:32|debit_card    |PROD00002561|5       |13.95        |standard       |returned |3571.05 |217.48    |0.07       |3324.29     |TRK7003571361  |714.21    |\n",
      "|CUST00048554|127.78         |0.19            |NULL                                              |2024-08-09|ORD0000000003|2024-12-25 03:01:45|debit_card    |PROD00005334|1       |4.04         |pickup         |returned |672.51  |65.37     |0.12       |610.1       |TRK9039155580  |672.51    |\n",
      "|CUST00021847|793.85         |0.15            |PghqSnNoZGGTdKWChRrDsXHkACceuBSdCAfxpfTWZzDHumiORQ|2023-08-31|ORD0000000004|2024-01-02 23:42:15|cash          |PROD00004224|6       |5.95         |pickup         |completed|5292.36 |674.78    |0.15       |5173.28     |TRK9798745649  |882.06    |\n",
      "|CUST00077307|395.51         |0.26            |NULL                                              |2024-09-22|ORD0000000005|2023-03-02 07:57:18|paypal        |PROD00000753|8       |29.14        |overnight      |pending  |1521.2  |78.8      |0.07       |1204.49     |TRK1260430163  |190.15    |\n",
      "+------------+---------------+----------------+--------------------------------------------------+----------+-------------+-------------------+--------------+------------+--------+-------------+---------------+---------+--------+----------+-----------+------------+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "‚úÖ Large dataset creation completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/03 17:16:15 WARN TaskSetManager: Stage 20 contains a task of very large size (4737 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "from datetime import datetime, timedelta\n",
    "import builtins\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä CREATING LARGE DATASET (TARGET: ~2GB)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "NUM_CUSTOMERS = 100_000      # 100K customers\n",
    "NUM_ORDERS = 500_000       # 5M orders (~2GB)\n",
    "NUM_PRODUCTS = 10_000        # 10K products\n",
    "NUM_CATEGORIES = 100         # 100 categories\n",
    "\n",
    "print(f\"\\nüìã Configuration:\")\n",
    "print(f\"   Customers:  {NUM_CUSTOMERS:,}\")\n",
    "print(f\"   Orders:     {NUM_ORDERS:,}\")\n",
    "print(f\"   Products:   {NUM_PRODUCTS:,}\")\n",
    "print(f\"   Categories: {NUM_CATEGORIES:,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "def random_string(length=10):\n",
    "    \"\"\"Generate random string\"\"\"\n",
    "    return ''.join(random.choices(string.ascii_letters, k=length))\n",
    "\n",
    "def random_date(start_date, end_date):\n",
    "    \"\"\"Generate random date between start and end\"\"\"\n",
    "    delta = end_date - start_date\n",
    "    random_days = random.randint(0, delta.days)\n",
    "    return (start_date + timedelta(days=random_days)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def random_timestamp(start_date, end_date):\n",
    "    \"\"\"Generate random timestamp\"\"\"\n",
    "    delta = end_date - start_date\n",
    "    random_seconds = random.randint(0, int(delta.total_seconds()))\n",
    "    return (start_date + timedelta(seconds=random_seconds)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(\"\\n‚úÖ Helper functions defined\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. CREATE CUSTOMERS (100K rows, ~20MB)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1Ô∏è‚É£  CREATING CUSTOMERS (100K rows)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cities = [\"Hanoi\", \"HCM\", \"Danang\", \"Haiphong\", \"Cantho\", \"Nha Trang\", \"Hue\", \"Vung Tau\", \"Bien Hoa\", \"Can Tho\"]\n",
    "segments = [\"Premium\", \"Gold\", \"Silver\", \"Bronze\", \"Standard\"]\n",
    "countries = [\"Vietnam\", \"USA\", \"UK\", \"Japan\", \"Singapore\", \"Thailand\", \"Malaysia\"]\n",
    "\n",
    "customers_data = []\n",
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = datetime(2024, 12, 31)\n",
    "\n",
    "print(\"üìù Generating customer data...\")\n",
    "for i in range(1, NUM_CUSTOMERS + 1):\n",
    "    customers_data.append({\n",
    "        \"customer_id\": f\"CUST{i:08d}\",\n",
    "        \"first_name\": random_string(8),\n",
    "        \"last_name\": random_string(10),\n",
    "        \"email\": f\"customer{i}@{random.choice(['gmail', 'yahoo', 'outlook'])}.com\",\n",
    "        \"phone\": f\"+84{random.randint(900000000, 999999999)}\",\n",
    "        \"city\": random.choice(cities),\n",
    "        \"country\": random.choice(countries),\n",
    "        \"age\": random.randint(18, 80),\n",
    "        \"gender\": random.choice([\"M\", \"F\", \"O\"]),\n",
    "        \"segment\": random.choice(segments),\n",
    "        \"join_date\": random_date(start_date, end_date),\n",
    "        \"is_active\": random.choice([True, False]),\n",
    "        \"lifetime_value\": builtins.round(random.uniform(100, 50000), 2)\n",
    "    })\n",
    "    \n",
    "    if (i % 10000) == 0:\n",
    "        print(f\"   Progress: {i:,} / {NUM_CUSTOMERS:,} ({i/NUM_CUSTOMERS*100:.1f}%)\")\n",
    "\n",
    "customers_df = spark.createDataFrame(customers_data)\n",
    "print(f\"\\n‚úÖ Customers created: {customers_df.count():,} rows\")\n",
    "\n",
    "# Estimate size\n",
    "print(f\"   Estimated size: ~20 MB\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. CREATE PRODUCTS (10K rows, ~5MB)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2Ô∏è‚É£  CREATING PRODUCTS (10K rows)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "categories = [f\"Category_{i}\" for i in range(1, NUM_CATEGORIES + 1)]\n",
    "brands = [f\"Brand_{i}\" for i in range(1, 500)]\n",
    "suppliers = [f\"Supplier_{i}\" for i in range(1, 200)]\n",
    "\n",
    "products_data = []\n",
    "\n",
    "print(\"üìù Generating product data...\")\n",
    "for i in range(1, NUM_PRODUCTS + 1):\n",
    "    products_data.append({\n",
    "        \"product_id\": f\"PROD{i:08d}\",\n",
    "        \"product_name\": f\"Product {random_string(15)}\",\n",
    "        \"category\": random.choice(categories),\n",
    "        \"subcategory\": f\"SubCat_{random.randint(1, 500)}\",\n",
    "        \"brand\": random.choice(brands),\n",
    "        \"supplier\": random.choice(suppliers),\n",
    "        \"unit_price\": builtins.round(random.uniform(5, 5000), 2),\n",
    "        \"cost_price\": builtins.round(random.uniform(3, 3000), 2),\n",
    "        \"weight_kg\": builtins.round(random.uniform(0.1, 50), 2),\n",
    "        \"is_available\": random.choice([True, False]),\n",
    "        \"stock_quantity\": random.randint(0, 10000),\n",
    "        \"rating\": builtins.round(random.uniform(1, 5), 1),\n",
    "        \"review_count\": random.randint(0, 10000)\n",
    "    })\n",
    "    \n",
    "    if (i % 1000) == 0:\n",
    "        print(f\"   Progress: {i:,} / {NUM_PRODUCTS:,} ({i/NUM_PRODUCTS*100:.1f}%)\")\n",
    "\n",
    "products_df = spark.createDataFrame(products_data)\n",
    "print(f\"\\n‚úÖ Products created: {products_df.count():,} rows\")\n",
    "print(f\"   Estimated size: ~5 MB\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. CREATE ORDERS (5M rows, ~2GB)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3Ô∏è‚É£  CREATING ORDERS (5M rows - THIS WILL TAKE A WHILE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "statuses = [\"completed\", \"pending\", \"cancelled\", \"shipped\", \"delivered\", \"returned\"]\n",
    "payment_methods = [\"credit_card\", \"debit_card\", \"paypal\", \"bank_transfer\", \"cash\"]\n",
    "shipping_methods = [\"standard\", \"express\", \"overnight\", \"pickup\"]\n",
    "\n",
    "# Generate in batches to avoid memory issues\n",
    "BATCH_SIZE = 100_000\n",
    "num_batches = NUM_ORDERS // BATCH_SIZE\n",
    "\n",
    "print(f\"üìù Generating {NUM_ORDERS:,} orders in {num_batches} batches...\")\n",
    "print(f\"   Batch size: {BATCH_SIZE:,}\")\n",
    "\n",
    "orders_batches = []\n",
    "\n",
    "for batch_num in range(num_batches):\n",
    "    batch_data = []\n",
    "    start_idx = batch_num * BATCH_SIZE\n",
    "    \n",
    "    for i in range(BATCH_SIZE):\n",
    "        order_idx = start_idx + i + 1\n",
    "        \n",
    "        # Random customer and product\n",
    "        customer_id = f\"CUST{random.randint(1, NUM_CUSTOMERS):08d}\"\n",
    "        product_id = f\"PROD{random.randint(1, NUM_PRODUCTS):08d}\"\n",
    "        \n",
    "        # Order details\n",
    "        quantity = random.randint(1, 10)\n",
    "        unit_price = builtins.round(random.uniform(10, 1000), 2)\n",
    "        discount = builtins.round(random.uniform(0, 0.3), 2)\n",
    "        tax = builtins.round(random.uniform(0.05, 0.15), 2)\n",
    "        \n",
    "        subtotal = unit_price * quantity\n",
    "        discount_amount = subtotal * discount\n",
    "        tax_amount = (subtotal - discount_amount) * tax\n",
    "        total_amount = subtotal - discount_amount + tax_amount\n",
    "        \n",
    "        batch_data.append({\n",
    "            \"order_id\": f\"ORD{order_idx:010d}\",\n",
    "            \"customer_id\": customer_id,\n",
    "            \"product_id\": product_id,\n",
    "            \"order_date\": random_date(datetime(2023, 1, 1), datetime(2024, 12, 31)),\n",
    "            \"order_timestamp\": random_timestamp(datetime(2023, 1, 1), datetime(2024, 12, 31)),\n",
    "            \"quantity\": quantity,\n",
    "            \"unit_price\": unit_price,\n",
    "            \"discount_percent\": discount,\n",
    "            \"discount_amount\": builtins.round(discount_amount, 2),\n",
    "            \"tax_percent\": tax,\n",
    "            \"tax_amount\": builtins.round(tax_amount, 2),\n",
    "            \"subtotal\": builtins.round(subtotal, 2),\n",
    "            \"total_amount\": builtins.round(total_amount, 2),\n",
    "            \"status\": random.choice(statuses),\n",
    "            \"payment_method\": random.choice(payment_methods),\n",
    "            \"shipping_method\": random.choice(shipping_methods),\n",
    "            \"shipping_cost\": builtins.round(random.uniform(0, 50), 2),\n",
    "            \"tracking_number\": f\"TRK{random.randint(1000000000, 9999999999)}\",\n",
    "            \"notes\": random_string(50) if random.random() > 0.7 else None\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame for this batch\n",
    "    batch_df = spark.createDataFrame(batch_data)\n",
    "    orders_batches.append(batch_df)\n",
    "    \n",
    "    print(f\"   ‚úÖ Batch {batch_num + 1}/{num_batches} completed ({(batch_num + 1) * BATCH_SIZE:,} orders)\")\n",
    "\n",
    "# Union all batches\n",
    "print(\"\\nüì¶ Combining all batches...\")\n",
    "orders_df = orders_batches[0]\n",
    "for batch_df in orders_batches[1:]:\n",
    "    orders_df = orders_df.union(batch_df)\n",
    "\n",
    "print(f\"\\n‚úÖ Orders created: {orders_df.count():,} rows\")\n",
    "print(f\"   Estimated size: ~2 GB\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä DATASET SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚úÖ All data created successfully!\")\n",
    "print(f\"\\nüìã Dataset Details:\")\n",
    "print(f\"   Customers:  {customers_df.count():,} rows (~20 MB)\")\n",
    "print(f\"   Products:   {products_df.count():,} rows (~5 MB)\")\n",
    "print(f\"   Orders:     {orders_df.count():,} rows (~2 GB)\")\n",
    "print(f\"   Total:      ~2.025 GB\")\n",
    "\n",
    "print(f\"\\nüìä Sample Data:\")\n",
    "print(\"\\nCustomers:\")\n",
    "customers_df.show(5, truncate=False)\n",
    "\n",
    "print(\"\\nProducts:\")\n",
    "products_df.show(5, truncate=False)\n",
    "\n",
    "print(\"\\nOrders:\")\n",
    "orders_df.show(5, truncate=False)\n",
    "\n",
    "print(\"\\n‚úÖ Large dataset creation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WRITE DATA WITHOUT BUCKETING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "WRITING DATA WITHOUT BUCKETING (MINIO)\n",
      "================================================================================\n",
      "\n",
      "üìù Writing orders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/03 17:16:16 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "26/01/03 17:16:18 WARN TaskSetManager: Stage 21 contains a task of very large size (23956 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Orders written\n",
      "\n",
      "üìù Writing customers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/03 17:16:27 WARN TaskSetManager: Stage 22 contains a task of very large size (2803 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Customers written\n",
      "\n",
      "üìù Writing products...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/03 17:16:28 WARN TaskSetManager: Stage 23 contains a task of very large size (1167 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Products written\n",
      "\n",
      "‚úÖ All data written without bucketing!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"WRITING DATA WITHOUT BUCKETING (MINIO)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Write to MinIO staging bucket\n",
    "print(\"\\nüìù Writing orders...\")\n",
    "orders_df.coalesce(4).write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"s3a://staging/orders_no_bucket\")\n",
    "print(\"‚úÖ Orders written\")\n",
    "\n",
    "print(\"\\nüìù Writing customers...\")\n",
    "customers_df.coalesce(4).write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"s3a://staging/customers_no_bucket\")\n",
    "print(\"‚úÖ Customers written\")\n",
    "\n",
    "print(\"\\nüìù Writing products...\")\n",
    "products_df.coalesce(1).write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"s3a://staging/products_no_bucket\")\n",
    "print(\"‚úÖ Products written\")\n",
    "\n",
    "print(\"\\n‚úÖ All data written without bucketing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WRITE DATA WITH BUCKETING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "WRITING DATA WITH BUCKETING (MINIO)\n",
      "================================================================================\n",
      "\n",
      "üìù Writing orders_bucketed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/03 17:17:52 WARN TaskSetManager: Stage 26 contains a task of very large size (9659 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Orders bucketed\n",
      "\n",
      "üìù Writing customers_bucketed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/03 17:18:01 WARN TaskSetManager: Stage 27 contains a task of very large size (2803 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Customers bucketed\n",
      "\n",
      "üìù Writing products_bucketed...\n",
      "‚úÖ Products bucketed\n",
      "\n",
      "‚úÖ All data written with bucketing!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"WRITING DATA WITH BUCKETING (MINIO)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Drop existing tables\n",
    "spark.sql(\"DROP TABLE IF EXISTS orders_bucketed\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS customers_bucketed\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS products_bucketed\")\n",
    "\n",
    "# Write orders with bucketing\n",
    "print(\"\\nüìù Writing orders_bucketed...\")\n",
    "orders_df.coalesce(10).write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .bucketBy(10, \"customer_id\") \\\n",
    "    .sortBy(\"customer_id\") \\\n",
    "    .option(\"path\", \"s3a://warehouse/orders_bucketed\") \\\n",
    "    .saveAsTable(\"orders_bucketed\")\n",
    "print(\"‚úÖ Orders bucketed\")\n",
    "\n",
    "# Write customers with bucketing\n",
    "print(\"\\nüìù Writing customers_bucketed...\")\n",
    "customers_df.coalesce(10).write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .bucketBy(10, \"customer_id\") \\\n",
    "    .sortBy(\"customer_id\") \\\n",
    "    .option(\"path\", \"s3a://warehouse/customers_bucketed\") \\\n",
    "    .saveAsTable(\"customers_bucketed\")\n",
    "print(\"‚úÖ Customers bucketed\")\n",
    "\n",
    "# Write products with bucketing (by product id)\n",
    "print(\"\\nüìù Writing products_bucketed...\")\n",
    "products_df.coalesce(5).write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .bucketBy(5, \"product_id\") \\\n",
    "    .sortBy(\"product_id\") \\\n",
    "    .option(\"path\", \"s3a://warehouse/products_bucketed\") \\\n",
    "    .saveAsTable(\"products_bucketed\")\n",
    "print(\"‚úÖ Products bucketed\")\n",
    "\n",
    "print(\"\\n‚úÖ All data written with bucketing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINE COMPLEX QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Complex query function defined\n"
     ]
    }
   ],
   "source": [
    "def complex_query(orders, customers, products, query_name=\"Query\"):\n",
    "    \"\"\"\n",
    "    Complex query with:\n",
    "    - Multiple joins\n",
    "    - Aggregations\n",
    "    - Window functions\n",
    "    - Filtering\n",
    "    - Sorting\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Executing {query_name}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ‚úÖ FIX: Add aliases to avoid ambiguous references\n",
    "    orders_alias = orders.alias(\"o\")\n",
    "    customers_alias = customers.alias(\"c\")\n",
    "    products_alias = products.alias(\"p\")\n",
    "    \n",
    "    # Step 1: Join orders with customers\n",
    "    df = orders_alias.join(customers_alias, col(\"o.customer_id\") == col(\"c.customer_id\"))\n",
    "    \n",
    "    # Step 2: Join with products\n",
    "    df = df.join(products_alias, col(\"o.product_id\") == col(\"p.product_id\"))\n",
    "    \n",
    "    # Step 3: Filter\n",
    "    df = df.filter(\n",
    "        (col(\"o.status\") == \"completed\") & \n",
    "        (col(\"c.city\").isin([\"Hanoi\", \"HCM\"])) &\n",
    "        (col(\"c.age\") >= 25)\n",
    "    )\n",
    "    \n",
    "    # Step 4: Add calculated columns\n",
    "    # ‚úÖ FIX: Use explicit table aliases\n",
    "    df = df.withColumn(\"revenue\", col(\"o.total_amount\"))\n",
    "    df = df.withColumn(\"year\", year(to_date(col(\"o.order_date\"))))\n",
    "    df = df.withColumn(\"month\", month(to_date(col(\"o.order_date\"))))\n",
    "    \n",
    "    # Step 5: Aggregation by customer\n",
    "    customer_agg = df.groupBy(\n",
    "        col(\"c.customer_id\").alias(\"customer_id\"),\n",
    "        col(\"c.first_name\").alias(\"first_name\"),\n",
    "        col(\"c.last_name\").alias(\"last_name\"),\n",
    "        col(\"c.city\").alias(\"city\"),\n",
    "        col(\"c.segment\").alias(\"segment\")\n",
    "    ).agg(\n",
    "        countDistinct(\"o.order_id\").alias(\"total_orders\"),\n",
    "        sum(\"revenue\").alias(\"total_revenue\"),\n",
    "        avg(\"revenue\").alias(\"avg_revenue\"),\n",
    "        countDistinct(\"o.product_id\").alias(\"unique_products\"),\n",
    "        max(\"o.order_date\").alias(\"last_order_date\")\n",
    "    )\n",
    "    \n",
    "    # Step 6: Window function - rank customers by revenue in each city\n",
    "    window_spec = Window.partitionBy(\"city\").orderBy(col(\"total_revenue\").desc())\n",
    "    customer_agg = customer_agg.withColumn(\"rank_in_city\", rank().over(window_spec))\n",
    "    \n",
    "    # Step 7: Filter top 100 customers per city\n",
    "    customer_agg = customer_agg.filter(col(\"rank_in_city\") <= 100)\n",
    "    \n",
    "    # Step 8: Category analysis\n",
    "    # ‚úÖ FIX: Use explicit table aliases to avoid ambiguity\n",
    "    category_agg = df.groupBy(\n",
    "        col(\"p.category\").alias(\"category\"),\n",
    "        col(\"c.city\").alias(\"city\")\n",
    "    ).agg(\n",
    "        countDistinct(\"o.order_id\").alias(\"orders_count\"),\n",
    "        sum(\"revenue\").alias(\"category_revenue\"),\n",
    "        avg(\"o.unit_price\").alias(\"avg_unit_price\")  # ‚úÖ FIX: o.unit_price\n",
    "    )\n",
    "    \n",
    "    # Step 9: Join aggregations\n",
    "    result = customer_agg.join(\n",
    "        category_agg.groupBy(\"city\").agg(\n",
    "            sum(\"category_revenue\").alias(\"city_total_revenue\")\n",
    "        ),\n",
    "        \"city\"\n",
    "    )\n",
    "    \n",
    "    # Step 10: Calculate percentage\n",
    "    result = result.withColumn(\n",
    "        \"revenue_percentage\",\n",
    "        (col(\"total_revenue\") / col(\"city_total_revenue\") * 100)\n",
    "    )\n",
    "    \n",
    "    # Step 11: Sort and collect\n",
    "    result = result.orderBy(col(\"total_revenue\").desc())\n",
    "    \n",
    "    # Trigger execution\n",
    "    count = result.count()\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"   ‚úÖ Completed: {count:,} rows\")\n",
    "    print(f\"   ‚è±Ô∏è  Time: {execution_time:.2f}s\")\n",
    "    \n",
    "    return result, execution_time\n",
    "\n",
    "print(\"‚úÖ Complex query function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST WITHOUT BUCKETING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEST 1: WITHOUT BUCKETING\n",
      "================================================================================\n",
      "\n",
      "üîç Executing WITHOUT BUCKETING...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Completed: 200 rows\n",
      "   ‚è±Ô∏è  Time: 7.08s\n",
      "\n",
      "üìä Sample results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+----------+----------+--------+------------+------------------+-----------------+---------------+---------------+------------+--------------------+-------------------+\n",
      "|city |customer_id |first_name|last_name |segment |total_orders|total_revenue     |avg_revenue      |unique_products|last_order_date|rank_in_city|city_total_revenue  |revenue_percentage |\n",
      "+-----+------------+----------+----------+--------+------------+------------------+-----------------+---------------+---------------+------------+--------------------+-------------------+\n",
      "|Hanoi|CUST00053775|oHPssYGh  |TvRtubOmcR|Silver  |6           |27143.039999999997|4523.839999999999|6              |2024-12-30     |1           |1.9888377880000003E7|0.13647689200080704|\n",
      "|HCM  |CUST00092493|dicHJDQq  |bQnqVMDbKC|Standard|4           |25526.07          |6381.5175        |4              |2024-12-28     |1           |2.0387343240000013E7|0.1252054752770228 |\n",
      "|Hanoi|CUST00032448|CTzuyiVI  |EFybDbdfYP|Premium |4           |25300.82          |6325.205         |4              |2024-10-29     |2           |1.9888377880000003E7|0.12721409535084716|\n",
      "|Hanoi|CUST00093763|lRkdjNEH  |LlKHgWbKbT|Standard|4           |22805.4           |5701.35          |4              |2024-08-22     |3           |1.9888377880000003E7|0.11466696850593026|\n",
      "|Hanoi|CUST00011360|COxomAxX  |xcQnumKaUH|Premium |4           |22660.46          |5665.115         |4              |2023-11-23     |4           |1.9888377880000003E7|0.11393820117822498|\n",
      "|Hanoi|CUST00037368|reRxfFBy  |boOcWHWjaJ|Silver  |3           |21964.14          |7321.38          |3              |2024-10-11     |5           |1.9888377880000003E7|0.11043706094345386|\n",
      "|HCM  |CUST00017425|TQNxkbTZ  |fYecTnhqxO|Silver  |4           |21796.309999999998|5449.077499999999|4              |2024-05-10     |2           |2.0387343240000013E7|0.10691098758388287|\n",
      "|HCM  |CUST00058776|pvQZIvFg  |haJLtyRPur|Gold    |7           |21670.510000000002|3095.787142857143|7              |2024-08-24     |3           |2.0387343240000013E7|0.10629393808155646|\n",
      "|HCM  |CUST00075622|eaCTahon  |JbJkPRLiBF|Bronze  |3           |20558.01          |6852.669999999999|3              |2024-08-06     |4           |2.0387343240000013E7|0.10083712113928182|\n",
      "|HCM  |CUST00000946|zgTIPiQh  |SywaXGGMwA|Standard|5           |20429.93          |4085.986         |5              |2024-07-26     |5           |2.0387343240000013E7|0.10020888822785126|\n",
      "+-----+------------+----------+----------+--------+------------+------------------+-----------------+---------------+---------------+------------+--------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "üìã Execution Plan:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [total_revenue#2707 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(total_revenue#2707 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=4860]\n",
      "      +- Project [city#2300, customer_id#2302, first_name#2304, last_name#2308, segment#2311, total_orders#2705L, total_revenue#2707, avg_revenue#2709, unique_products#2710L, last_order_date#2712, rank_in_city#2727, city_total_revenue#2812, ((total_revenue#2707 / city_total_revenue#2812) * 100.0) AS revenue_percentage#2875]\n",
      "         +- SortMergeJoin [city#2300], [city#2835], Inner\n",
      "            :- Filter (rank_in_city#2727 <= 100)\n",
      "            :  +- Window [rank(total_revenue#2707) windowspecdefinition(city#2300, total_revenue#2707 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_in_city#2727], [city#2300], [total_revenue#2707 DESC NULLS LAST]\n",
      "            :     +- WindowGroupLimit [city#2300], [total_revenue#2707 DESC NULLS LAST], rank(total_revenue#2707), 100, Final\n",
      "            :        +- Sort [city#2300 ASC NULLS FIRST, total_revenue#2707 DESC NULLS LAST], false, 0\n",
      "            :           +- Exchange hashpartitioning(city#2300, 200), ENSURE_REQUIREMENTS, [plan_id=4826]\n",
      "            :              +- WindowGroupLimit [city#2300], [total_revenue#2707 DESC NULLS LAST], rank(total_revenue#2707), 100, Partial\n",
      "            :                 +- Sort [city#2300 ASC NULLS FIRST, total_revenue#2707 DESC NULLS LAST], false, 0\n",
      "            :                    +- SortAggregate(key=[customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311], functions=[count(o.order_id#3237), first(sum(revenue)#3241, true), first(avg(revenue)#3243, true), count(o.product_id#3238), first(max(o.order_date)#3245, true)])\n",
      "            :                       +- Sort [customer_id#2302 ASC NULLS FIRST, first_name#2304 ASC NULLS FIRST, last_name#2308 ASC NULLS FIRST, city#2300 ASC NULLS FIRST, segment#2311 ASC NULLS FIRST], false, 0\n",
      "            :                          +- Exchange hashpartitioning(customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, 200), ENSURE_REQUIREMENTS, [plan_id=4819]\n",
      "            :                             +- SortAggregate(key=[customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311], functions=[partial_count(o.order_id#3237) FILTER (WHERE (gid#3236 = 1)), partial_first(sum(revenue)#3241, true) FILTER (WHERE (gid#3236 = 0)), partial_first(avg(revenue)#3243, true) FILTER (WHERE (gid#3236 = 0)), partial_count(o.product_id#3238) FILTER (WHERE (gid#3236 = 2)), partial_first(max(o.order_date)#3245, true) FILTER (WHERE (gid#3236 = 0))])\n",
      "            :                                +- SortAggregate(key=[customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, o.order_id#3237, o.product_id#3238, gid#3236], functions=[sum(revenue#3239), avg(revenue#3239), max(o.order_date#3240)])\n",
      "            :                                   +- Sort [customer_id#2302 ASC NULLS FIRST, first_name#2304 ASC NULLS FIRST, last_name#2308 ASC NULLS FIRST, city#2300 ASC NULLS FIRST, segment#2311 ASC NULLS FIRST, o.order_id#3237 ASC NULLS FIRST, o.product_id#3238 ASC NULLS FIRST, gid#3236 ASC NULLS FIRST], false, 0\n",
      "            :                                      +- Exchange hashpartitioning(customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, o.order_id#3237, o.product_id#3238, gid#3236, 200), ENSURE_REQUIREMENTS, [plan_id=4814]\n",
      "            :                                         +- SortAggregate(key=[customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, o.order_id#3237, o.product_id#3238, gid#3236], functions=[partial_sum(revenue#3239), partial_avg(revenue#3239), partial_max(o.order_date#3240)])\n",
      "            :                                            +- Sort [customer_id#2302 ASC NULLS FIRST, first_name#2304 ASC NULLS FIRST, last_name#2308 ASC NULLS FIRST, city#2300 ASC NULLS FIRST, segment#2311 ASC NULLS FIRST, o.order_id#3237 ASC NULLS FIRST, o.product_id#3238 ASC NULLS FIRST, gid#3236 ASC NULLS FIRST], false, 0\n",
      "            :                                               +- Expand [[customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, null, null, 0, revenue#2508, order_date#2265], [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, order_id#2266, null, 1, null, null], [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, null, product_id#2269, 2, null, null]], [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, o.order_id#3237, o.product_id#3238, gid#3236, revenue#3239, o.order_date#3240]\n",
      "            :                                                  +- Project [order_date#2265, order_id#2266, product_id#2269, city#2300, customer_id#2302, first_name#2304, last_name#2308, segment#2311, total_amount#2277 AS revenue#2508]\n",
      "            :                                                     +- SortMergeJoin [product_id#2269], [product_id#2329], Inner\n",
      "            :                                                        :- Sort [product_id#2269 ASC NULLS FIRST], false, 0\n",
      "            :                                                        :  +- Exchange hashpartitioning(product_id#2269, 200), ENSURE_REQUIREMENTS, [plan_id=4803]\n",
      "            :                                                        :     +- Project [order_date#2265, order_id#2266, product_id#2269, total_amount#2277, city#2300, customer_id#2302, first_name#2304, last_name#2308, segment#2311]\n",
      "            :                                                        :        +- SortMergeJoin [customer_id#2261], [customer_id#2302], Inner\n",
      "            :                                                        :           :- Sort [customer_id#2261 ASC NULLS FIRST], false, 0\n",
      "            :                                                        :           :  +- Exchange hashpartitioning(customer_id#2261, 200), ENSURE_REQUIREMENTS, [plan_id=4795]\n",
      "            :                                                        :           :     +- Project [customer_id#2261, order_date#2265, order_id#2266, product_id#2269, total_amount#2277]\n",
      "            :                                                        :           :        +- Filter (((isnotnull(status#2273) AND (status#2273 = completed)) AND isnotnull(customer_id#2261)) AND isnotnull(product_id#2269))\n",
      "            :                                                        :           :           +- FileScan parquet [customer_id#2261,order_date#2265,order_id#2266,product_id#2269,status#2273,total_amount#2277] Batched: true, DataFilters: [isnotnull(status#2273), (status#2273 = completed), isnotnull(customer_id#2261), isnotnull(produc..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://staging/orders_no_bucket], PartitionFilters: [], PushedFilters: [IsNotNull(status), EqualTo(status,completed), IsNotNull(customer_id), IsNotNull(product_id)], ReadSchema: struct<customer_id:string,order_date:string,order_id:string,product_id:string,status:string,total...\n",
      "            :                                                        :           +- Sort [customer_id#2302 ASC NULLS FIRST], false, 0\n",
      "            :                                                        :              +- Exchange hashpartitioning(customer_id#2302, 200), ENSURE_REQUIREMENTS, [plan_id=4796]\n",
      "            :                                                        :                 +- Project [city#2300, customer_id#2302, first_name#2304, last_name#2308, segment#2311]\n",
      "            :                                                        :                    +- Filter ((((isnotnull(age#2299L) AND city#2300 IN (Hanoi,HCM)) AND (age#2299L >= 25)) AND isnotnull(customer_id#2302)) AND isnotnull(city#2300))\n",
      "            :                                                        :                       +- FileScan parquet [age#2299L,city#2300,customer_id#2302,first_name#2304,last_name#2308,segment#2311] Batched: true, DataFilters: [isnotnull(age#2299L), city#2300 IN (Hanoi,HCM), (age#2299L >= 25), isnotnull(customer_id#2302), ..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://staging/customers_no_bucket], PartitionFilters: [], PushedFilters: [IsNotNull(age), In(city, [HCM,Hanoi]), GreaterThanOrEqual(age,25), IsNotNull(customer_id), IsNot..., ReadSchema: struct<age:bigint,city:string,customer_id:string,first_name:string,last_name:string,segment:string>\n",
      "            :                                                        +- Sort [product_id#2329 ASC NULLS FIRST], false, 0\n",
      "            :                                                           +- Exchange hashpartitioning(product_id#2329, 200), ENSURE_REQUIREMENTS, [plan_id=4804]\n",
      "            :                                                              +- Filter isnotnull(product_id#2329)\n",
      "            :                                                                 +- FileScan parquet [product_id#2329] Batched: true, DataFilters: [isnotnull(product_id#2329)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://staging/products_no_bucket], PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<product_id:string>\n",
      "            +- Sort [city#2835 ASC NULLS FIRST], false, 0\n",
      "               +- HashAggregate(keys=[city#2835], functions=[sum(category_revenue#2797)])\n",
      "                  +- Exchange hashpartitioning(city#2835, 200), ENSURE_REQUIREMENTS, [plan_id=4852]\n",
      "                     +- HashAggregate(keys=[city#2835], functions=[partial_sum(category_revenue#2797)])\n",
      "                        +- HashAggregate(keys=[category#2848, city#2835], functions=[sum(revenue#2508)])\n",
      "                           +- Exchange hashpartitioning(category#2848, city#2835, 200), ENSURE_REQUIREMENTS, [plan_id=4848]\n",
      "                              +- HashAggregate(keys=[category#2848, city#2835], functions=[partial_sum(revenue#2508)])\n",
      "                                 +- Project [city#2835, category#2848, total_amount#2831 AS revenue#2508]\n",
      "                                    +- SortMergeJoin [product_id#2823], [product_id#2851], Inner\n",
      "                                       :- Sort [product_id#2823 ASC NULLS FIRST], false, 0\n",
      "                                       :  +- Exchange hashpartitioning(product_id#2823, 200), ENSURE_REQUIREMENTS, [plan_id=4840]\n",
      "                                       :     +- Project [product_id#2823, total_amount#2831, city#2835]\n",
      "                                       :        +- SortMergeJoin [customer_id#2815], [customer_id#2837], Inner\n",
      "                                       :           :- Sort [customer_id#2815 ASC NULLS FIRST], false, 0\n",
      "                                       :           :  +- Exchange hashpartitioning(customer_id#2815, 200), ENSURE_REQUIREMENTS, [plan_id=4832]\n",
      "                                       :           :     +- Project [customer_id#2815, product_id#2823, total_amount#2831]\n",
      "                                       :           :        +- Filter (((isnotnull(status#2827) AND (status#2827 = completed)) AND isnotnull(customer_id#2815)) AND isnotnull(product_id#2823))\n",
      "                                       :           :           +- FileScan parquet [customer_id#2815,product_id#2823,status#2827,total_amount#2831] Batched: true, DataFilters: [isnotnull(status#2827), (status#2827 = completed), isnotnull(customer_id#2815), isnotnull(produc..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://staging/orders_no_bucket], PartitionFilters: [], PushedFilters: [IsNotNull(status), EqualTo(status,completed), IsNotNull(customer_id), IsNotNull(product_id)], ReadSchema: struct<customer_id:string,product_id:string,status:string,total_amount:double>\n",
      "                                       :           +- Sort [customer_id#2837 ASC NULLS FIRST], false, 0\n",
      "                                       :              +- Exchange hashpartitioning(customer_id#2837, 200), ENSURE_REQUIREMENTS, [plan_id=4833]\n",
      "                                       :                 +- Project [city#2835, customer_id#2837]\n",
      "                                       :                    +- Filter ((((isnotnull(age#2834L) AND city#2835 IN (Hanoi,HCM)) AND (age#2834L >= 25)) AND isnotnull(customer_id#2837)) AND isnotnull(city#2835))\n",
      "                                       :                       +- FileScan parquet [age#2834L,city#2835,customer_id#2837] Batched: true, DataFilters: [isnotnull(age#2834L), city#2835 IN (Hanoi,HCM), (age#2834L >= 25), isnotnull(customer_id#2837), ..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://staging/customers_no_bucket], PartitionFilters: [], PushedFilters: [IsNotNull(age), In(city, [HCM,Hanoi]), GreaterThanOrEqual(age,25), IsNotNull(customer_id), IsNot..., ReadSchema: struct<age:bigint,city:string,customer_id:string>\n",
      "                                       +- Sort [product_id#2851 ASC NULLS FIRST], false, 0\n",
      "                                          +- Exchange hashpartitioning(product_id#2851, 200), ENSURE_REQUIREMENTS, [plan_id=4841]\n",
      "                                             +- Filter isnotnull(product_id#2851)\n",
      "                                                +- FileScan parquet [category#2848,product_id#2851] Batched: true, DataFilters: [isnotnull(product_id#2851)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://staging/products_no_bucket], PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<category:string,product_id:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TEST 1: WITHOUT BUCKETING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Read data without bucketing\n",
    "orders_no_bucket = spark.read.parquet(\"s3a://staging/orders_no_bucket\")\n",
    "customers_no_bucket = spark.read.parquet(\"s3a://staging/customers_no_bucket\")\n",
    "products_no_bucket = spark.read.parquet(\"s3a://staging/products_no_bucket\")\n",
    "\n",
    "# Execute complex query\n",
    "result_no_bucket, time_no_bucket = complex_query(\n",
    "    orders_no_bucket,\n",
    "    customers_no_bucket,\n",
    "    products_no_bucket,\n",
    "    \"WITHOUT BUCKETING\"\n",
    ")\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\nüìä Sample results:\")\n",
    "result_no_bucket.show(10, truncate=False)\n",
    "\n",
    "# Show execution plan\n",
    "print(\"\\nüìã Execution Plan:\")\n",
    "result_no_bucket.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST WITH BUCKETING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEST 2: WITH BUCKETING\n",
      "================================================================================\n",
      "\n",
      "üîç Executing WITH BUCKETING...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Completed: 200 rows\n",
      "   ‚è±Ô∏è  Time: 6.97s\n",
      "\n",
      "üìä Sample results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 178:==================================>                      (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+----------+----------+--------+------------+------------------+-----------------+---------------+---------------+------------+--------------------+-------------------+\n",
      "|city |customer_id |first_name|last_name |segment |total_orders|total_revenue     |avg_revenue      |unique_products|last_order_date|rank_in_city|city_total_revenue  |revenue_percentage |\n",
      "+-----+------------+----------+----------+--------+------------+------------------+-----------------+---------------+---------------+------------+--------------------+-------------------+\n",
      "|Hanoi|CUST00053775|oHPssYGh  |TvRtubOmcR|Silver  |6           |27143.039999999997|4523.839999999999|6              |2024-12-30     |1           |1.9888377880000006E7|0.136476892000807  |\n",
      "|HCM  |CUST00092493|dicHJDQq  |bQnqVMDbKC|Standard|4           |25526.07          |6381.5175        |4              |2024-12-28     |1           |2.038734324000001E7 |0.12520547527702283|\n",
      "|Hanoi|CUST00032448|CTzuyiVI  |EFybDbdfYP|Premium |4           |25300.82          |6325.205         |4              |2024-10-29     |2           |1.9888377880000006E7|0.12721409535084713|\n",
      "|Hanoi|CUST00093763|lRkdjNEH  |LlKHgWbKbT|Standard|4           |22805.399999999998|5701.349999999999|4              |2024-08-22     |3           |1.9888377880000006E7|0.11466696850593022|\n",
      "|Hanoi|CUST00011360|COxomAxX  |xcQnumKaUH|Premium |4           |22660.46          |5665.115         |4              |2023-11-23     |4           |1.9888377880000006E7|0.11393820117822497|\n",
      "|Hanoi|CUST00037368|reRxfFBy  |boOcWHWjaJ|Silver  |3           |21964.14          |7321.38          |3              |2024-10-11     |5           |1.9888377880000006E7|0.11043706094345383|\n",
      "|HCM  |CUST00017425|TQNxkbTZ  |fYecTnhqxO|Silver  |4           |21796.31          |5449.0775        |4              |2024-05-10     |2           |2.038734324000001E7 |0.10691098758388291|\n",
      "|HCM  |CUST00058776|pvQZIvFg  |haJLtyRPur|Gold    |7           |21670.510000000002|3095.787142857143|7              |2024-08-24     |3           |2.038734324000001E7 |0.10629393808155647|\n",
      "|HCM  |CUST00075622|eaCTahon  |JbJkPRLiBF|Bronze  |3           |20558.01          |6852.669999999999|3              |2024-08-06     |4           |2.038734324000001E7 |0.10083712113928185|\n",
      "|HCM  |CUST00000946|zgTIPiQh  |SywaXGGMwA|Standard|5           |20429.93          |4085.986         |5              |2024-07-26     |5           |2.038734324000001E7 |0.10020888822785126|\n",
      "+-----+------------+----------+----------+--------+------------+------------------+-----------------+---------------+---------------+------------+--------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "üìã Execution Plan:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [total_revenue#3717 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(total_revenue#3717 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=7348]\n",
      "      +- Project [city#3310, customer_id#3312, first_name#3314, last_name#3318, segment#3321, total_orders#3715L, total_revenue#3717, avg_revenue#3719, unique_products#3720L, last_order_date#3722, rank_in_city#3737, city_total_revenue#3822, ((total_revenue#3717 / city_total_revenue#3822) * 100.0) AS revenue_percentage#3885]\n",
      "         +- SortMergeJoin [city#3310], [city#3845], Inner\n",
      "            :- Filter (rank_in_city#3737 <= 100)\n",
      "            :  +- Window [rank(total_revenue#3717) windowspecdefinition(city#3310, total_revenue#3717 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_in_city#3737], [city#3310], [total_revenue#3717 DESC NULLS LAST]\n",
      "            :     +- WindowGroupLimit [city#3310], [total_revenue#3717 DESC NULLS LAST], rank(total_revenue#3717), 100, Final\n",
      "            :        +- Sort [city#3310 ASC NULLS FIRST, total_revenue#3717 DESC NULLS LAST], false, 0\n",
      "            :           +- Exchange hashpartitioning(city#3310, 200), ENSURE_REQUIREMENTS, [plan_id=7316]\n",
      "            :              +- WindowGroupLimit [city#3310], [total_revenue#3717 DESC NULLS LAST], rank(total_revenue#3717), 100, Partial\n",
      "            :                 +- Sort [city#3310 ASC NULLS FIRST, total_revenue#3717 DESC NULLS LAST], false, 0\n",
      "            :                    +- SortAggregate(key=[customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321], functions=[count(o.order_id#4208), first(sum(revenue)#4212, true), first(avg(revenue)#4214, true), count(o.product_id#4209), first(max(o.order_date)#4216, true)])\n",
      "            :                       +- Sort [customer_id#3312 ASC NULLS FIRST, first_name#3314 ASC NULLS FIRST, last_name#3318 ASC NULLS FIRST, city#3310 ASC NULLS FIRST, segment#3321 ASC NULLS FIRST], false, 0\n",
      "            :                          +- Exchange hashpartitioning(customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, 200), ENSURE_REQUIREMENTS, [plan_id=7309]\n",
      "            :                             +- SortAggregate(key=[customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321], functions=[partial_count(o.order_id#4208) FILTER (WHERE (gid#4207 = 1)), partial_first(sum(revenue)#4212, true) FILTER (WHERE (gid#4207 = 0)), partial_first(avg(revenue)#4214, true) FILTER (WHERE (gid#4207 = 0)), partial_count(o.product_id#4209) FILTER (WHERE (gid#4207 = 2)), partial_first(max(o.order_date)#4216, true) FILTER (WHERE (gid#4207 = 0))])\n",
      "            :                                +- SortAggregate(key=[customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, o.order_id#4208, o.product_id#4209, gid#4207], functions=[sum(revenue#4210), avg(revenue#4210), max(o.order_date#4211)])\n",
      "            :                                   +- Sort [customer_id#3312 ASC NULLS FIRST, first_name#3314 ASC NULLS FIRST, last_name#3318 ASC NULLS FIRST, city#3310 ASC NULLS FIRST, segment#3321 ASC NULLS FIRST, o.order_id#4208 ASC NULLS FIRST, o.product_id#4209 ASC NULLS FIRST, gid#4207 ASC NULLS FIRST], false, 0\n",
      "            :                                      +- Exchange hashpartitioning(customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, o.order_id#4208, o.product_id#4209, gid#4207, 200), ENSURE_REQUIREMENTS, [plan_id=7304]\n",
      "            :                                         +- SortAggregate(key=[customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, o.order_id#4208, o.product_id#4209, gid#4207], functions=[partial_sum(revenue#4210), partial_avg(revenue#4210), partial_max(o.order_date#4211)])\n",
      "            :                                            +- Sort [customer_id#3312 ASC NULLS FIRST, first_name#3314 ASC NULLS FIRST, last_name#3318 ASC NULLS FIRST, city#3310 ASC NULLS FIRST, segment#3321 ASC NULLS FIRST, o.order_id#4208 ASC NULLS FIRST, o.product_id#4209 ASC NULLS FIRST, gid#4207 ASC NULLS FIRST], false, 0\n",
      "            :                                               +- Expand [[customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, null, null, 0, revenue#3518, order_date#3275], [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, order_id#3276, null, 1, null, null], [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, null, product_id#3279, 2, null, null]], [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, o.order_id#4208, o.product_id#4209, gid#4207, revenue#4210, o.order_date#4211]\n",
      "            :                                                  +- Project [order_date#3275, order_id#3276, product_id#3279, city#3310, customer_id#3312, first_name#3314, last_name#3318, segment#3321, total_amount#3287 AS revenue#3518]\n",
      "            :                                                     +- SortMergeJoin [product_id#3279], [product_id#3339], Inner\n",
      "            :                                                        :- Sort [product_id#3279 ASC NULLS FIRST], false, 0\n",
      "            :                                                        :  +- Exchange hashpartitioning(product_id#3279, 5), ENSURE_REQUIREMENTS, [plan_id=7294]\n",
      "            :                                                        :     +- Project [order_date#3275, order_id#3276, product_id#3279, total_amount#3287, city#3310, customer_id#3312, first_name#3314, last_name#3318, segment#3321]\n",
      "            :                                                        :        +- SortMergeJoin [customer_id#3271], [customer_id#3312], Inner\n",
      "            :                                                        :           :- Sort [customer_id#3271 ASC NULLS FIRST], false, 0\n",
      "            :                                                        :           :  +- Project [customer_id#3271, order_date#3275, order_id#3276, product_id#3279, total_amount#3287]\n",
      "            :                                                        :           :     +- Filter (((isnotnull(status#3283) AND (status#3283 = completed)) AND isnotnull(customer_id#3271)) AND isnotnull(product_id#3279))\n",
      "            :                                                        :           :        +- FileScan parquet spark_catalog.default.orders_bucketed[customer_id#3271,order_date#3275,order_id#3276,product_id#3279,status#3283,total_amount#3287] Batched: true, Bucketed: true, DataFilters: [isnotnull(status#3283), (status#3283 = completed), isnotnull(customer_id#3271), isnotnull(produc..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://warehouse/orders_bucketed], PartitionFilters: [], PushedFilters: [IsNotNull(status), EqualTo(status,completed), IsNotNull(customer_id), IsNotNull(product_id)], ReadSchema: struct<customer_id:string,order_date:string,order_id:string,product_id:string,status:string,total..., SelectedBucketsCount: 10 out of 10\n",
      "            :                                                        :           +- Sort [customer_id#3312 ASC NULLS FIRST], false, 0\n",
      "            :                                                        :              +- Project [city#3310, customer_id#3312, first_name#3314, last_name#3318, segment#3321]\n",
      "            :                                                        :                 +- Filter ((((isnotnull(age#3309L) AND city#3310 IN (Hanoi,HCM)) AND (age#3309L >= 25)) AND isnotnull(customer_id#3312)) AND isnotnull(city#3310))\n",
      "            :                                                        :                    +- FileScan parquet spark_catalog.default.customers_bucketed[age#3309L,city#3310,customer_id#3312,first_name#3314,last_name#3318,segment#3321] Batched: true, Bucketed: true, DataFilters: [isnotnull(age#3309L), city#3310 IN (Hanoi,HCM), (age#3309L >= 25), isnotnull(customer_id#3312), ..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://warehouse/customers_bucketed], PartitionFilters: [], PushedFilters: [IsNotNull(age), In(city, [HCM,Hanoi]), GreaterThanOrEqual(age,25), IsNotNull(customer_id), IsNot..., ReadSchema: struct<age:bigint,city:string,customer_id:string,first_name:string,last_name:string,segment:string>, SelectedBucketsCount: 10 out of 10\n",
      "            :                                                        +- Sort [product_id#3339 ASC NULLS FIRST], false, 0\n",
      "            :                                                           +- Filter isnotnull(product_id#3339)\n",
      "            :                                                              +- FileScan parquet spark_catalog.default.products_bucketed[product_id#3339] Batched: true, Bucketed: true, DataFilters: [isnotnull(product_id#3339)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://warehouse/products_bucketed], PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<product_id:string>, SelectedBucketsCount: 5 out of 5\n",
      "            +- Sort [city#3845 ASC NULLS FIRST], false, 0\n",
      "               +- HashAggregate(keys=[city#3845], functions=[sum(category_revenue#3807)])\n",
      "                  +- Exchange hashpartitioning(city#3845, 200), ENSURE_REQUIREMENTS, [plan_id=7340]\n",
      "                     +- HashAggregate(keys=[city#3845], functions=[partial_sum(category_revenue#3807)])\n",
      "                        +- HashAggregate(keys=[category#3858, city#3845], functions=[sum(revenue#3518)])\n",
      "                           +- Exchange hashpartitioning(category#3858, city#3845, 200), ENSURE_REQUIREMENTS, [plan_id=7336]\n",
      "                              +- HashAggregate(keys=[category#3858, city#3845], functions=[partial_sum(revenue#3518)])\n",
      "                                 +- Project [city#3845, category#3858, total_amount#3841 AS revenue#3518]\n",
      "                                    +- SortMergeJoin [product_id#3833], [product_id#3861], Inner\n",
      "                                       :- Sort [product_id#3833 ASC NULLS FIRST], false, 0\n",
      "                                       :  +- Exchange hashpartitioning(product_id#3833, 5), ENSURE_REQUIREMENTS, [plan_id=7329]\n",
      "                                       :     +- Project [product_id#3833, total_amount#3841, city#3845]\n",
      "                                       :        +- SortMergeJoin [customer_id#3825], [customer_id#3847], Inner\n",
      "                                       :           :- Sort [customer_id#3825 ASC NULLS FIRST], false, 0\n",
      "                                       :           :  +- Project [customer_id#3825, product_id#3833, total_amount#3841]\n",
      "                                       :           :     +- Filter (((isnotnull(status#3837) AND (status#3837 = completed)) AND isnotnull(customer_id#3825)) AND isnotnull(product_id#3833))\n",
      "                                       :           :        +- FileScan parquet spark_catalog.default.orders_bucketed[customer_id#3825,product_id#3833,status#3837,total_amount#3841] Batched: true, Bucketed: true, DataFilters: [isnotnull(status#3837), (status#3837 = completed), isnotnull(customer_id#3825), isnotnull(produc..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://warehouse/orders_bucketed], PartitionFilters: [], PushedFilters: [IsNotNull(status), EqualTo(status,completed), IsNotNull(customer_id), IsNotNull(product_id)], ReadSchema: struct<customer_id:string,product_id:string,status:string,total_amount:double>, SelectedBucketsCount: 10 out of 10\n",
      "                                       :           +- Sort [customer_id#3847 ASC NULLS FIRST], false, 0\n",
      "                                       :              +- Project [city#3845, customer_id#3847]\n",
      "                                       :                 +- Filter ((((isnotnull(age#3844L) AND city#3845 IN (Hanoi,HCM)) AND (age#3844L >= 25)) AND isnotnull(customer_id#3847)) AND isnotnull(city#3845))\n",
      "                                       :                    +- FileScan parquet spark_catalog.default.customers_bucketed[age#3844L,city#3845,customer_id#3847] Batched: true, Bucketed: true, DataFilters: [isnotnull(age#3844L), city#3845 IN (Hanoi,HCM), (age#3844L >= 25), isnotnull(customer_id#3847), ..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://warehouse/customers_bucketed], PartitionFilters: [], PushedFilters: [IsNotNull(age), In(city, [HCM,Hanoi]), GreaterThanOrEqual(age,25), IsNotNull(customer_id), IsNot..., ReadSchema: struct<age:bigint,city:string,customer_id:string>, SelectedBucketsCount: 10 out of 10\n",
      "                                       +- Sort [product_id#3861 ASC NULLS FIRST], false, 0\n",
      "                                          +- Filter isnotnull(product_id#3861)\n",
      "                                             +- FileScan parquet spark_catalog.default.products_bucketed[category#3858,product_id#3861] Batched: true, Bucketed: true, DataFilters: [isnotnull(product_id#3861)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://warehouse/products_bucketed], PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<category:string,product_id:string>, SelectedBucketsCount: 5 out of 5\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TEST 2: WITH BUCKETING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Read data with bucketing\n",
    "orders_bucketed = spark.table(\"orders_bucketed\")\n",
    "customers_bucketed = spark.table(\"customers_bucketed\")\n",
    "products_bucketed = spark.table(\"products_bucketed\")\n",
    "\n",
    "# Execute complex query\n",
    "result_bucketed, time_bucketed = complex_query(\n",
    "    orders_bucketed,\n",
    "    customers_bucketed,\n",
    "    products_bucketed,\n",
    "    \"WITH BUCKETING\"\n",
    ")\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\nüìä Sample results:\")\n",
    "result_bucketed.show(10, truncate=False)\n",
    "\n",
    "# Show execution plan\n",
    "print(\"\\nüìã Execution Plan:\")\n",
    "result_bucketed.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PERFORMANCE COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PERFORMANCE COMPARISON\n",
      "================================================================================\n",
      "\n",
      "üìä Results:\n",
      "   WITHOUT Bucketing: 7.08s\n",
      "   WITH Bucketing:    6.97s\n",
      "   Speedup:           1.02x\n",
      "   Improvement:       1.6%\n",
      "\n",
      "üìã Comparison Table:\n",
      "           Method  Time (seconds)\n",
      "Without Bucketing        7.081867\n",
      "   With Bucketing        6.971917\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Comparison completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate speedup\n",
    "speedup = time_no_bucket / time_bucketed if time_bucketed > 0 else 0\n",
    "improvement = ((time_no_bucket - time_bucketed) / time_no_bucket * 100) if time_no_bucket > 0 else 0\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"   WITHOUT Bucketing: {time_no_bucket:.2f}s\")\n",
    "print(f\"   WITH Bucketing:    {time_bucketed:.2f}s\")\n",
    "print(f\"   Speedup:           {speedup:.2f}x\")\n",
    "print(f\"   Improvement:       {improvement:.1f}%\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = {\n",
    "    'Method': ['Without Bucketing', 'With Bucketing'],\n",
    "    'Time (seconds)': [time_no_bucket, time_bucketed]\n",
    "}\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\nüìã Comparison Table:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart\n",
    "colors = ['#ff6b6b', '#4ecdc4']\n",
    "bars = ax1.bar(comparison_df['Method'], comparison_df['Time (seconds)'], color=colors)\n",
    "ax1.set_ylabel('Time (seconds)', fontsize=12)\n",
    "ax1.set_title('Execution Time Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2f}s',\n",
    "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Speedup visualization\n",
    "speedup_data = pd.DataFrame({\n",
    "    'Metric': ['Speedup', 'Improvement %'],\n",
    "    'Value': [speedup, improvement]\n",
    "})\n",
    "\n",
    "bars2 = ax2.bar(speedup_data['Metric'], speedup_data['Value'], color=['#95e1d3', '#f38181'])\n",
    "ax2.set_ylabel('Value', fontsize=12)\n",
    "ax2.set_title('Performance Improvement', fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2f}',\n",
    "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Comparison completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DETAILED ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DETAILED ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üîç Execution Plan Analysis:\n",
      "\n",
      "1Ô∏è‚É£ WITHOUT BUCKETING:\n",
      "   - Check for 'Exchange hashpartitioning' (indicates shuffle)\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (69)\n",
      "+- Sort (68)\n",
      "   +- Exchange (67)\n",
      "      +- Project (66)\n",
      "         +- SortMergeJoin Inner (65)\n",
      "            :- Filter (37)\n",
      "            :  +- Window (36)\n",
      "            :     +- WindowGroupLimit (35)\n",
      "            :        +- Sort (34)\n",
      "            :           +- Exchange (33)\n",
      "            :              +- WindowGroupLimit (32)\n",
      "            :                 +- Sort (31)\n",
      "            :                    +- SortAggregate (30)\n",
      "            :                       +- Sort (29)\n",
      "            :                          +- Exchange (28)\n",
      "            :                             +- SortAggregate (27)\n",
      "            :                                +- SortAggregate (26)\n",
      "            :                                   +- Sort (25)\n",
      "            :                                      +- Exchange (24)\n",
      "            :                                         +- SortAggregate (23)\n",
      "            :                                            +- Sort (22)\n",
      "            :                                               +- Expand (21)\n",
      "            :                                                  +- Project (20)\n",
      "            :                                                     +- SortMergeJoin Inner (19)\n",
      "            :                                                        :- Sort (14)\n",
      "            :                                                        :  +- Exchange (13)\n",
      "            :                                                        :     +- Project (12)\n",
      "            :                                                        :        +- SortMergeJoin Inner (11)\n",
      "            :                                                        :           :- Sort (5)\n",
      "            :                                                        :           :  +- Exchange (4)\n",
      "            :                                                        :           :     +- Project (3)\n",
      "            :                                                        :           :        +- Filter (2)\n",
      "            :                                                        :           :           +- Scan parquet  (1)\n",
      "            :                                                        :           +- Sort (10)\n",
      "            :                                                        :              +- Exchange (9)\n",
      "            :                                                        :                 +- Project (8)\n",
      "            :                                                        :                    +- Filter (7)\n",
      "            :                                                        :                       +- Scan parquet  (6)\n",
      "            :                                                        +- Sort (18)\n",
      "            :                                                           +- Exchange (17)\n",
      "            :                                                              +- Filter (16)\n",
      "            :                                                                 +- Scan parquet  (15)\n",
      "            +- Sort (64)\n",
      "               +- HashAggregate (63)\n",
      "                  +- Exchange (62)\n",
      "                     +- HashAggregate (61)\n",
      "                        +- HashAggregate (60)\n",
      "                           +- Exchange (59)\n",
      "                              +- HashAggregate (58)\n",
      "                                 +- Project (57)\n",
      "                                    +- SortMergeJoin Inner (56)\n",
      "                                       :- Sort (51)\n",
      "                                       :  +- Exchange (50)\n",
      "                                       :     +- Project (49)\n",
      "                                       :        +- SortMergeJoin Inner (48)\n",
      "                                       :           :- Sort (42)\n",
      "                                       :           :  +- Exchange (41)\n",
      "                                       :           :     +- Project (40)\n",
      "                                       :           :        +- Filter (39)\n",
      "                                       :           :           +- Scan parquet  (38)\n",
      "                                       :           +- Sort (47)\n",
      "                                       :              +- Exchange (46)\n",
      "                                       :                 +- Project (45)\n",
      "                                       :                    +- Filter (44)\n",
      "                                       :                       +- Scan parquet  (43)\n",
      "                                       +- Sort (55)\n",
      "                                          +- Exchange (54)\n",
      "                                             +- Filter (53)\n",
      "                                                +- Scan parquet  (52)\n",
      "\n",
      "\n",
      "(1) Scan parquet \n",
      "Output [6]: [customer_id#2261, order_date#2265, order_id#2266, product_id#2269, status#2273, total_amount#2277]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [s3a://staging/orders_no_bucket]\n",
      "PushedFilters: [IsNotNull(status), EqualTo(status,completed), IsNotNull(customer_id), IsNotNull(product_id)]\n",
      "ReadSchema: struct<customer_id:string,order_date:string,order_id:string,product_id:string,status:string,total_amount:double>\n",
      "\n",
      "(2) Filter\n",
      "Input [6]: [customer_id#2261, order_date#2265, order_id#2266, product_id#2269, status#2273, total_amount#2277]\n",
      "Condition : (((isnotnull(status#2273) AND (status#2273 = completed)) AND isnotnull(customer_id#2261)) AND isnotnull(product_id#2269))\n",
      "\n",
      "(3) Project\n",
      "Output [5]: [customer_id#2261, order_date#2265, order_id#2266, product_id#2269, total_amount#2277]\n",
      "Input [6]: [customer_id#2261, order_date#2265, order_id#2266, product_id#2269, status#2273, total_amount#2277]\n",
      "\n",
      "(4) Exchange\n",
      "Input [5]: [customer_id#2261, order_date#2265, order_id#2266, product_id#2269, total_amount#2277]\n",
      "Arguments: hashpartitioning(customer_id#2261, 200), ENSURE_REQUIREMENTS, [plan_id=4795]\n",
      "\n",
      "(5) Sort\n",
      "Input [5]: [customer_id#2261, order_date#2265, order_id#2266, product_id#2269, total_amount#2277]\n",
      "Arguments: [customer_id#2261 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(6) Scan parquet \n",
      "Output [6]: [age#2299L, city#2300, customer_id#2302, first_name#2304, last_name#2308, segment#2311]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [s3a://staging/customers_no_bucket]\n",
      "PushedFilters: [IsNotNull(age), In(city, [HCM,Hanoi]), GreaterThanOrEqual(age,25), IsNotNull(customer_id), IsNotNull(city)]\n",
      "ReadSchema: struct<age:bigint,city:string,customer_id:string,first_name:string,last_name:string,segment:string>\n",
      "\n",
      "(7) Filter\n",
      "Input [6]: [age#2299L, city#2300, customer_id#2302, first_name#2304, last_name#2308, segment#2311]\n",
      "Condition : ((((isnotnull(age#2299L) AND city#2300 IN (Hanoi,HCM)) AND (age#2299L >= 25)) AND isnotnull(customer_id#2302)) AND isnotnull(city#2300))\n",
      "\n",
      "(8) Project\n",
      "Output [5]: [city#2300, customer_id#2302, first_name#2304, last_name#2308, segment#2311]\n",
      "Input [6]: [age#2299L, city#2300, customer_id#2302, first_name#2304, last_name#2308, segment#2311]\n",
      "\n",
      "(9) Exchange\n",
      "Input [5]: [city#2300, customer_id#2302, first_name#2304, last_name#2308, segment#2311]\n",
      "Arguments: hashpartitioning(customer_id#2302, 200), ENSURE_REQUIREMENTS, [plan_id=4796]\n",
      "\n",
      "(10) Sort\n",
      "Input [5]: [city#2300, customer_id#2302, first_name#2304, last_name#2308, segment#2311]\n",
      "Arguments: [customer_id#2302 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(11) SortMergeJoin\n",
      "Left keys [1]: [customer_id#2261]\n",
      "Right keys [1]: [customer_id#2302]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(12) Project\n",
      "Output [9]: [order_date#2265, order_id#2266, product_id#2269, total_amount#2277, city#2300, customer_id#2302, first_name#2304, last_name#2308, segment#2311]\n",
      "Input [10]: [customer_id#2261, order_date#2265, order_id#2266, product_id#2269, total_amount#2277, city#2300, customer_id#2302, first_name#2304, last_name#2308, segment#2311]\n",
      "\n",
      "(13) Exchange\n",
      "Input [9]: [order_date#2265, order_id#2266, product_id#2269, total_amount#2277, city#2300, customer_id#2302, first_name#2304, last_name#2308, segment#2311]\n",
      "Arguments: hashpartitioning(product_id#2269, 200), ENSURE_REQUIREMENTS, [plan_id=4803]\n",
      "\n",
      "(14) Sort\n",
      "Input [9]: [order_date#2265, order_id#2266, product_id#2269, total_amount#2277, city#2300, customer_id#2302, first_name#2304, last_name#2308, segment#2311]\n",
      "Arguments: [product_id#2269 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(15) Scan parquet \n",
      "Output [1]: [product_id#2329]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [s3a://staging/products_no_bucket]\n",
      "PushedFilters: [IsNotNull(product_id)]\n",
      "ReadSchema: struct<product_id:string>\n",
      "\n",
      "(16) Filter\n",
      "Input [1]: [product_id#2329]\n",
      "Condition : isnotnull(product_id#2329)\n",
      "\n",
      "(17) Exchange\n",
      "Input [1]: [product_id#2329]\n",
      "Arguments: hashpartitioning(product_id#2329, 200), ENSURE_REQUIREMENTS, [plan_id=4804]\n",
      "\n",
      "(18) Sort\n",
      "Input [1]: [product_id#2329]\n",
      "Arguments: [product_id#2329 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(19) SortMergeJoin\n",
      "Left keys [1]: [product_id#2269]\n",
      "Right keys [1]: [product_id#2329]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(20) Project\n",
      "Output [9]: [order_date#2265, order_id#2266, product_id#2269, city#2300, customer_id#2302, first_name#2304, last_name#2308, segment#2311, total_amount#2277 AS revenue#2508]\n",
      "Input [10]: [order_date#2265, order_id#2266, product_id#2269, total_amount#2277, city#2300, customer_id#2302, first_name#2304, last_name#2308, segment#2311, product_id#2329]\n",
      "\n",
      "(21) Expand\n",
      "Input [9]: [order_date#2265, order_id#2266, product_id#2269, city#2300, customer_id#2302, first_name#2304, last_name#2308, segment#2311, revenue#2508]\n",
      "Arguments: [[customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, null, null, 0, revenue#2508, order_date#2265], [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, order_id#2266, null, 1, null, null], [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, null, product_id#2269, 2, null, null]], [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, o.order_id#3237, o.product_id#3238, gid#3236, revenue#3239, o.order_date#3240]\n",
      "\n",
      "(22) Sort\n",
      "Input [10]: [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, o.order_id#3237, o.product_id#3238, gid#3236, revenue#3239, o.order_date#3240]\n",
      "Arguments: [customer_id#2302 ASC NULLS FIRST, first_name#2304 ASC NULLS FIRST, last_name#2308 ASC NULLS FIRST, city#2300 ASC NULLS FIRST, segment#2311 ASC NULLS FIRST, o.order_id#3237 ASC NULLS FIRST, o.product_id#3238 ASC NULLS FIRST, gid#3236 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(23) SortAggregate\n",
      "Input [10]: [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, o.order_id#3237, o.product_id#3238, gid#3236, revenue#3239, o.order_date#3240]\n",
      "Keys [8]: [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, o.order_id#3237, o.product_id#3238, gid#3236]\n",
      "Functions [3]: [partial_sum(revenue#3239), partial_avg(revenue#3239), partial_max(o.order_date#3240)]\n",
      "Aggregate Attributes [4]: [sum#3263, sum#3264, count#3265L, max#3266]\n",
      "Results [12]: [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, o.order_id#3237, o.product_id#3238, gid#3236, sum#3267, sum#3268, count#3269L, max#3270]\n",
      "\n",
      "(24) Exchange\n",
      "Input [12]: [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, o.order_id#3237, o.product_id#3238, gid#3236, sum#3267, sum#3268, count#3269L, max#3270]\n",
      "Arguments: hashpartitioning(customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, o.order_id#3237, o.product_id#3238, gid#3236, 200), ENSURE_REQUIREMENTS, [plan_id=4814]\n",
      "\n",
      "(25) Sort\n",
      "Input [12]: [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, o.order_id#3237, o.product_id#3238, gid#3236, sum#3267, sum#3268, count#3269L, max#3270]\n",
      "Arguments: [customer_id#2302 ASC NULLS FIRST, first_name#2304 ASC NULLS FIRST, last_name#2308 ASC NULLS FIRST, city#2300 ASC NULLS FIRST, segment#2311 ASC NULLS FIRST, o.order_id#3237 ASC NULLS FIRST, o.product_id#3238 ASC NULLS FIRST, gid#3236 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(26) SortAggregate\n",
      "Input [12]: [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, o.order_id#3237, o.product_id#3238, gid#3236, sum#3267, sum#3268, count#3269L, max#3270]\n",
      "Keys [8]: [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, o.order_id#3237, o.product_id#3238, gid#3236]\n",
      "Functions [3]: [sum(revenue#3239), avg(revenue#3239), max(o.order_date#3240)]\n",
      "Aggregate Attributes [3]: [sum(revenue#3239)#2706, avg(revenue#3239)#2708, max(o.order_date#3240)#2711]\n",
      "Results [11]: [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, o.order_id#3237, o.product_id#3238, gid#3236, sum(revenue#3239)#2706 AS sum(revenue)#3241, avg(revenue#3239)#2708 AS avg(revenue)#3243, max(o.order_date#3240)#2711 AS max(o.order_date)#3245]\n",
      "\n",
      "(27) SortAggregate\n",
      "Input [11]: [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, o.order_id#3237, o.product_id#3238, gid#3236, sum(revenue)#3241, avg(revenue)#3243, max(o.order_date)#3245]\n",
      "Keys [5]: [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311]\n",
      "Functions [5]: [partial_count(o.order_id#3237) FILTER (WHERE (gid#3236 = 1)), partial_first(sum(revenue)#3241, true) FILTER (WHERE (gid#3236 = 0)), partial_first(avg(revenue)#3243, true) FILTER (WHERE (gid#3236 = 0)), partial_count(o.product_id#3238) FILTER (WHERE (gid#3236 = 2)), partial_first(max(o.order_date)#3245, true) FILTER (WHERE (gid#3236 = 0))]\n",
      "Aggregate Attributes [8]: [count#3247L, first#3248, valueSet#3249, first#3250, valueSet#3251, count#3252L, first#3253, valueSet#3254]\n",
      "Results [13]: [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, count#3255L, first#3256, valueSet#3257, first#3258, valueSet#3259, count#3260L, first#3261, valueSet#3262]\n",
      "\n",
      "(28) Exchange\n",
      "Input [13]: [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, count#3255L, first#3256, valueSet#3257, first#3258, valueSet#3259, count#3260L, first#3261, valueSet#3262]\n",
      "Arguments: hashpartitioning(customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, 200), ENSURE_REQUIREMENTS, [plan_id=4819]\n",
      "\n",
      "(29) Sort\n",
      "Input [13]: [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, count#3255L, first#3256, valueSet#3257, first#3258, valueSet#3259, count#3260L, first#3261, valueSet#3262]\n",
      "Arguments: [customer_id#2302 ASC NULLS FIRST, first_name#2304 ASC NULLS FIRST, last_name#2308 ASC NULLS FIRST, city#2300 ASC NULLS FIRST, segment#2311 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(30) SortAggregate\n",
      "Input [13]: [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, count#3255L, first#3256, valueSet#3257, first#3258, valueSet#3259, count#3260L, first#3261, valueSet#3262]\n",
      "Keys [5]: [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311]\n",
      "Functions [5]: [count(o.order_id#3237), first(sum(revenue)#3241, true), first(avg(revenue)#3243, true), count(o.product_id#3238), first(max(o.order_date)#3245, true)]\n",
      "Aggregate Attributes [5]: [count(o.order_id#3237)#2713L, first(sum(revenue)#3241) ignore nulls#3242, first(avg(revenue)#3243) ignore nulls#3244, count(o.product_id#3238)#2714L, first(max(o.order_date)#3245) ignore nulls#3246]\n",
      "Results [10]: [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, count(o.order_id#3237)#2713L AS total_orders#2705L, first(sum(revenue)#3241) ignore nulls#3242 AS total_revenue#2707, first(avg(revenue)#3243) ignore nulls#3244 AS avg_revenue#2709, count(o.product_id#3238)#2714L AS unique_products#2710L, first(max(o.order_date)#3245) ignore nulls#3246 AS last_order_date#2712]\n",
      "\n",
      "(31) Sort\n",
      "Input [10]: [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, total_orders#2705L, total_revenue#2707, avg_revenue#2709, unique_products#2710L, last_order_date#2712]\n",
      "Arguments: [city#2300 ASC NULLS FIRST, total_revenue#2707 DESC NULLS LAST], false, 0\n",
      "\n",
      "(32) WindowGroupLimit\n",
      "Input [10]: [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, total_orders#2705L, total_revenue#2707, avg_revenue#2709, unique_products#2710L, last_order_date#2712]\n",
      "Arguments: [city#2300], [total_revenue#2707 DESC NULLS LAST], rank(total_revenue#2707), 100, Partial\n",
      "\n",
      "(33) Exchange\n",
      "Input [10]: [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, total_orders#2705L, total_revenue#2707, avg_revenue#2709, unique_products#2710L, last_order_date#2712]\n",
      "Arguments: hashpartitioning(city#2300, 200), ENSURE_REQUIREMENTS, [plan_id=4826]\n",
      "\n",
      "(34) Sort\n",
      "Input [10]: [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, total_orders#2705L, total_revenue#2707, avg_revenue#2709, unique_products#2710L, last_order_date#2712]\n",
      "Arguments: [city#2300 ASC NULLS FIRST, total_revenue#2707 DESC NULLS LAST], false, 0\n",
      "\n",
      "(35) WindowGroupLimit\n",
      "Input [10]: [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, total_orders#2705L, total_revenue#2707, avg_revenue#2709, unique_products#2710L, last_order_date#2712]\n",
      "Arguments: [city#2300], [total_revenue#2707 DESC NULLS LAST], rank(total_revenue#2707), 100, Final\n",
      "\n",
      "(36) Window\n",
      "Input [10]: [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, total_orders#2705L, total_revenue#2707, avg_revenue#2709, unique_products#2710L, last_order_date#2712]\n",
      "Arguments: [rank(total_revenue#2707) windowspecdefinition(city#2300, total_revenue#2707 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_in_city#2727], [city#2300], [total_revenue#2707 DESC NULLS LAST]\n",
      "\n",
      "(37) Filter\n",
      "Input [11]: [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, total_orders#2705L, total_revenue#2707, avg_revenue#2709, unique_products#2710L, last_order_date#2712, rank_in_city#2727]\n",
      "Condition : (rank_in_city#2727 <= 100)\n",
      "\n",
      "(38) Scan parquet \n",
      "Output [4]: [customer_id#2815, product_id#2823, status#2827, total_amount#2831]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [s3a://staging/orders_no_bucket]\n",
      "PushedFilters: [IsNotNull(status), EqualTo(status,completed), IsNotNull(customer_id), IsNotNull(product_id)]\n",
      "ReadSchema: struct<customer_id:string,product_id:string,status:string,total_amount:double>\n",
      "\n",
      "(39) Filter\n",
      "Input [4]: [customer_id#2815, product_id#2823, status#2827, total_amount#2831]\n",
      "Condition : (((isnotnull(status#2827) AND (status#2827 = completed)) AND isnotnull(customer_id#2815)) AND isnotnull(product_id#2823))\n",
      "\n",
      "(40) Project\n",
      "Output [3]: [customer_id#2815, product_id#2823, total_amount#2831]\n",
      "Input [4]: [customer_id#2815, product_id#2823, status#2827, total_amount#2831]\n",
      "\n",
      "(41) Exchange\n",
      "Input [3]: [customer_id#2815, product_id#2823, total_amount#2831]\n",
      "Arguments: hashpartitioning(customer_id#2815, 200), ENSURE_REQUIREMENTS, [plan_id=4832]\n",
      "\n",
      "(42) Sort\n",
      "Input [3]: [customer_id#2815, product_id#2823, total_amount#2831]\n",
      "Arguments: [customer_id#2815 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(43) Scan parquet \n",
      "Output [3]: [age#2834L, city#2835, customer_id#2837]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [s3a://staging/customers_no_bucket]\n",
      "PushedFilters: [IsNotNull(age), In(city, [HCM,Hanoi]), GreaterThanOrEqual(age,25), IsNotNull(customer_id), IsNotNull(city)]\n",
      "ReadSchema: struct<age:bigint,city:string,customer_id:string>\n",
      "\n",
      "(44) Filter\n",
      "Input [3]: [age#2834L, city#2835, customer_id#2837]\n",
      "Condition : ((((isnotnull(age#2834L) AND city#2835 IN (Hanoi,HCM)) AND (age#2834L >= 25)) AND isnotnull(customer_id#2837)) AND isnotnull(city#2835))\n",
      "\n",
      "(45) Project\n",
      "Output [2]: [city#2835, customer_id#2837]\n",
      "Input [3]: [age#2834L, city#2835, customer_id#2837]\n",
      "\n",
      "(46) Exchange\n",
      "Input [2]: [city#2835, customer_id#2837]\n",
      "Arguments: hashpartitioning(customer_id#2837, 200), ENSURE_REQUIREMENTS, [plan_id=4833]\n",
      "\n",
      "(47) Sort\n",
      "Input [2]: [city#2835, customer_id#2837]\n",
      "Arguments: [customer_id#2837 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(48) SortMergeJoin\n",
      "Left keys [1]: [customer_id#2815]\n",
      "Right keys [1]: [customer_id#2837]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(49) Project\n",
      "Output [3]: [product_id#2823, total_amount#2831, city#2835]\n",
      "Input [5]: [customer_id#2815, product_id#2823, total_amount#2831, city#2835, customer_id#2837]\n",
      "\n",
      "(50) Exchange\n",
      "Input [3]: [product_id#2823, total_amount#2831, city#2835]\n",
      "Arguments: hashpartitioning(product_id#2823, 200), ENSURE_REQUIREMENTS, [plan_id=4840]\n",
      "\n",
      "(51) Sort\n",
      "Input [3]: [product_id#2823, total_amount#2831, city#2835]\n",
      "Arguments: [product_id#2823 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(52) Scan parquet \n",
      "Output [2]: [category#2848, product_id#2851]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [s3a://staging/products_no_bucket]\n",
      "PushedFilters: [IsNotNull(product_id)]\n",
      "ReadSchema: struct<category:string,product_id:string>\n",
      "\n",
      "(53) Filter\n",
      "Input [2]: [category#2848, product_id#2851]\n",
      "Condition : isnotnull(product_id#2851)\n",
      "\n",
      "(54) Exchange\n",
      "Input [2]: [category#2848, product_id#2851]\n",
      "Arguments: hashpartitioning(product_id#2851, 200), ENSURE_REQUIREMENTS, [plan_id=4841]\n",
      "\n",
      "(55) Sort\n",
      "Input [2]: [category#2848, product_id#2851]\n",
      "Arguments: [product_id#2851 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(56) SortMergeJoin\n",
      "Left keys [1]: [product_id#2823]\n",
      "Right keys [1]: [product_id#2851]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(57) Project\n",
      "Output [3]: [city#2835, category#2848, total_amount#2831 AS revenue#2508]\n",
      "Input [5]: [product_id#2823, total_amount#2831, city#2835, category#2848, product_id#2851]\n",
      "\n",
      "(58) HashAggregate\n",
      "Input [3]: [city#2835, category#2848, revenue#2508]\n",
      "Keys [2]: [category#2848, city#2835]\n",
      "Functions [1]: [partial_sum(revenue#2508)]\n",
      "Aggregate Attributes [1]: [sum#3093]\n",
      "Results [3]: [category#2848, city#2835, sum#3094]\n",
      "\n",
      "(59) Exchange\n",
      "Input [3]: [category#2848, city#2835, sum#3094]\n",
      "Arguments: hashpartitioning(category#2848, city#2835, 200), ENSURE_REQUIREMENTS, [plan_id=4848]\n",
      "\n",
      "(60) HashAggregate\n",
      "Input [3]: [category#2848, city#2835, sum#3094]\n",
      "Keys [2]: [category#2848, city#2835]\n",
      "Functions [1]: [sum(revenue#2508)]\n",
      "Aggregate Attributes [1]: [sum(revenue#2508)#2796]\n",
      "Results [2]: [city#2835, sum(revenue#2508)#2796 AS category_revenue#2797]\n",
      "\n",
      "(61) HashAggregate\n",
      "Input [2]: [city#2835, category_revenue#2797]\n",
      "Keys [1]: [city#2835]\n",
      "Functions [1]: [partial_sum(category_revenue#2797)]\n",
      "Aggregate Attributes [1]: [sum#3091]\n",
      "Results [2]: [city#2835, sum#3092]\n",
      "\n",
      "(62) Exchange\n",
      "Input [2]: [city#2835, sum#3092]\n",
      "Arguments: hashpartitioning(city#2835, 200), ENSURE_REQUIREMENTS, [plan_id=4852]\n",
      "\n",
      "(63) HashAggregate\n",
      "Input [2]: [city#2835, sum#3092]\n",
      "Keys [1]: [city#2835]\n",
      "Functions [1]: [sum(category_revenue#2797)]\n",
      "Aggregate Attributes [1]: [sum(category_revenue#2797)#2811]\n",
      "Results [2]: [city#2835, sum(category_revenue#2797)#2811 AS city_total_revenue#2812]\n",
      "\n",
      "(64) Sort\n",
      "Input [2]: [city#2835, city_total_revenue#2812]\n",
      "Arguments: [city#2835 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(65) SortMergeJoin\n",
      "Left keys [1]: [city#2300]\n",
      "Right keys [1]: [city#2835]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(66) Project\n",
      "Output [13]: [city#2300, customer_id#2302, first_name#2304, last_name#2308, segment#2311, total_orders#2705L, total_revenue#2707, avg_revenue#2709, unique_products#2710L, last_order_date#2712, rank_in_city#2727, city_total_revenue#2812, ((total_revenue#2707 / city_total_revenue#2812) * 100.0) AS revenue_percentage#2875]\n",
      "Input [13]: [customer_id#2302, first_name#2304, last_name#2308, city#2300, segment#2311, total_orders#2705L, total_revenue#2707, avg_revenue#2709, unique_products#2710L, last_order_date#2712, rank_in_city#2727, city#2835, city_total_revenue#2812]\n",
      "\n",
      "(67) Exchange\n",
      "Input [13]: [city#2300, customer_id#2302, first_name#2304, last_name#2308, segment#2311, total_orders#2705L, total_revenue#2707, avg_revenue#2709, unique_products#2710L, last_order_date#2712, rank_in_city#2727, city_total_revenue#2812, revenue_percentage#2875]\n",
      "Arguments: rangepartitioning(total_revenue#2707 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=4860]\n",
      "\n",
      "(68) Sort\n",
      "Input [13]: [city#2300, customer_id#2302, first_name#2304, last_name#2308, segment#2311, total_orders#2705L, total_revenue#2707, avg_revenue#2709, unique_products#2710L, last_order_date#2712, rank_in_city#2727, city_total_revenue#2812, revenue_percentage#2875]\n",
      "Arguments: [total_revenue#2707 DESC NULLS LAST], true, 0\n",
      "\n",
      "(69) AdaptiveSparkPlan\n",
      "Output [13]: [city#2300, customer_id#2302, first_name#2304, last_name#2308, segment#2311, total_orders#2705L, total_revenue#2707, avg_revenue#2709, unique_products#2710L, last_order_date#2712, rank_in_city#2727, city_total_revenue#2812, revenue_percentage#2875]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "\n",
      "2Ô∏è‚É£ WITH BUCKETING:\n",
      "   - Should show 'Bucketed: true'\n",
      "   - Fewer 'Exchange' operations\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (63)\n",
      "+- Sort (62)\n",
      "   +- Exchange (61)\n",
      "      +- Project (60)\n",
      "         +- SortMergeJoin Inner (59)\n",
      "            :- Filter (34)\n",
      "            :  +- Window (33)\n",
      "            :     +- WindowGroupLimit (32)\n",
      "            :        +- Sort (31)\n",
      "            :           +- Exchange (30)\n",
      "            :              +- WindowGroupLimit (29)\n",
      "            :                 +- Sort (28)\n",
      "            :                    +- SortAggregate (27)\n",
      "            :                       +- Sort (26)\n",
      "            :                          +- Exchange (25)\n",
      "            :                             +- SortAggregate (24)\n",
      "            :                                +- SortAggregate (23)\n",
      "            :                                   +- Sort (22)\n",
      "            :                                      +- Exchange (21)\n",
      "            :                                         +- SortAggregate (20)\n",
      "            :                                            +- Sort (19)\n",
      "            :                                               +- Expand (18)\n",
      "            :                                                  +- Project (17)\n",
      "            :                                                     +- SortMergeJoin Inner (16)\n",
      "            :                                                        :- Sort (12)\n",
      "            :                                                        :  +- Exchange (11)\n",
      "            :                                                        :     +- Project (10)\n",
      "            :                                                        :        +- SortMergeJoin Inner (9)\n",
      "            :                                                        :           :- Sort (4)\n",
      "            :                                                        :           :  +- Project (3)\n",
      "            :                                                        :           :     +- Filter (2)\n",
      "            :                                                        :           :        +- Scan parquet spark_catalog.default.orders_bucketed (1)\n",
      "            :                                                        :           +- Sort (8)\n",
      "            :                                                        :              +- Project (7)\n",
      "            :                                                        :                 +- Filter (6)\n",
      "            :                                                        :                    +- Scan parquet spark_catalog.default.customers_bucketed (5)\n",
      "            :                                                        +- Sort (15)\n",
      "            :                                                           +- Filter (14)\n",
      "            :                                                              +- Scan parquet spark_catalog.default.products_bucketed (13)\n",
      "            +- Sort (58)\n",
      "               +- HashAggregate (57)\n",
      "                  +- Exchange (56)\n",
      "                     +- HashAggregate (55)\n",
      "                        +- HashAggregate (54)\n",
      "                           +- Exchange (53)\n",
      "                              +- HashAggregate (52)\n",
      "                                 +- Project (51)\n",
      "                                    +- SortMergeJoin Inner (50)\n",
      "                                       :- Sort (46)\n",
      "                                       :  +- Exchange (45)\n",
      "                                       :     +- Project (44)\n",
      "                                       :        +- SortMergeJoin Inner (43)\n",
      "                                       :           :- Sort (38)\n",
      "                                       :           :  +- Project (37)\n",
      "                                       :           :     +- Filter (36)\n",
      "                                       :           :        +- Scan parquet spark_catalog.default.orders_bucketed (35)\n",
      "                                       :           +- Sort (42)\n",
      "                                       :              +- Project (41)\n",
      "                                       :                 +- Filter (40)\n",
      "                                       :                    +- Scan parquet spark_catalog.default.customers_bucketed (39)\n",
      "                                       +- Sort (49)\n",
      "                                          +- Filter (48)\n",
      "                                             +- Scan parquet spark_catalog.default.products_bucketed (47)\n",
      "\n",
      "\n",
      "(1) Scan parquet spark_catalog.default.orders_bucketed\n",
      "Output [6]: [customer_id#3271, order_date#3275, order_id#3276, product_id#3279, status#3283, total_amount#3287]\n",
      "Batched: true\n",
      "Bucketed: true\n",
      "Location: InMemoryFileIndex [s3a://warehouse/orders_bucketed]\n",
      "PushedFilters: [IsNotNull(status), EqualTo(status,completed), IsNotNull(customer_id), IsNotNull(product_id)]\n",
      "ReadSchema: struct<customer_id:string,order_date:string,order_id:string,product_id:string,status:string,total_amount:double>\n",
      "SelectedBucketsCount: 10 out of 10\n",
      "\n",
      "(2) Filter\n",
      "Input [6]: [customer_id#3271, order_date#3275, order_id#3276, product_id#3279, status#3283, total_amount#3287]\n",
      "Condition : (((isnotnull(status#3283) AND (status#3283 = completed)) AND isnotnull(customer_id#3271)) AND isnotnull(product_id#3279))\n",
      "\n",
      "(3) Project\n",
      "Output [5]: [customer_id#3271, order_date#3275, order_id#3276, product_id#3279, total_amount#3287]\n",
      "Input [6]: [customer_id#3271, order_date#3275, order_id#3276, product_id#3279, status#3283, total_amount#3287]\n",
      "\n",
      "(4) Sort\n",
      "Input [5]: [customer_id#3271, order_date#3275, order_id#3276, product_id#3279, total_amount#3287]\n",
      "Arguments: [customer_id#3271 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(5) Scan parquet spark_catalog.default.customers_bucketed\n",
      "Output [6]: [age#3309L, city#3310, customer_id#3312, first_name#3314, last_name#3318, segment#3321]\n",
      "Batched: true\n",
      "Bucketed: true\n",
      "Location: InMemoryFileIndex [s3a://warehouse/customers_bucketed]\n",
      "PushedFilters: [IsNotNull(age), In(city, [HCM,Hanoi]), GreaterThanOrEqual(age,25), IsNotNull(customer_id), IsNotNull(city)]\n",
      "ReadSchema: struct<age:bigint,city:string,customer_id:string,first_name:string,last_name:string,segment:string>\n",
      "SelectedBucketsCount: 10 out of 10\n",
      "\n",
      "(6) Filter\n",
      "Input [6]: [age#3309L, city#3310, customer_id#3312, first_name#3314, last_name#3318, segment#3321]\n",
      "Condition : ((((isnotnull(age#3309L) AND city#3310 IN (Hanoi,HCM)) AND (age#3309L >= 25)) AND isnotnull(customer_id#3312)) AND isnotnull(city#3310))\n",
      "\n",
      "(7) Project\n",
      "Output [5]: [city#3310, customer_id#3312, first_name#3314, last_name#3318, segment#3321]\n",
      "Input [6]: [age#3309L, city#3310, customer_id#3312, first_name#3314, last_name#3318, segment#3321]\n",
      "\n",
      "(8) Sort\n",
      "Input [5]: [city#3310, customer_id#3312, first_name#3314, last_name#3318, segment#3321]\n",
      "Arguments: [customer_id#3312 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(9) SortMergeJoin\n",
      "Left keys [1]: [customer_id#3271]\n",
      "Right keys [1]: [customer_id#3312]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(10) Project\n",
      "Output [9]: [order_date#3275, order_id#3276, product_id#3279, total_amount#3287, city#3310, customer_id#3312, first_name#3314, last_name#3318, segment#3321]\n",
      "Input [10]: [customer_id#3271, order_date#3275, order_id#3276, product_id#3279, total_amount#3287, city#3310, customer_id#3312, first_name#3314, last_name#3318, segment#3321]\n",
      "\n",
      "(11) Exchange\n",
      "Input [9]: [order_date#3275, order_id#3276, product_id#3279, total_amount#3287, city#3310, customer_id#3312, first_name#3314, last_name#3318, segment#3321]\n",
      "Arguments: hashpartitioning(product_id#3279, 5), ENSURE_REQUIREMENTS, [plan_id=7294]\n",
      "\n",
      "(12) Sort\n",
      "Input [9]: [order_date#3275, order_id#3276, product_id#3279, total_amount#3287, city#3310, customer_id#3312, first_name#3314, last_name#3318, segment#3321]\n",
      "Arguments: [product_id#3279 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(13) Scan parquet spark_catalog.default.products_bucketed\n",
      "Output [1]: [product_id#3339]\n",
      "Batched: true\n",
      "Bucketed: true\n",
      "Location: InMemoryFileIndex [s3a://warehouse/products_bucketed]\n",
      "PushedFilters: [IsNotNull(product_id)]\n",
      "ReadSchema: struct<product_id:string>\n",
      "SelectedBucketsCount: 5 out of 5\n",
      "\n",
      "(14) Filter\n",
      "Input [1]: [product_id#3339]\n",
      "Condition : isnotnull(product_id#3339)\n",
      "\n",
      "(15) Sort\n",
      "Input [1]: [product_id#3339]\n",
      "Arguments: [product_id#3339 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(16) SortMergeJoin\n",
      "Left keys [1]: [product_id#3279]\n",
      "Right keys [1]: [product_id#3339]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(17) Project\n",
      "Output [9]: [order_date#3275, order_id#3276, product_id#3279, city#3310, customer_id#3312, first_name#3314, last_name#3318, segment#3321, total_amount#3287 AS revenue#3518]\n",
      "Input [10]: [order_date#3275, order_id#3276, product_id#3279, total_amount#3287, city#3310, customer_id#3312, first_name#3314, last_name#3318, segment#3321, product_id#3339]\n",
      "\n",
      "(18) Expand\n",
      "Input [9]: [order_date#3275, order_id#3276, product_id#3279, city#3310, customer_id#3312, first_name#3314, last_name#3318, segment#3321, revenue#3518]\n",
      "Arguments: [[customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, null, null, 0, revenue#3518, order_date#3275], [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, order_id#3276, null, 1, null, null], [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, null, product_id#3279, 2, null, null]], [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, o.order_id#4208, o.product_id#4209, gid#4207, revenue#4210, o.order_date#4211]\n",
      "\n",
      "(19) Sort\n",
      "Input [10]: [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, o.order_id#4208, o.product_id#4209, gid#4207, revenue#4210, o.order_date#4211]\n",
      "Arguments: [customer_id#3312 ASC NULLS FIRST, first_name#3314 ASC NULLS FIRST, last_name#3318 ASC NULLS FIRST, city#3310 ASC NULLS FIRST, segment#3321 ASC NULLS FIRST, o.order_id#4208 ASC NULLS FIRST, o.product_id#4209 ASC NULLS FIRST, gid#4207 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(20) SortAggregate\n",
      "Input [10]: [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, o.order_id#4208, o.product_id#4209, gid#4207, revenue#4210, o.order_date#4211]\n",
      "Keys [8]: [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, o.order_id#4208, o.product_id#4209, gid#4207]\n",
      "Functions [3]: [partial_sum(revenue#4210), partial_avg(revenue#4210), partial_max(o.order_date#4211)]\n",
      "Aggregate Attributes [4]: [sum#4234, sum#4235, count#4236L, max#4237]\n",
      "Results [12]: [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, o.order_id#4208, o.product_id#4209, gid#4207, sum#4238, sum#4239, count#4240L, max#4241]\n",
      "\n",
      "(21) Exchange\n",
      "Input [12]: [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, o.order_id#4208, o.product_id#4209, gid#4207, sum#4238, sum#4239, count#4240L, max#4241]\n",
      "Arguments: hashpartitioning(customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, o.order_id#4208, o.product_id#4209, gid#4207, 200), ENSURE_REQUIREMENTS, [plan_id=7304]\n",
      "\n",
      "(22) Sort\n",
      "Input [12]: [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, o.order_id#4208, o.product_id#4209, gid#4207, sum#4238, sum#4239, count#4240L, max#4241]\n",
      "Arguments: [customer_id#3312 ASC NULLS FIRST, first_name#3314 ASC NULLS FIRST, last_name#3318 ASC NULLS FIRST, city#3310 ASC NULLS FIRST, segment#3321 ASC NULLS FIRST, o.order_id#4208 ASC NULLS FIRST, o.product_id#4209 ASC NULLS FIRST, gid#4207 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(23) SortAggregate\n",
      "Input [12]: [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, o.order_id#4208, o.product_id#4209, gid#4207, sum#4238, sum#4239, count#4240L, max#4241]\n",
      "Keys [8]: [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, o.order_id#4208, o.product_id#4209, gid#4207]\n",
      "Functions [3]: [sum(revenue#4210), avg(revenue#4210), max(o.order_date#4211)]\n",
      "Aggregate Attributes [3]: [sum(revenue#4210)#3716, avg(revenue#4210)#3718, max(o.order_date#4211)#3721]\n",
      "Results [11]: [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, o.order_id#4208, o.product_id#4209, gid#4207, sum(revenue#4210)#3716 AS sum(revenue)#4212, avg(revenue#4210)#3718 AS avg(revenue)#4214, max(o.order_date#4211)#3721 AS max(o.order_date)#4216]\n",
      "\n",
      "(24) SortAggregate\n",
      "Input [11]: [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, o.order_id#4208, o.product_id#4209, gid#4207, sum(revenue)#4212, avg(revenue)#4214, max(o.order_date)#4216]\n",
      "Keys [5]: [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321]\n",
      "Functions [5]: [partial_count(o.order_id#4208) FILTER (WHERE (gid#4207 = 1)), partial_first(sum(revenue)#4212, true) FILTER (WHERE (gid#4207 = 0)), partial_first(avg(revenue)#4214, true) FILTER (WHERE (gid#4207 = 0)), partial_count(o.product_id#4209) FILTER (WHERE (gid#4207 = 2)), partial_first(max(o.order_date)#4216, true) FILTER (WHERE (gid#4207 = 0))]\n",
      "Aggregate Attributes [8]: [count#4218L, first#4219, valueSet#4220, first#4221, valueSet#4222, count#4223L, first#4224, valueSet#4225]\n",
      "Results [13]: [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, count#4226L, first#4227, valueSet#4228, first#4229, valueSet#4230, count#4231L, first#4232, valueSet#4233]\n",
      "\n",
      "(25) Exchange\n",
      "Input [13]: [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, count#4226L, first#4227, valueSet#4228, first#4229, valueSet#4230, count#4231L, first#4232, valueSet#4233]\n",
      "Arguments: hashpartitioning(customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, 200), ENSURE_REQUIREMENTS, [plan_id=7309]\n",
      "\n",
      "(26) Sort\n",
      "Input [13]: [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, count#4226L, first#4227, valueSet#4228, first#4229, valueSet#4230, count#4231L, first#4232, valueSet#4233]\n",
      "Arguments: [customer_id#3312 ASC NULLS FIRST, first_name#3314 ASC NULLS FIRST, last_name#3318 ASC NULLS FIRST, city#3310 ASC NULLS FIRST, segment#3321 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(27) SortAggregate\n",
      "Input [13]: [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, count#4226L, first#4227, valueSet#4228, first#4229, valueSet#4230, count#4231L, first#4232, valueSet#4233]\n",
      "Keys [5]: [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321]\n",
      "Functions [5]: [count(o.order_id#4208), first(sum(revenue)#4212, true), first(avg(revenue)#4214, true), count(o.product_id#4209), first(max(o.order_date)#4216, true)]\n",
      "Aggregate Attributes [5]: [count(o.order_id#4208)#3723L, first(sum(revenue)#4212) ignore nulls#4213, first(avg(revenue)#4214) ignore nulls#4215, count(o.product_id#4209)#3724L, first(max(o.order_date)#4216) ignore nulls#4217]\n",
      "Results [10]: [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, count(o.order_id#4208)#3723L AS total_orders#3715L, first(sum(revenue)#4212) ignore nulls#4213 AS total_revenue#3717, first(avg(revenue)#4214) ignore nulls#4215 AS avg_revenue#3719, count(o.product_id#4209)#3724L AS unique_products#3720L, first(max(o.order_date)#4216) ignore nulls#4217 AS last_order_date#3722]\n",
      "\n",
      "(28) Sort\n",
      "Input [10]: [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, total_orders#3715L, total_revenue#3717, avg_revenue#3719, unique_products#3720L, last_order_date#3722]\n",
      "Arguments: [city#3310 ASC NULLS FIRST, total_revenue#3717 DESC NULLS LAST], false, 0\n",
      "\n",
      "(29) WindowGroupLimit\n",
      "Input [10]: [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, total_orders#3715L, total_revenue#3717, avg_revenue#3719, unique_products#3720L, last_order_date#3722]\n",
      "Arguments: [city#3310], [total_revenue#3717 DESC NULLS LAST], rank(total_revenue#3717), 100, Partial\n",
      "\n",
      "(30) Exchange\n",
      "Input [10]: [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, total_orders#3715L, total_revenue#3717, avg_revenue#3719, unique_products#3720L, last_order_date#3722]\n",
      "Arguments: hashpartitioning(city#3310, 200), ENSURE_REQUIREMENTS, [plan_id=7316]\n",
      "\n",
      "(31) Sort\n",
      "Input [10]: [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, total_orders#3715L, total_revenue#3717, avg_revenue#3719, unique_products#3720L, last_order_date#3722]\n",
      "Arguments: [city#3310 ASC NULLS FIRST, total_revenue#3717 DESC NULLS LAST], false, 0\n",
      "\n",
      "(32) WindowGroupLimit\n",
      "Input [10]: [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, total_orders#3715L, total_revenue#3717, avg_revenue#3719, unique_products#3720L, last_order_date#3722]\n",
      "Arguments: [city#3310], [total_revenue#3717 DESC NULLS LAST], rank(total_revenue#3717), 100, Final\n",
      "\n",
      "(33) Window\n",
      "Input [10]: [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, total_orders#3715L, total_revenue#3717, avg_revenue#3719, unique_products#3720L, last_order_date#3722]\n",
      "Arguments: [rank(total_revenue#3717) windowspecdefinition(city#3310, total_revenue#3717 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_in_city#3737], [city#3310], [total_revenue#3717 DESC NULLS LAST]\n",
      "\n",
      "(34) Filter\n",
      "Input [11]: [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, total_orders#3715L, total_revenue#3717, avg_revenue#3719, unique_products#3720L, last_order_date#3722, rank_in_city#3737]\n",
      "Condition : (rank_in_city#3737 <= 100)\n",
      "\n",
      "(35) Scan parquet spark_catalog.default.orders_bucketed\n",
      "Output [4]: [customer_id#3825, product_id#3833, status#3837, total_amount#3841]\n",
      "Batched: true\n",
      "Bucketed: true\n",
      "Location: InMemoryFileIndex [s3a://warehouse/orders_bucketed]\n",
      "PushedFilters: [IsNotNull(status), EqualTo(status,completed), IsNotNull(customer_id), IsNotNull(product_id)]\n",
      "ReadSchema: struct<customer_id:string,product_id:string,status:string,total_amount:double>\n",
      "SelectedBucketsCount: 10 out of 10\n",
      "\n",
      "(36) Filter\n",
      "Input [4]: [customer_id#3825, product_id#3833, status#3837, total_amount#3841]\n",
      "Condition : (((isnotnull(status#3837) AND (status#3837 = completed)) AND isnotnull(customer_id#3825)) AND isnotnull(product_id#3833))\n",
      "\n",
      "(37) Project\n",
      "Output [3]: [customer_id#3825, product_id#3833, total_amount#3841]\n",
      "Input [4]: [customer_id#3825, product_id#3833, status#3837, total_amount#3841]\n",
      "\n",
      "(38) Sort\n",
      "Input [3]: [customer_id#3825, product_id#3833, total_amount#3841]\n",
      "Arguments: [customer_id#3825 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(39) Scan parquet spark_catalog.default.customers_bucketed\n",
      "Output [3]: [age#3844L, city#3845, customer_id#3847]\n",
      "Batched: true\n",
      "Bucketed: true\n",
      "Location: InMemoryFileIndex [s3a://warehouse/customers_bucketed]\n",
      "PushedFilters: [IsNotNull(age), In(city, [HCM,Hanoi]), GreaterThanOrEqual(age,25), IsNotNull(customer_id), IsNotNull(city)]\n",
      "ReadSchema: struct<age:bigint,city:string,customer_id:string>\n",
      "SelectedBucketsCount: 10 out of 10\n",
      "\n",
      "(40) Filter\n",
      "Input [3]: [age#3844L, city#3845, customer_id#3847]\n",
      "Condition : ((((isnotnull(age#3844L) AND city#3845 IN (Hanoi,HCM)) AND (age#3844L >= 25)) AND isnotnull(customer_id#3847)) AND isnotnull(city#3845))\n",
      "\n",
      "(41) Project\n",
      "Output [2]: [city#3845, customer_id#3847]\n",
      "Input [3]: [age#3844L, city#3845, customer_id#3847]\n",
      "\n",
      "(42) Sort\n",
      "Input [2]: [city#3845, customer_id#3847]\n",
      "Arguments: [customer_id#3847 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(43) SortMergeJoin\n",
      "Left keys [1]: [customer_id#3825]\n",
      "Right keys [1]: [customer_id#3847]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(44) Project\n",
      "Output [3]: [product_id#3833, total_amount#3841, city#3845]\n",
      "Input [5]: [customer_id#3825, product_id#3833, total_amount#3841, city#3845, customer_id#3847]\n",
      "\n",
      "(45) Exchange\n",
      "Input [3]: [product_id#3833, total_amount#3841, city#3845]\n",
      "Arguments: hashpartitioning(product_id#3833, 5), ENSURE_REQUIREMENTS, [plan_id=7329]\n",
      "\n",
      "(46) Sort\n",
      "Input [3]: [product_id#3833, total_amount#3841, city#3845]\n",
      "Arguments: [product_id#3833 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(47) Scan parquet spark_catalog.default.products_bucketed\n",
      "Output [2]: [category#3858, product_id#3861]\n",
      "Batched: true\n",
      "Bucketed: true\n",
      "Location: InMemoryFileIndex [s3a://warehouse/products_bucketed]\n",
      "PushedFilters: [IsNotNull(product_id)]\n",
      "ReadSchema: struct<category:string,product_id:string>\n",
      "SelectedBucketsCount: 5 out of 5\n",
      "\n",
      "(48) Filter\n",
      "Input [2]: [category#3858, product_id#3861]\n",
      "Condition : isnotnull(product_id#3861)\n",
      "\n",
      "(49) Sort\n",
      "Input [2]: [category#3858, product_id#3861]\n",
      "Arguments: [product_id#3861 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(50) SortMergeJoin\n",
      "Left keys [1]: [product_id#3833]\n",
      "Right keys [1]: [product_id#3861]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(51) Project\n",
      "Output [3]: [city#3845, category#3858, total_amount#3841 AS revenue#3518]\n",
      "Input [5]: [product_id#3833, total_amount#3841, city#3845, category#3858, product_id#3861]\n",
      "\n",
      "(52) HashAggregate\n",
      "Input [3]: [city#3845, category#3858, revenue#3518]\n",
      "Keys [2]: [category#3858, city#3845]\n",
      "Functions [1]: [partial_sum(revenue#3518)]\n",
      "Aggregate Attributes [1]: [sum#4084]\n",
      "Results [3]: [category#3858, city#3845, sum#4085]\n",
      "\n",
      "(53) Exchange\n",
      "Input [3]: [category#3858, city#3845, sum#4085]\n",
      "Arguments: hashpartitioning(category#3858, city#3845, 200), ENSURE_REQUIREMENTS, [plan_id=7336]\n",
      "\n",
      "(54) HashAggregate\n",
      "Input [3]: [category#3858, city#3845, sum#4085]\n",
      "Keys [2]: [category#3858, city#3845]\n",
      "Functions [1]: [sum(revenue#3518)]\n",
      "Aggregate Attributes [1]: [sum(revenue#3518)#3806]\n",
      "Results [2]: [city#3845, sum(revenue#3518)#3806 AS category_revenue#3807]\n",
      "\n",
      "(55) HashAggregate\n",
      "Input [2]: [city#3845, category_revenue#3807]\n",
      "Keys [1]: [city#3845]\n",
      "Functions [1]: [partial_sum(category_revenue#3807)]\n",
      "Aggregate Attributes [1]: [sum#4082]\n",
      "Results [2]: [city#3845, sum#4083]\n",
      "\n",
      "(56) Exchange\n",
      "Input [2]: [city#3845, sum#4083]\n",
      "Arguments: hashpartitioning(city#3845, 200), ENSURE_REQUIREMENTS, [plan_id=7340]\n",
      "\n",
      "(57) HashAggregate\n",
      "Input [2]: [city#3845, sum#4083]\n",
      "Keys [1]: [city#3845]\n",
      "Functions [1]: [sum(category_revenue#3807)]\n",
      "Aggregate Attributes [1]: [sum(category_revenue#3807)#3821]\n",
      "Results [2]: [city#3845, sum(category_revenue#3807)#3821 AS city_total_revenue#3822]\n",
      "\n",
      "(58) Sort\n",
      "Input [2]: [city#3845, city_total_revenue#3822]\n",
      "Arguments: [city#3845 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(59) SortMergeJoin\n",
      "Left keys [1]: [city#3310]\n",
      "Right keys [1]: [city#3845]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(60) Project\n",
      "Output [13]: [city#3310, customer_id#3312, first_name#3314, last_name#3318, segment#3321, total_orders#3715L, total_revenue#3717, avg_revenue#3719, unique_products#3720L, last_order_date#3722, rank_in_city#3737, city_total_revenue#3822, ((total_revenue#3717 / city_total_revenue#3822) * 100.0) AS revenue_percentage#3885]\n",
      "Input [13]: [customer_id#3312, first_name#3314, last_name#3318, city#3310, segment#3321, total_orders#3715L, total_revenue#3717, avg_revenue#3719, unique_products#3720L, last_order_date#3722, rank_in_city#3737, city#3845, city_total_revenue#3822]\n",
      "\n",
      "(61) Exchange\n",
      "Input [13]: [city#3310, customer_id#3312, first_name#3314, last_name#3318, segment#3321, total_orders#3715L, total_revenue#3717, avg_revenue#3719, unique_products#3720L, last_order_date#3722, rank_in_city#3737, city_total_revenue#3822, revenue_percentage#3885]\n",
      "Arguments: rangepartitioning(total_revenue#3717 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=7348]\n",
      "\n",
      "(62) Sort\n",
      "Input [13]: [city#3310, customer_id#3312, first_name#3314, last_name#3318, segment#3321, total_orders#3715L, total_revenue#3717, avg_revenue#3719, unique_products#3720L, last_order_date#3722, rank_in_city#3737, city_total_revenue#3822, revenue_percentage#3885]\n",
      "Arguments: [total_revenue#3717 DESC NULLS LAST], true, 0\n",
      "\n",
      "(63) AdaptiveSparkPlan\n",
      "Output [13]: [city#3310, customer_id#3312, first_name#3314, last_name#3318, segment#3321, total_orders#3715L, total_revenue#3717, avg_revenue#3719, unique_products#3720L, last_order_date#3722, rank_in_city#3737, city_total_revenue#3822, revenue_percentage#3885]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "\n",
      "üìä Shuffle Analysis:\n",
      "   WITHOUT Bucketing: Multiple shuffle operations on joins\n",
      "   WITH Bucketing:    Reduced/eliminated shuffle on bucketed columns\n",
      "\n",
      "üíæ Memory Efficiency:\n",
      "   WITHOUT Bucketing: Higher memory usage due to shuffle\n",
      "   WITH Bucketing:    Lower memory usage, data pre-organized\n",
      "\n",
      "üìà Scalability:\n",
      "   WITHOUT Bucketing: Performance degrades with data size\n",
      "   WITH Bucketing:    Better scalability for large datasets\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DETAILED ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analyze execution plans\n",
    "print(\"\\nüîç Execution Plan Analysis:\")\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ WITHOUT BUCKETING:\")\n",
    "print(\"   - Check for 'Exchange hashpartitioning' (indicates shuffle)\")\n",
    "result_no_bucket.explain(mode=\"formatted\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ WITH BUCKETING:\")\n",
    "print(\"   - Should show 'Bucketed: true'\")\n",
    "print(\"   - Fewer 'Exchange' operations\")\n",
    "result_bucketed.explain(mode=\"formatted\")\n",
    "\n",
    "# Compare shuffle operations\n",
    "print(\"\\nüìä Shuffle Analysis:\")\n",
    "print(f\"   WITHOUT Bucketing: Multiple shuffle operations on joins\")\n",
    "print(f\"   WITH Bucketing:    Reduced/eliminated shuffle on bucketed columns\")\n",
    "\n",
    "# Memory usage comparison\n",
    "print(\"\\nüíæ Memory Efficiency:\")\n",
    "print(f\"   WITHOUT Bucketing: Higher memory usage due to shuffle\")\n",
    "print(f\"   WITH Bucketing:    Lower memory usage, data pre-organized\")\n",
    "\n",
    "# Scalability\n",
    "print(\"\\nüìà Scalability:\")\n",
    "print(f\"   WITHOUT Bucketing: Performance degrades with data size\")\n",
    "print(f\"   WITH Bucketing:    Better scalability for large datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDITIONAL TESTS (OPTIONAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ADDITIONAL PERFORMANCE TESTS\n",
      "================================================================================\n",
      "\n",
      "üß™ Test 1: Simple JOIN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Test 2: JOIN + GROUP BY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 220:============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Test 3: Multiple JOINs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_USING_COLUMN_FOR_JOIN] USING column `product` cannot be resolved on the left side of the join. The left-side columns: [`age`, `city`, `country`, `customer_id`, `discount_amount`, `discount_percent`, `email`, `first_name`, `gender`, `is_active`, `join_date`, `last_name`, `lifetime_value`, `notes`, `order_date`, `order_id`, `order_timestamp`, `payment_method`, `phone`, `product_id`, `quantity`, `segment`, `shipping_cost`, `shipping_method`, `status`, `subtotal`, `tax_amount`, `tax_percent`, `total_amount`, `tracking_number`, `unit_price`].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müß™ Test 3: Multiple JOINs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 47\u001b[0m \u001b[43morders_no_bucket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcustomers_no_bucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcustomer_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproducts_no_bucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproduct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m     49\u001b[0m time_multi_no_bucket \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     51\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/dataframe.py:2491\u001b[0m, in \u001b[0;36mDataFrame.join\u001b[0;34m(self, other, on, how)\u001b[0m\n\u001b[1;32m   2489\u001b[0m         on \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jseq([])\n\u001b[1;32m   2490\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(how, \u001b[38;5;28mstr\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhow should be a string\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2491\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_USING_COLUMN_FOR_JOIN] USING column `product` cannot be resolved on the left side of the join. The left-side columns: [`age`, `city`, `country`, `customer_id`, `discount_amount`, `discount_percent`, `email`, `first_name`, `gender`, `is_active`, `join_date`, `last_name`, `lifetime_value`, `notes`, `order_date`, `order_id`, `order_timestamp`, `payment_method`, `phone`, `product_id`, `quantity`, `segment`, `shipping_cost`, `shipping_method`, `status`, `subtotal`, `tax_amount`, `tax_percent`, `total_amount`, `tracking_number`, `unit_price`]."
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ADDITIONAL PERFORMANCE TESTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test different operations\n",
    "test_results = []\n",
    "\n",
    "# Test 1: Simple JOIN\n",
    "print(\"\\nüß™ Test 1: Simple JOIN\")\n",
    "start = time.time()\n",
    "orders_no_bucket.join(customers_no_bucket, \"customer_id\").count()\n",
    "time_simple_no_bucket = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "orders_bucketed.join(customers_bucketed, \"customer_id\").count()\n",
    "time_simple_bucketed = time.time() - start\n",
    "\n",
    "test_results.append({\n",
    "    'Test': 'Simple JOIN',\n",
    "    'Without Bucketing': time_simple_no_bucket,\n",
    "    'With Bucketing': time_simple_bucketed,\n",
    "    'Speedup': time_simple_no_bucket / time_simple_bucketed\n",
    "})\n",
    "\n",
    "# Test 2: JOIN + GROUP BY\n",
    "print(\"\\nüß™ Test 2: JOIN + GROUP BY\")\n",
    "start = time.time()\n",
    "orders_no_bucket.join(customers_no_bucket, \"customer_id\") \\\n",
    "    .groupBy(\"customer_id\").agg(count(\"*\").alias(\"cnt\")).count()\n",
    "time_groupby_no_bucket = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "orders_bucketed.join(customers_bucketed, \"customer_id\") \\\n",
    "    .groupBy(\"customer_id\").agg(count(\"*\").alias(\"cnt\")).count()\n",
    "time_groupby_bucketed = time.time() - start\n",
    "\n",
    "test_results.append({\n",
    "    'Test': 'JOIN + GROUP BY',\n",
    "    'Without Bucketing': time_groupby_no_bucket,\n",
    "    'With Bucketing': time_groupby_bucketed,\n",
    "    'Speedup': time_groupby_no_bucket / time_groupby_bucketed\n",
    "})\n",
    "\n",
    "# Test 3: Multiple JOINs\n",
    "print(\"\\nüß™ Test 3: Multiple JOINs\")\n",
    "start = time.time()\n",
    "orders_no_bucket.join(customers_no_bucket, \"customer_id\") \\\n",
    "    .join(products_no_bucket, \"product\").count()\n",
    "time_multi_no_bucket = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "orders_bucketed.join(customers_bucketed, \"customer_id\") \\\n",
    "    .join(products_bucketed, \"product\").count()\n",
    "time_multi_bucketed = time.time() - start\n",
    "\n",
    "test_results.append({\n",
    "    'Test': 'Multiple JOINs',\n",
    "    'Without Bucketing': time_multi_no_bucket,\n",
    "    'With Bucketing': time_multi_bucketed,\n",
    "    'Speedup': time_multi_no_bucket / time_multi_bucketed\n",
    "})\n",
    "\n",
    "# Display results\n",
    "test_df = pd.DataFrame(test_results)\n",
    "print(\"\\nüìä Test Results:\")\n",
    "print(test_df.to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = range(len(test_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar([i - width/2 for i in x], test_df['Without Bucketing'], \n",
    "               width, label='Without Bucketing', color='#ff6b6b')\n",
    "bars2 = ax.bar([i + width/2 for i in x], test_df['With Bucketing'], \n",
    "               width, label='With Bucketing', color='#4ecdc4')\n",
    "\n",
    "ax.set_xlabel('Test', fontsize=12)\n",
    "ax.set_ylabel('Time (seconds)', fontsize=12)\n",
    "ax.set_title('Performance Comparison Across Different Operations', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(test_df['Test'])\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Additional tests completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMMARY & RECOMMENDATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SUMMARY & RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = f\"\"\"\n",
    "üìä PERFORMANCE SUMMARY:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "Complex Query Performance:\n",
    "   ‚Ä¢ WITHOUT Bucketing: {time_no_bucket:.2f}s\n",
    "   ‚Ä¢ WITH Bucketing:    {time_bucketed:.2f}s\n",
    "   ‚Ä¢ Speedup:           {speedup:.2f}x\n",
    "   ‚Ä¢ Improvement:       {improvement:.1f}%\n",
    "\n",
    "Key Findings:\n",
    "   ‚úÖ Bucketing eliminates shuffle on join keys\n",
    "   ‚úÖ Significant performance improvement for complex queries\n",
    "   ‚úÖ Better scalability for large datasets\n",
    "   ‚úÖ Reduced memory usage\n",
    "\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "üí° RECOMMENDATIONS:\n",
    "\n",
    "1. WHEN TO USE BUCKETING:\n",
    "   ‚úì Large tables (> 1GB)\n",
    "   ‚úì Frequent joins on same columns\n",
    "   ‚úì Repeated aggregations\n",
    "   ‚úì High cardinality join keys\n",
    "\n",
    "2. BEST PRACTICES:\n",
    "   ‚úì Choose high-cardinality columns for bucketing\n",
    "   ‚úì Use same bucket count for tables being joined\n",
    "   ‚úì Bucket count = 2^n (power of 2) for better distribution\n",
    "   ‚úì Enable bucketing: spark.sql.sources.bucketing.enabled=true\n",
    "\n",
    "3. OPTIMAL BUCKET COUNT:\n",
    "   ‚úì Small tables (< 1GB):     4-10 buckets\n",
    "   ‚úì Medium tables (1-10GB):   10-50 buckets\n",
    "   ‚úì Large tables (> 10GB):    50-200 buckets\n",
    "\n",
    "4. STORAGE CONSIDERATIONS:\n",
    "   ‚úì Development: Use local filesystem (faster)\n",
    "   ‚úì Production: Use MinIO/S3 (persistent, distributed)\n",
    "   ‚úì Both support bucketing!\n",
    "\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save results to file\n",
    "results_dict = {\n",
    "    'complex_query_no_bucket': time_no_bucket,\n",
    "    'complex_query_bucketed': time_bucketed,\n",
    "    'speedup': speedup,\n",
    "    'improvement_percent': improvement,\n",
    "    'test_results': test_results\n",
    "}\n",
    "\n",
    "print(\"\\nüíæ Results saved to memory\")\n",
    "print(\"‚úÖ Analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLEANUP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üóëÔ∏è  CLEANUP: DELETING ALL DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. DROP ALL SPARK TABLES\n",
    "# ============================================================================\n",
    "print(\"\\nüìã Dropping Spark tables...\")\n",
    "\n",
    "tables_to_drop = [\n",
    "    \"orders_bucketed\",\n",
    "    \"customers_bucketed\",\n",
    "    \"products_bucketed\",\n",
    "    \"orders_bucketed_s3\",\n",
    "    \"customers_bucketed_s3\",\n",
    "    \"orders_bucketed_local\",\n",
    "    \"customers_bucketed_local\"\n",
    "]\n",
    "\n",
    "for table in tables_to_drop:\n",
    "    try:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {table}\")\n",
    "        print(f\"   ‚úÖ Dropped: {table}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  {table}: {str(e)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. DELETE LOCAL FILES\n",
    "# ============================================================================\n",
    "print(\"\\nüìÅ Deleting local files...\")\n",
    "\n",
    "local_paths = [\n",
    "    \"/opt/spark-data/staging/orders_no_bucket\",\n",
    "    \"/opt/spark-data/staging/customers_no_bucket\",\n",
    "    \"/opt/spark-data/staging/products_no_bucket\",\n",
    "    \"/opt/spark-data/warehouse/orders_bucketed\",\n",
    "    \"/opt/spark-data/warehouse/customers_bucketed\",\n",
    "    \"/opt/spark-data/warehouse/products_bucketed\",\n",
    "    \"/opt/spark-data/warehouse/orders_bucketed_local\",\n",
    "    \"/opt/spark-data/warehouse/customers_bucketed_local\"\n",
    "]\n",
    "\n",
    "for path in local_paths:\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            shutil.rmtree(path)\n",
    "            print(f\"   üóëÔ∏è  Deleted: {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  {path}: {str(e)}\")\n",
    "    else:\n",
    "        print(f\"   ‚ÑπÔ∏è  Not exists: {path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. DELETE MINIO (S3) FILES\n",
    "# ============================================================================\n",
    "print(\"\\n‚òÅÔ∏è  Deleting MinIO files...\")\n",
    "\n",
    "# List of S3 paths to delete\n",
    "s3_paths = [\n",
    "    \"s3a://staging/orders_no_bucket\",\n",
    "    \"s3a://staging/customers_no_bucket\",\n",
    "    \"s3a://staging/products_no_bucket\",\n",
    "    \"s3a://warehouse/orders_bucketed\",\n",
    "    \"s3a://warehouse/customers_bucketed\",\n",
    "    \"s3a://warehouse/products_bucketed\"\n",
    "]\n",
    "\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "for s3_path in s3_paths:\n",
    "    try:\n",
    "        # Try to read to check if exists\n",
    "        spark.read.parquet(s3_path)\n",
    "        \n",
    "        # If exists, delete using Hadoop FileSystem API\n",
    "        hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "        fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "            spark._jvm.java.net.URI(s3_path),\n",
    "            hadoop_conf\n",
    "        )\n",
    "        path = spark._jvm.org.apache.hadoop.fs.Path(s3_path)\n",
    "        fs.delete(path, True)  # True = recursive delete\n",
    "        \n",
    "        print(f\"   üóëÔ∏è  Deleted: {s3_path}\")\n",
    "    except AnalysisException:\n",
    "        print(f\"   ‚ÑπÔ∏è  Not exists: {s3_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  {s3_path}: {str(e)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. VERIFY CLEANUP\n",
    "# ============================================================================\n",
    "print(\"\\n‚úÖ Cleanup verification:\")\n",
    "\n",
    "# Check local warehouse\n",
    "warehouse_dir = \"/opt/spark-data/warehouse\"\n",
    "if os.path.exists(warehouse_dir):\n",
    "    remaining = os.listdir(warehouse_dir)\n",
    "    if remaining:\n",
    "        print(f\"   ‚ö†Ô∏è  Remaining in warehouse: {remaining}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Warehouse is empty\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Warehouse directory doesn't exist\")\n",
    "\n",
    "# Check local staging\n",
    "staging_dir = \"/opt/spark-data/staging\"\n",
    "if os.path.exists(staging_dir):\n",
    "    remaining = os.listdir(staging_dir)\n",
    "    if remaining:\n",
    "        print(f\"   ‚ö†Ô∏è  Remaining in staging: {remaining}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Staging is empty\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Staging directory doesn't exist\")\n",
    "\n",
    "print(\"\\nüéâ Cleanup completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RECOMMENDED ARCHITECTURE\n",
    "# ============================================================================\n",
    "\n",
    "# üìÅ BRONZE (Raw) - Fast ingestion\n",
    "# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "orders_raw.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"order_date\") \\\n",
    "    .parquet(\"s3a://bronze/orders/\")\n",
    "\n",
    "# Retention: 7 days\n",
    "# Storage: 70 GB\n",
    "# Ingestion: Fast (15s/1M rows)\n",
    "\n",
    "# üìÅ SILVER (Analytics-ready) - With bucketing\n",
    "# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "# Daily ETL: Bronze ‚Üí Silver (with deduplication)\n",
    "new_orders = spark.read.parquet(\"s3a://bronze/orders/order_date=2024-01-04/\")\n",
    "\n",
    "# Deduplicate\n",
    "existing_ids = spark.table(\"silver.orders\").select(\"order_id\")\n",
    "new_only = new_orders.join(existing_ids, \"order_id\", \"left_anti\")\n",
    "\n",
    "# Append to silver\n",
    "new_only.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"order_date\") \\\n",
    "    .bucketBy(50, \"customer_id\") \\  # Single key\n",
    "    .sortBy(\"customer_id\") \\\n",
    "    .saveAsTable(\"silver.orders\")\n",
    "\n",
    "# Retention: 1 year\n",
    "# Storage: 3.6 TB\n",
    "# Transformation: 45s/1M rows (once per day)\n",
    "\n",
    "# üìÅ GOLD (Business metrics) - Aggregated\n",
    "# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "customer_metrics = spark.sql(\"\"\"\n",
    "    SELECT customer_id, order_date, SUM(amount) as revenue\n",
    "    FROM silver.orders\n",
    "    GROUP BY customer_id, order_date\n",
    "\"\"\")\n",
    "\n",
    "customer_metrics.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"order_date\") \\\n",
    "    .parquet(\"s3a://gold/customer_metrics/\")\n",
    "\n",
    "# Retention: Long-term\n",
    "# Storage: 365 GB\n",
    "# Query: Very fast (pre-aggregated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
