{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßπ DATA CLEANING WITH PYSPARK\n",
    "\n",
    "---\n",
    "\n",
    "## üìã **OBJECTIVES**\n",
    "\n",
    "1. Handle missing values (null, NaN)\n",
    "2. Remove duplicates\n",
    "3. Data type conversions\n",
    "4. String cleaning & transformations\n",
    "5. Outlier detection\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß **SETUP SPARK SESSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/04 17:50:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark Session Created\n",
      "Spark Version: 3.5.1\n",
      "Master: spark://spark-master:7077\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataCleaning\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark Session Created\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä **1. CREATE DIRTY DATASET**\n",
    "\n",
    "T·∫°o dataset c√≥ nhi·ªÅu v·∫•n ƒë·ªÅ ƒë·ªÉ th·ª±c h√†nh cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Original Dirty Data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-----------------+----+--------+-----------------+-------+\n",
      "|customer_id|name           |email            |age |salary  |registration_date|country|\n",
      "+-----------+---------------+-----------------+----+--------+-----------------+-------+\n",
      "|CUST001    |John Doe       |john@email.com   |25  |50000.0 |2024-01-01       |USA    |\n",
      "|CUST002    |Jane Smith     |JANE@EMAIL.COM   |30  |60000.0 |2024-01-02       |UK     |\n",
      "|CUST003    |  Bob Johnson  |bob@email.com    |NULL|55000.0 |2024-01-03       |Canada |\n",
      "|CUST004    |Alice Brown    |NULL             |28  |70000.0 |2024-01-04       |USA    |\n",
      "|CUST005    |Charlie Wilson |charlie@email.com|35  |NULL    |2024-01-05       |UK     |\n",
      "|CUST001    |John Doe       |john@email.com   |25  |50000.0 |2024-01-01       |USA    |\n",
      "|CUST006    |NULL           |david@email.com  |40  |80000.0 |2024-01-06       |Canada |\n",
      "|CUST007    |Eve Davis      |eve@email.com    |-5  |90000.0 |2024-01-07       |USA    |\n",
      "|CUST008    |Frank Miller   |frank@email.com  |150 |100000.0|2024-01-08       |UK     |\n",
      "|CUST009    |Grace Lee      |grace@email.com  |32  |-10000.0|2024-01-09       |Canada |\n",
      "|CUST010    |Henry Taylor   |HENRY@EMAIL.COM  |29  |65000.0 |invalid-date     |USA    |\n",
      "|CUST011    |Ivy Anderson   |ivy@email.com    |27  |58000.0 |2024-01-11       |NULL   |\n",
      "|CUST012    |Jack Thomas    |jack@email.com   |33  |72000.0 |2024-01-12       |  UK   |\n",
      "|CUST013    |KAREN JACKSON  |karen@email.com  |31  |68000.0 |2024-01-13       |usa    |\n",
      "|CUST014    |Leo White      |leo@email.com    |26  |54000.0 |2024-01-14       |Canada |\n",
      "|CUST015    |Mia Harris     |mia@email.com    |NULL|NULL    |NULL             |NULL   |\n",
      "+-----------+---------------+-----------------+----+--------+-----------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total rows: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# T·∫°o dirty data v·ªõi nhi·ªÅu v·∫•n ƒë·ªÅ\n",
    "dirty_data = [\n",
    "    (\"CUST001\", \"John Doe\", \"john@email.com\", 25, 50000.0, \"2024-01-01\", \"USA\"),\n",
    "    (\"CUST002\", \"Jane Smith\", \"JANE@EMAIL.COM\", 30, 60000.0, \"2024-01-02\", \"UK\"),\n",
    "    (\"CUST003\", \"  Bob Johnson  \", \"bob@email.com\", None, 55000.0, \"2024-01-03\", \"Canada\"),  # Missing age\n",
    "    (\"CUST004\", \"Alice Brown\", None, 28, 70000.0, \"2024-01-04\", \"USA\"),  # Missing email\n",
    "    (\"CUST005\", \"Charlie Wilson\", \"charlie@email.com\", 35, None, \"2024-01-05\", \"UK\"),  # Missing salary\n",
    "    (\"CUST001\", \"John Doe\", \"john@email.com\", 25, 50000.0, \"2024-01-01\", \"USA\"),  # Duplicate\n",
    "    (\"CUST006\", None, \"david@email.com\", 40, 80000.0, \"2024-01-06\", \"Canada\"),  # Missing name\n",
    "    (\"CUST007\", \"Eve Davis\", \"eve@email.com\", -5, 90000.0, \"2024-01-07\", \"USA\"),  # Invalid age\n",
    "    (\"CUST008\", \"Frank Miller\", \"frank@email.com\", 150, 100000.0, \"2024-01-08\", \"UK\"),  # Outlier age\n",
    "    (\"CUST009\", \"Grace Lee\", \"grace@email.com\", 32, -10000.0, \"2024-01-09\", \"Canada\"),  # Invalid salary\n",
    "    (\"CUST010\", \"Henry Taylor\", \"HENRY@EMAIL.COM\", 29, 65000.0, \"invalid-date\", \"USA\"),  # Invalid date\n",
    "    (\"CUST011\", \"Ivy Anderson\", \"ivy@email.com\", 27, 58000.0, \"2024-01-11\", None),  # Missing country\n",
    "    (\"CUST012\", \"Jack Thomas\", \"jack@email.com\", 33, 72000.0, \"2024-01-12\", \"  UK  \"),  # Whitespace\n",
    "    (\"CUST013\", \"KAREN JACKSON\", \"karen@email.com\", 31, 68000.0, \"2024-01-13\", \"usa\"),  # Case inconsistency\n",
    "    (\"CUST014\", \"Leo White\", \"leo@email.com\", 26, 54000.0, \"2024-01-14\", \"Canada\"),\n",
    "    (\"CUST015\", \"Mia Harris\", \"mia@email.com\", None, None, None, None),  # All nulls except ID\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True),\n",
    "    StructField(\"registration_date\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(dirty_data, schema)\n",
    "\n",
    "print(\"üìä Original Dirty Data:\")\n",
    "df.show(20, truncate=False)\n",
    "print(f\"\\nTotal rows: {df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç **2. DATA PROFILING**\n",
    "\n",
    "Ph√¢n t√≠ch data ƒë·ªÉ hi·ªÉu v·∫•n ƒë·ªÅ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã SCHEMA:\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- registration_date: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n",
      "\n",
      "üìä SUMMARY STATISTICS:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/04 17:50:45 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------------+---------------+------------------+-----------------+-----------------+-------+\n",
      "|summary|customer_id|           name|          email|               age|           salary|registration_date|country|\n",
      "+-------+-----------+---------------+---------------+------------------+-----------------+-----------------+-------+\n",
      "|  count|         16|             15|             15|                14|               14|               15|     14|\n",
      "|   mean|       NULL|           NULL|           NULL|36.142857142857146|61571.42857142857|             NULL|   NULL|\n",
      "| stddev|       NULL|           NULL|           NULL| 34.32392559326319| 25364.1609232527|             NULL|   NULL|\n",
      "|    min|    CUST001|  Bob Johnson  |HENRY@EMAIL.COM|                -5|         -10000.0|       2024-01-01|   UK  |\n",
      "|    max|    CUST015|     Mia Harris|  mia@email.com|               150|         100000.0|     invalid-date|    usa|\n",
      "+-------+-----------+---------------+---------------+------------------+-----------------+-----------------+-------+\n",
      "\n",
      "\n",
      "‚ùå NULL COUNTS:\n",
      "+-----------+----+-----+---+------+-----------------+-------+\n",
      "|customer_id|name|email|age|salary|registration_date|country|\n",
      "+-----------+----+-----+---+------+-----------------+-------+\n",
      "|          0|   1|    1|  2|     2|                1|      2|\n",
      "+-----------+----+-----+---+------+-----------------+-------+\n",
      "\n",
      "\n",
      "üîÑ DUPLICATE CHECK:\n",
      "Total rows: 16\n",
      "Distinct rows: 15\n",
      "Duplicates: 1\n",
      "\n",
      "üìà VALUE COUNTS (Country):\n",
      "+-------+-----+\n",
      "|country|count|\n",
      "+-------+-----+\n",
      "|    USA|    5|\n",
      "| Canada|    4|\n",
      "|     UK|    3|\n",
      "|   NULL|    2|\n",
      "|    usa|    1|\n",
      "|   UK  |    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Schema & Data Types\n",
    "print(\"üìã SCHEMA:\")\n",
    "df.printSchema()\n",
    "\n",
    "# 2.2 Summary Statistics\n",
    "print(\"\\nüìä SUMMARY STATISTICS:\")\n",
    "df.describe().show()\n",
    "\n",
    "# 2.3 Count nulls per column\n",
    "print(\"\\n‚ùå NULL COUNTS:\")\n",
    "null_counts = df.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in df.columns\n",
    "])\n",
    "null_counts.show()\n",
    "\n",
    "# 2.4 Duplicate check\n",
    "print(\"\\nüîÑ DUPLICATE CHECK:\")\n",
    "total_rows = df.count()\n",
    "distinct_rows = df.distinct().count()\n",
    "duplicates = total_rows - distinct_rows\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Distinct rows: {distinct_rows}\")\n",
    "print(f\"Duplicates: {duplicates}\")\n",
    "\n",
    "# 2.5 Value counts per column\n",
    "print(\"\\nüìà VALUE COUNTS (Country):\")\n",
    "df.groupBy(\"country\").count().orderBy(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßπ **3. HANDLE MISSING VALUES**\n",
    "\n",
    "### **3.1 Identify Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå ROWS WITH NULL VALUES:\n",
      "+-----------+---------------+-----------------+----+-------+-----------------+-------+\n",
      "|customer_id|name           |email            |age |salary |registration_date|country|\n",
      "+-----------+---------------+-----------------+----+-------+-----------------+-------+\n",
      "|CUST003    |  Bob Johnson  |bob@email.com    |NULL|55000.0|2024-01-03       |Canada |\n",
      "|CUST004    |Alice Brown    |NULL             |28  |70000.0|2024-01-04       |USA    |\n",
      "|CUST005    |Charlie Wilson |charlie@email.com|35  |NULL   |2024-01-05       |UK     |\n",
      "|CUST006    |NULL           |david@email.com  |40  |80000.0|2024-01-06       |Canada |\n",
      "|CUST011    |Ivy Anderson   |ivy@email.com    |27  |58000.0|2024-01-11       |NULL   |\n",
      "|CUST015    |Mia Harris     |mia@email.com    |NULL|NULL   |NULL             |NULL   |\n",
      "+-----------+---------------+-----------------+----+-------+-----------------+-------+\n",
      "\n",
      "Rows with nulls: 6\n"
     ]
    }
   ],
   "source": [
    "# Show rows with any null\n",
    "print(\"‚ùå ROWS WITH NULL VALUES:\")\n",
    "df_with_nulls = df.filter(\n",
    "    col(\"name\").isNull() | \n",
    "    col(\"email\").isNull() | \n",
    "    col(\"age\").isNull() | \n",
    "    col(\"salary\").isNull() | \n",
    "    col(\"country\").isNull()\n",
    ")\n",
    "df_with_nulls.show(truncate=False)\n",
    "print(f\"Rows with nulls: {df_with_nulls.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 Drop Rows with Nulls**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Drop ANY null: 16 ‚Üí 10 rows\n",
      "‚úÖ Drop ALL null: 16 ‚Üí 16 rows\n",
      "‚úÖ Drop null in [customer_id, email]: 16 ‚Üí 15 rows\n",
      "‚úÖ Drop rows with < 5 non-nulls: 16 ‚Üí 15 rows\n"
     ]
    }
   ],
   "source": [
    "# Strategy 1: Drop rows with ANY null\n",
    "df_drop_any = df.dropna(how=\"any\")\n",
    "print(f\"‚úÖ Drop ANY null: {df.count()} ‚Üí {df_drop_any.count()} rows\")\n",
    "\n",
    "# Strategy 2: Drop rows with ALL nulls\n",
    "df_drop_all = df.dropna(how=\"all\")\n",
    "print(f\"‚úÖ Drop ALL null: {df.count()} ‚Üí {df_drop_all.count()} rows\")\n",
    "\n",
    "# Strategy 3: Drop rows with nulls in specific columns\n",
    "df_drop_subset = df.dropna(subset=[\"customer_id\", \"email\"])\n",
    "print(f\"‚úÖ Drop null in [customer_id, email]: {df.count()} ‚Üí {df_drop_subset.count()} rows\")\n",
    "\n",
    "# Strategy 4: Drop rows with nulls in at least N columns\n",
    "df_drop_thresh = df.dropna(thresh=5)  # Keep rows with at least 5 non-null values\n",
    "print(f\"‚úÖ Drop rows with < 5 non-nulls: {df.count()} ‚Üí {df_drop_thresh.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3 Fill Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FILL WITH CONSTANTS:\n",
      "+-----------+---------------+--------------------+---+--------+-----------------+-------+\n",
      "|customer_id|name           |email               |age|salary  |registration_date|country|\n",
      "+-----------+---------------+--------------------+---+--------+-----------------+-------+\n",
      "|CUST001    |John Doe       |john@email.com      |25 |50000.0 |2024-01-01       |USA    |\n",
      "|CUST002    |Jane Smith     |JANE@EMAIL.COM      |30 |60000.0 |2024-01-02       |UK     |\n",
      "|CUST003    |  Bob Johnson  |bob@email.com       |0  |55000.0 |2024-01-03       |Canada |\n",
      "|CUST004    |Alice Brown    |no-email@example.com|28 |70000.0 |2024-01-04       |USA    |\n",
      "|CUST005    |Charlie Wilson |charlie@email.com   |35 |0.0     |2024-01-05       |UK     |\n",
      "|CUST001    |John Doe       |john@email.com      |25 |50000.0 |2024-01-01       |USA    |\n",
      "|CUST006    |Unknown        |david@email.com     |40 |80000.0 |2024-01-06       |Canada |\n",
      "|CUST007    |Eve Davis      |eve@email.com       |-5 |90000.0 |2024-01-07       |USA    |\n",
      "|CUST008    |Frank Miller   |frank@email.com     |150|100000.0|2024-01-08       |UK     |\n",
      "|CUST009    |Grace Lee      |grace@email.com     |32 |-10000.0|2024-01-09       |Canada |\n",
      "|CUST010    |Henry Taylor   |HENRY@EMAIL.COM     |29 |65000.0 |invalid-date     |USA    |\n",
      "|CUST011    |Ivy Anderson   |ivy@email.com       |27 |58000.0 |2024-01-11       |Unknown|\n",
      "|CUST012    |Jack Thomas    |jack@email.com      |33 |72000.0 |2024-01-12       |  UK   |\n",
      "|CUST013    |KAREN JACKSON  |karen@email.com     |31 |68000.0 |2024-01-13       |usa    |\n",
      "|CUST014    |Leo White      |leo@email.com       |26 |54000.0 |2024-01-14       |Canada |\n",
      "|CUST015    |Mia Harris     |mia@email.com       |0  |0.0     |NULL             |Unknown|\n",
      "+-----------+---------------+--------------------+---+--------+-----------------+-------+\n",
      "\n",
      "\n",
      "üìä Mean age: 36.14\n",
      "üìä Mean salary: 61571.43\n",
      "\n",
      "‚úÖ FILL WITH MEAN:\n",
      "+-----------+---------------+-----------------+---+-----------------+-----------------+-------+\n",
      "|customer_id|name           |email            |age|salary           |registration_date|country|\n",
      "+-----------+---------------+-----------------+---+-----------------+-----------------+-------+\n",
      "|CUST001    |John Doe       |john@email.com   |25 |50000.0          |2024-01-01       |USA    |\n",
      "|CUST002    |Jane Smith     |JANE@EMAIL.COM   |30 |60000.0          |2024-01-02       |UK     |\n",
      "|CUST003    |  Bob Johnson  |bob@email.com    |36 |55000.0          |2024-01-03       |Canada |\n",
      "|CUST004    |Alice Brown    |NULL             |28 |70000.0          |2024-01-04       |USA    |\n",
      "|CUST005    |Charlie Wilson |charlie@email.com|35 |61571.42857142857|2024-01-05       |UK     |\n",
      "|CUST001    |John Doe       |john@email.com   |25 |50000.0          |2024-01-01       |USA    |\n",
      "|CUST006    |NULL           |david@email.com  |40 |80000.0          |2024-01-06       |Canada |\n",
      "|CUST007    |Eve Davis      |eve@email.com    |-5 |90000.0          |2024-01-07       |USA    |\n",
      "|CUST008    |Frank Miller   |frank@email.com  |150|100000.0         |2024-01-08       |UK     |\n",
      "|CUST009    |Grace Lee      |grace@email.com  |32 |-10000.0         |2024-01-09       |Canada |\n",
      "|CUST010    |Henry Taylor   |HENRY@EMAIL.COM  |29 |65000.0          |invalid-date     |USA    |\n",
      "|CUST011    |Ivy Anderson   |ivy@email.com    |27 |58000.0          |2024-01-11       |NULL   |\n",
      "|CUST012    |Jack Thomas    |jack@email.com   |33 |72000.0          |2024-01-12       |  UK   |\n",
      "|CUST013    |KAREN JACKSON  |karen@email.com  |31 |68000.0          |2024-01-13       |usa    |\n",
      "|CUST014    |Leo White      |leo@email.com    |26 |54000.0          |2024-01-14       |Canada |\n",
      "|CUST015    |Mia Harris     |mia@email.com    |36 |61571.42857142857|NULL             |NULL   |\n",
      "+-----------+---------------+-----------------+---+-----------------+-----------------+-------+\n",
      "\n",
      "\n",
      "üìä Mode country: USA\n",
      "\n",
      "‚úÖ FILL WITH MODE:\n",
      "+-----------+---------------+-----------------+----+--------+-----------------+-------+\n",
      "|customer_id|name           |email            |age |salary  |registration_date|country|\n",
      "+-----------+---------------+-----------------+----+--------+-----------------+-------+\n",
      "|CUST001    |John Doe       |john@email.com   |25  |50000.0 |2024-01-01       |USA    |\n",
      "|CUST002    |Jane Smith     |JANE@EMAIL.COM   |30  |60000.0 |2024-01-02       |UK     |\n",
      "|CUST003    |  Bob Johnson  |bob@email.com    |NULL|55000.0 |2024-01-03       |Canada |\n",
      "|CUST004    |Alice Brown    |NULL             |28  |70000.0 |2024-01-04       |USA    |\n",
      "|CUST005    |Charlie Wilson |charlie@email.com|35  |NULL    |2024-01-05       |UK     |\n",
      "|CUST001    |John Doe       |john@email.com   |25  |50000.0 |2024-01-01       |USA    |\n",
      "|CUST006    |NULL           |david@email.com  |40  |80000.0 |2024-01-06       |Canada |\n",
      "|CUST007    |Eve Davis      |eve@email.com    |-5  |90000.0 |2024-01-07       |USA    |\n",
      "|CUST008    |Frank Miller   |frank@email.com  |150 |100000.0|2024-01-08       |UK     |\n",
      "|CUST009    |Grace Lee      |grace@email.com  |32  |-10000.0|2024-01-09       |Canada |\n",
      "|CUST010    |Henry Taylor   |HENRY@EMAIL.COM  |29  |65000.0 |invalid-date     |USA    |\n",
      "|CUST011    |Ivy Anderson   |ivy@email.com    |27  |58000.0 |2024-01-11       |USA    |\n",
      "|CUST012    |Jack Thomas    |jack@email.com   |33  |72000.0 |2024-01-12       |  UK   |\n",
      "|CUST013    |KAREN JACKSON  |karen@email.com  |31  |68000.0 |2024-01-13       |usa    |\n",
      "|CUST014    |Leo White      |leo@email.com    |26  |54000.0 |2024-01-14       |Canada |\n",
      "|CUST015    |Mia Harris     |mia@email.com    |NULL|NULL    |NULL             |USA    |\n",
      "+-----------+---------------+-----------------+----+--------+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Strategy 1: Fill with constant values\n",
    "df_fill_const = df.fillna({\n",
    "    \"name\": \"Unknown\",\n",
    "    \"email\": \"no-email@example.com\",\n",
    "    \"age\": 0,\n",
    "    \"salary\": 0.0,\n",
    "    \"country\": \"Unknown\"\n",
    "})\n",
    "\n",
    "print(\"‚úÖ FILL WITH CONSTANTS:\")\n",
    "df_fill_const.show(truncate=False)\n",
    "\n",
    "# Strategy 2: Fill with mean/median\n",
    "from pyspark.sql.functions import mean, median\n",
    "\n",
    "# Calculate mean age and salary\n",
    "stats = df.select(\n",
    "    mean(\"age\").alias(\"mean_age\"),\n",
    "    mean(\"salary\").alias(\"mean_salary\")\n",
    ").collect()[0]\n",
    "\n",
    "mean_age = stats[\"mean_age\"]\n",
    "mean_salary = stats[\"mean_salary\"]\n",
    "\n",
    "print(f\"\\nüìä Mean age: {mean_age:.2f}\")\n",
    "print(f\"üìä Mean salary: {mean_salary:.2f}\")\n",
    "\n",
    "df_fill_mean = df.fillna({\n",
    "    \"age\": int(mean_age),\n",
    "    \"salary\": mean_salary\n",
    "})\n",
    "\n",
    "print(\"\\n‚úÖ FILL WITH MEAN:\")\n",
    "df_fill_mean.show(truncate=False)\n",
    "\n",
    "# Strategy 3: Fill with mode (most frequent value)\n",
    "mode_country = df.groupBy(\"country\").count() \\\n",
    "    .orderBy(desc(\"count\")) \\\n",
    "    .first()[\"country\"]\n",
    "\n",
    "print(f\"\\nüìä Mode country: {mode_country}\")\n",
    "\n",
    "df_fill_mode = df.fillna({\"country\": mode_country})\n",
    "\n",
    "print(\"\\n‚úÖ FILL WITH MODE:\")\n",
    "df_fill_mode.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.4 Advanced: Fill with Forward/Backward Fill**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FORWARD FILL (age):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/04 17:50:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/04 17:50:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/04 17:50:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/04 17:50:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/04 17:50:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----------+\n",
      "|customer_id| age|age_filled|\n",
      "+-----------+----+----------+\n",
      "|    CUST001|  25|        25|\n",
      "|    CUST001|  25|        25|\n",
      "|    CUST002|  30|        30|\n",
      "|    CUST003|NULL|        30|\n",
      "|    CUST004|  28|        28|\n",
      "|    CUST005|  35|        35|\n",
      "|    CUST006|  40|        40|\n",
      "|    CUST007|  -5|        -5|\n",
      "|    CUST008| 150|       150|\n",
      "|    CUST009|  32|        32|\n",
      "|    CUST010|  29|        29|\n",
      "|    CUST011|  27|        27|\n",
      "|    CUST012|  33|        33|\n",
      "|    CUST013|  31|        31|\n",
      "|    CUST014|  26|        26|\n",
      "|    CUST015|NULL|        26|\n",
      "+-----------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Forward fill (fill with previous non-null value)\n",
    "windowSpec = Window.orderBy(\"customer_id\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "df_ffill = df.withColumn(\n",
    "    \"age_filled\",\n",
    "    last(\"age\", ignorenulls=True).over(windowSpec)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ FORWARD FILL (age):\")\n",
    "df_ffill.select(\"customer_id\", \"age\", \"age_filled\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ **4. REMOVE DUPLICATES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Remove exact duplicates: 16 ‚Üí 15 rows\n",
      "‚úÖ Remove duplicates by customer_id: 16 ‚Üí 15 rows\n",
      "\n",
      "‚úÖ Keep first occurrence: 16 ‚Üí 15 rows\n",
      "+-----------+---------------+-----------------+----+--------+-----------------+-------+\n",
      "|customer_id|name           |email            |age |salary  |registration_date|country|\n",
      "+-----------+---------------+-----------------+----+--------+-----------------+-------+\n",
      "|CUST001    |John Doe       |john@email.com   |25  |50000.0 |2024-01-01       |USA    |\n",
      "|CUST002    |Jane Smith     |JANE@EMAIL.COM   |30  |60000.0 |2024-01-02       |UK     |\n",
      "|CUST003    |  Bob Johnson  |bob@email.com    |NULL|55000.0 |2024-01-03       |Canada |\n",
      "|CUST004    |Alice Brown    |NULL             |28  |70000.0 |2024-01-04       |USA    |\n",
      "|CUST005    |Charlie Wilson |charlie@email.com|35  |NULL    |2024-01-05       |UK     |\n",
      "|CUST006    |NULL           |david@email.com  |40  |80000.0 |2024-01-06       |Canada |\n",
      "|CUST007    |Eve Davis      |eve@email.com    |-5  |90000.0 |2024-01-07       |USA    |\n",
      "|CUST008    |Frank Miller   |frank@email.com  |150 |100000.0|2024-01-08       |UK     |\n",
      "|CUST009    |Grace Lee      |grace@email.com  |32  |-10000.0|2024-01-09       |Canada |\n",
      "|CUST010    |Henry Taylor   |HENRY@EMAIL.COM  |29  |65000.0 |invalid-date     |USA    |\n",
      "|CUST011    |Ivy Anderson   |ivy@email.com    |27  |58000.0 |2024-01-11       |NULL   |\n",
      "|CUST012    |Jack Thomas    |jack@email.com   |33  |72000.0 |2024-01-12       |  UK   |\n",
      "|CUST013    |KAREN JACKSON  |karen@email.com  |31  |68000.0 |2024-01-13       |usa    |\n",
      "|CUST014    |Leo White      |leo@email.com    |26  |54000.0 |2024-01-14       |Canada |\n",
      "|CUST015    |Mia Harris     |mia@email.com    |NULL|NULL    |NULL             |NULL   |\n",
      "+-----------+---------------+-----------------+----+--------+-----------------+-------+\n",
      "\n",
      "\n",
      "üîç IDENTIFY DUPLICATES:\n",
      "+-----------+--------+--------------+---+-------+-----------------+-------+------------+\n",
      "|customer_id|name    |email         |age|salary |registration_date|country|is_duplicate|\n",
      "+-----------+--------+--------------+---+-------+-----------------+-------+------------+\n",
      "|CUST001    |John Doe|john@email.com|25 |50000.0|2024-01-01       |USA    |true        |\n",
      "|CUST001    |John Doe|john@email.com|25 |50000.0|2024-01-01       |USA    |true        |\n",
      "+-----------+--------+--------------+---+-------+-----------------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Remove exact duplicates (all columns)\n",
    "df_dedup_all = df.dropDuplicates()\n",
    "print(f\"‚úÖ Remove exact duplicates: {df.count()} ‚Üí {df_dedup_all.count()} rows\")\n",
    "\n",
    "# 4.2 Remove duplicates based on specific columns\n",
    "df_dedup_id = df.dropDuplicates([\"customer_id\"])\n",
    "print(f\"‚úÖ Remove duplicates by customer_id: {df.count()} ‚Üí {df_dedup_id.count()} rows\")\n",
    "\n",
    "# 4.3 Keep first/last occurrence\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Keep first occurrence (earliest registration_date)\n",
    "windowSpec = Window.partitionBy(\"customer_id\").orderBy(\"registration_date\")\n",
    "\n",
    "df_keep_first = df.withColumn(\"row_num\", row_number().over(windowSpec)) \\\n",
    "    .filter(col(\"row_num\") == 1) \\\n",
    "    .drop(\"row_num\")\n",
    "\n",
    "print(f\"\\n‚úÖ Keep first occurrence: {df.count()} ‚Üí {df_keep_first.count()} rows\")\n",
    "df_keep_first.show(truncate=False)\n",
    "\n",
    "# 4.4 Identify duplicates\n",
    "print(\"\\nüîç IDENTIFY DUPLICATES:\")\n",
    "df_with_dup_flag = df.withColumn(\n",
    "    \"is_duplicate\",\n",
    "    count(\"*\").over(Window.partitionBy(\"customer_id\")) > 1\n",
    ")\n",
    "\n",
    "df_with_dup_flag.filter(col(\"is_duplicate\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî§ **5. STRING CLEANING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TRIM WHITESPACE:\n",
      "+--------------+-------+\n",
      "|name          |country|\n",
      "+--------------+-------+\n",
      "|John Doe      |USA    |\n",
      "|Jane Smith    |UK     |\n",
      "|Bob Johnson   |Canada |\n",
      "|Alice Brown   |USA    |\n",
      "|Charlie Wilson|UK     |\n",
      "|John Doe      |USA    |\n",
      "|NULL          |Canada |\n",
      "|Eve Davis     |USA    |\n",
      "|Frank Miller  |UK     |\n",
      "|Grace Lee     |Canada |\n",
      "|Henry Taylor  |USA    |\n",
      "|Ivy Anderson  |NULL   |\n",
      "|Jack Thomas   |UK     |\n",
      "|KAREN JACKSON |usa    |\n",
      "|Leo White     |Canada |\n",
      "|Mia Harris    |NULL   |\n",
      "+--------------+-------+\n",
      "\n",
      "\n",
      "‚úÖ CASE CONVERSION:\n",
      "+-----------------+-------+\n",
      "|email            |country|\n",
      "+-----------------+-------+\n",
      "|john@email.com   |USA    |\n",
      "|jane@email.com   |UK     |\n",
      "|bob@email.com    |CANADA |\n",
      "|NULL             |USA    |\n",
      "|charlie@email.com|UK     |\n",
      "|john@email.com   |USA    |\n",
      "|david@email.com  |CANADA |\n",
      "|eve@email.com    |USA    |\n",
      "|frank@email.com  |UK     |\n",
      "|grace@email.com  |CANADA |\n",
      "|henry@email.com  |USA    |\n",
      "|ivy@email.com    |NULL   |\n",
      "|jack@email.com   |UK     |\n",
      "|karen@email.com  |USA    |\n",
      "|leo@email.com    |CANADA |\n",
      "|mia@email.com    |NULL   |\n",
      "+-----------------+-------+\n",
      "\n",
      "\n",
      "‚úÖ TITLE CASE:\n",
      "+--------------+\n",
      "|name          |\n",
      "+--------------+\n",
      "|John Doe      |\n",
      "|Jane Smith    |\n",
      "|Bob Johnson   |\n",
      "|Alice Brown   |\n",
      "|Charlie Wilson|\n",
      "|John Doe      |\n",
      "|NULL          |\n",
      "|Eve Davis     |\n",
      "|Frank Miller  |\n",
      "|Grace Lee     |\n",
      "|Henry Taylor  |\n",
      "|Ivy Anderson  |\n",
      "|Jack Thomas   |\n",
      "|Karen Jackson |\n",
      "|Leo White     |\n",
      "|Mia Harris    |\n",
      "+--------------+\n",
      "\n",
      "\n",
      "‚úÖ REMOVE SPECIAL CHARACTERS:\n",
      "+--------------+--------------+\n",
      "|name          |name_clean    |\n",
      "+--------------+--------------+\n",
      "|John Doe      |John Doe      |\n",
      "|Jane Smith    |Jane Smith    |\n",
      "|Bob Johnson   |Bob Johnson   |\n",
      "|Alice Brown   |Alice Brown   |\n",
      "|Charlie Wilson|Charlie Wilson|\n",
      "|John Doe      |John Doe      |\n",
      "|NULL          |NULL          |\n",
      "|Eve Davis     |Eve Davis     |\n",
      "|Frank Miller  |Frank Miller  |\n",
      "|Grace Lee     |Grace Lee     |\n",
      "|Henry Taylor  |Henry Taylor  |\n",
      "|Ivy Anderson  |Ivy Anderson  |\n",
      "|Jack Thomas   |Jack Thomas   |\n",
      "|Karen Jackson |Karen Jackson |\n",
      "|Leo White     |Leo White     |\n",
      "|Mia Harris    |Mia Harris    |\n",
      "+--------------+--------------+\n",
      "\n",
      "\n",
      "‚úÖ EXTRACT FIRST/LAST NAME:\n",
      "+--------------+----------+---------+\n",
      "|name          |first_name|last_name|\n",
      "+--------------+----------+---------+\n",
      "|John Doe      |John      |Doe      |\n",
      "|Jane Smith    |Jane      |Smith    |\n",
      "|Bob Johnson   |Bob       |Johnson  |\n",
      "|Alice Brown   |Alice     |Brown    |\n",
      "|Charlie Wilson|Charlie   |Wilson   |\n",
      "|John Doe      |John      |Doe      |\n",
      "|NULL          |NULL      |NULL     |\n",
      "|Eve Davis     |Eve       |Davis    |\n",
      "|Frank Miller  |Frank     |Miller   |\n",
      "|Grace Lee     |Grace     |Lee      |\n",
      "|Henry Taylor  |Henry     |Taylor   |\n",
      "|Ivy Anderson  |Ivy       |Anderson |\n",
      "|Jack Thomas   |Jack      |Thomas   |\n",
      "|Karen Jackson |Karen     |Jackson  |\n",
      "|Leo White     |Leo       |White    |\n",
      "|Mia Harris    |Mia       |Harris   |\n",
      "+--------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5.1 Trim whitespace\n",
    "df_trim = df.withColumn(\"name\", trim(col(\"name\"))) \\\n",
    "    .withColumn(\"country\", trim(col(\"country\")))\n",
    "\n",
    "print(\"‚úÖ TRIM WHITESPACE:\")\n",
    "df_trim.select(\"name\", \"country\").show(truncate=False)\n",
    "\n",
    "# 5.2 Convert to lowercase/uppercase\n",
    "df_case = df_trim.withColumn(\"email\", lower(col(\"email\"))) \\\n",
    "    .withColumn(\"country\", upper(col(\"country\")))\n",
    "\n",
    "print(\"\\n‚úÖ CASE CONVERSION:\")\n",
    "df_case.select(\"email\", \"country\").show(truncate=False)\n",
    "\n",
    "# 5.3 Title case for names\n",
    "df_title = df_case.withColumn(\"name\", initcap(col(\"name\")))\n",
    "\n",
    "print(\"\\n‚úÖ TITLE CASE:\")\n",
    "df_title.select(\"name\").show(truncate=False)\n",
    "\n",
    "# 5.4 Remove special characters\n",
    "df_clean = df_title.withColumn(\n",
    "    \"name_clean\",\n",
    "    regexp_replace(col(\"name\"), \"[^a-zA-Z\\\\s]\", \"\")\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ REMOVE SPECIAL CHARACTERS:\")\n",
    "df_clean.select(\"name\", \"name_clean\").show(truncate=False)\n",
    "\n",
    "# 5.5 Extract parts of string\n",
    "df_extract = df_clean.withColumn(\n",
    "    \"first_name\",\n",
    "    split(col(\"name\"), \" \").getItem(0)\n",
    ").withColumn(\n",
    "    \"last_name\",\n",
    "    split(col(\"name\"), \" \").getItem(1)\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ EXTRACT FIRST/LAST NAME:\")\n",
    "df_extract.select(\"name\", \"first_name\", \"last_name\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî¢ **6. DATA TYPE CONVERSIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ STRING TO DATE:\n",
      "+-----------------+------------------------+\n",
      "|registration_date|registration_date_parsed|\n",
      "+-----------------+------------------------+\n",
      "|       2024-01-01|              2024-01-01|\n",
      "|       2024-01-02|              2024-01-02|\n",
      "|       2024-01-03|              2024-01-03|\n",
      "|       2024-01-04|              2024-01-04|\n",
      "|       2024-01-05|              2024-01-05|\n",
      "|       2024-01-01|              2024-01-01|\n",
      "|       2024-01-06|              2024-01-06|\n",
      "|       2024-01-07|              2024-01-07|\n",
      "|       2024-01-08|              2024-01-08|\n",
      "|       2024-01-09|              2024-01-09|\n",
      "|     invalid-date|                    NULL|\n",
      "|       2024-01-11|              2024-01-11|\n",
      "|       2024-01-12|              2024-01-12|\n",
      "|       2024-01-13|              2024-01-13|\n",
      "|       2024-01-14|              2024-01-14|\n",
      "|             NULL|                    NULL|\n",
      "+-----------------+------------------------+\n",
      "\n",
      "\n",
      "‚úÖ SAFE DATE CONVERSION:\n",
      "+-----------------+----------------------+\n",
      "|registration_date|registration_date_safe|\n",
      "+-----------------+----------------------+\n",
      "|       2024-01-01|            2024-01-01|\n",
      "|       2024-01-02|            2024-01-02|\n",
      "|       2024-01-03|            2024-01-03|\n",
      "|       2024-01-04|            2024-01-04|\n",
      "|       2024-01-05|            2024-01-05|\n",
      "|       2024-01-01|            2024-01-01|\n",
      "|       2024-01-06|            2024-01-06|\n",
      "|       2024-01-07|            2024-01-07|\n",
      "|       2024-01-08|            2024-01-08|\n",
      "|       2024-01-09|            2024-01-09|\n",
      "|     invalid-date|                  NULL|\n",
      "|       2024-01-11|            2024-01-11|\n",
      "|       2024-01-12|            2024-01-12|\n",
      "|       2024-01-13|            2024-01-13|\n",
      "|       2024-01-14|            2024-01-14|\n",
      "|             NULL|                  NULL|\n",
      "+-----------------+----------------------+\n",
      "\n",
      "\n",
      "‚úÖ DATE TO TIMESTAMP:\n",
      "+----------------------+----------------------+\n",
      "|registration_date_safe|registration_timestamp|\n",
      "+----------------------+----------------------+\n",
      "|            2024-01-01|   2024-01-01 00:00:00|\n",
      "|            2024-01-02|   2024-01-02 00:00:00|\n",
      "|            2024-01-03|   2024-01-03 00:00:00|\n",
      "|            2024-01-04|   2024-01-04 00:00:00|\n",
      "|            2024-01-05|   2024-01-05 00:00:00|\n",
      "|            2024-01-01|   2024-01-01 00:00:00|\n",
      "|            2024-01-06|   2024-01-06 00:00:00|\n",
      "|            2024-01-07|   2024-01-07 00:00:00|\n",
      "|            2024-01-08|   2024-01-08 00:00:00|\n",
      "|            2024-01-09|   2024-01-09 00:00:00|\n",
      "|                  NULL|                  NULL|\n",
      "|            2024-01-11|   2024-01-11 00:00:00|\n",
      "|            2024-01-12|   2024-01-12 00:00:00|\n",
      "|            2024-01-13|   2024-01-13 00:00:00|\n",
      "|            2024-01-14|   2024-01-14 00:00:00|\n",
      "|                  NULL|                  NULL|\n",
      "+----------------------+----------------------+\n",
      "\n",
      "\n",
      "‚úÖ NUMERIC CASTING:\n",
      "+----+----------+--------+----------+\n",
      "| age|age_double|  salary|salary_int|\n",
      "+----+----------+--------+----------+\n",
      "|  25|      25.0| 50000.0|     50000|\n",
      "|  30|      30.0| 60000.0|     60000|\n",
      "|NULL|      NULL| 55000.0|     55000|\n",
      "|  28|      28.0| 70000.0|     70000|\n",
      "|  35|      35.0|    NULL|      NULL|\n",
      "|  25|      25.0| 50000.0|     50000|\n",
      "|  40|      40.0| 80000.0|     80000|\n",
      "|  -5|      -5.0| 90000.0|     90000|\n",
      "| 150|     150.0|100000.0|    100000|\n",
      "|  32|      32.0|-10000.0|    -10000|\n",
      "|  29|      29.0| 65000.0|     65000|\n",
      "|  27|      27.0| 58000.0|     58000|\n",
      "|  33|      33.0| 72000.0|     72000|\n",
      "|  31|      31.0| 68000.0|     68000|\n",
      "|  26|      26.0| 54000.0|     54000|\n",
      "|NULL|      NULL|    NULL|      NULL|\n",
      "+----+----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6.1 Convert string to date\n",
    "df_date = df.withColumn(\n",
    "    \"registration_date_parsed\",\n",
    "    to_date(col(\"registration_date\"), \"yyyy-MM-dd\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ STRING TO DATE:\")\n",
    "df_date.select(\"registration_date\", \"registration_date_parsed\").show()\n",
    "\n",
    "# 6.2 Handle invalid dates\n",
    "df_date_safe = df.withColumn(\n",
    "    \"registration_date_safe\",\n",
    "    when(\n",
    "        to_date(col(\"registration_date\"), \"yyyy-MM-dd\").isNotNull(),\n",
    "        to_date(col(\"registration_date\"), \"yyyy-MM-dd\")\n",
    "    ).otherwise(lit(None).cast(\"date\"))\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ SAFE DATE CONVERSION:\")\n",
    "df_date_safe.select(\"registration_date\", \"registration_date_safe\").show()\n",
    "\n",
    "# 6.3 Convert to timestamp\n",
    "df_timestamp = df_date_safe.withColumn(\n",
    "    \"registration_timestamp\",\n",
    "    to_timestamp(col(\"registration_date_safe\"))\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ DATE TO TIMESTAMP:\")\n",
    "df_timestamp.select(\"registration_date_safe\", \"registration_timestamp\").show()\n",
    "\n",
    "# 6.4 Cast numeric types\n",
    "df_cast = df.withColumn(\"age_double\", col(\"age\").cast(\"double\")) \\\n",
    "    .withColumn(\"salary_int\", col(\"salary\").cast(\"int\"))\n",
    "\n",
    "print(\"\\n‚úÖ NUMERIC CASTING:\")\n",
    "df_cast.select(\"age\", \"age_double\", \"salary\", \"salary_int\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üö® **7. HANDLE OUTLIERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä AGE STATISTICS:\n",
      "Q1: 26\n",
      "Q3: 33\n",
      "IQR: 7\n",
      "Lower bound: 15.5\n",
      "Upper bound: 43.5\n",
      "\n",
      "üö® OUTLIERS DETECTED:\n",
      "+-----------+------------+---------------+---+--------+-----------------+-------+--------------+\n",
      "|customer_id|name        |email          |age|salary  |registration_date|country|is_age_outlier|\n",
      "+-----------+------------+---------------+---+--------+-----------------+-------+--------------+\n",
      "|CUST007    |Eve Davis   |eve@email.com  |-5 |90000.0 |2024-01-07       |USA    |true          |\n",
      "|CUST008    |Frank Miller|frank@email.com|150|100000.0|2024-01-08       |UK     |true          |\n",
      "+-----------+------------+---------------+---+--------+-----------------+-------+--------------+\n",
      "\n",
      "\n",
      "‚úÖ Remove outliers: 16 ‚Üí 12 rows\n",
      "\n",
      "‚úÖ CAP OUTLIERS:\n",
      "+-----------+----+----------+\n",
      "|customer_id| age|age_capped|\n",
      "+-----------+----+----------+\n",
      "|    CUST001|  25|      25.0|\n",
      "|    CUST002|  30|      30.0|\n",
      "|    CUST003|NULL|      NULL|\n",
      "|    CUST004|  28|      28.0|\n",
      "|    CUST005|  35|      35.0|\n",
      "|    CUST001|  25|      25.0|\n",
      "|    CUST006|  40|      40.0|\n",
      "|    CUST007|  -5|      15.5|\n",
      "|    CUST008| 150|      43.5|\n",
      "|    CUST009|  32|      32.0|\n",
      "|    CUST010|  29|      29.0|\n",
      "|    CUST011|  27|      27.0|\n",
      "|    CUST012|  33|      33.0|\n",
      "|    CUST013|  31|      31.0|\n",
      "|    CUST014|  26|      26.0|\n",
      "|    CUST015|NULL|      NULL|\n",
      "+-----------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7.1 Identify outliers using IQR method\n",
    "from pyspark.sql.functions import percentile_approx\n",
    "\n",
    "# Calculate Q1, Q3, IQR for age\n",
    "quantiles = df.select(\n",
    "    percentile_approx(\"age\", 0.25).alias(\"Q1\"),\n",
    "    percentile_approx(\"age\", 0.75).alias(\"Q3\")\n",
    ").collect()[0]\n",
    "\n",
    "Q1 = quantiles[\"Q1\"]\n",
    "Q3 = quantiles[\"Q3\"]\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"üìä AGE STATISTICS:\")\n",
    "print(f\"Q1: {Q1}\")\n",
    "print(f\"Q3: {Q3}\")\n",
    "print(f\"IQR: {IQR}\")\n",
    "print(f\"Lower bound: {lower_bound}\")\n",
    "print(f\"Upper bound: {upper_bound}\")\n",
    "\n",
    "# Flag outliers\n",
    "df_outliers = df.withColumn(\n",
    "    \"is_age_outlier\",\n",
    "    (col(\"age\") < lower_bound) | (col(\"age\") > upper_bound)\n",
    ")\n",
    "\n",
    "print(\"\\nüö® OUTLIERS DETECTED:\")\n",
    "df_outliers.filter(col(\"is_age_outlier\")).show(truncate=False)\n",
    "\n",
    "# 7.2 Remove outliers\n",
    "df_no_outliers = df.filter(\n",
    "    (col(\"age\") >= lower_bound) & (col(\"age\") <= upper_bound)\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Remove outliers: {df.count()} ‚Üí {df_no_outliers.count()} rows\")\n",
    "\n",
    "# 7.3 Cap outliers (winsorization)\n",
    "df_capped = df.withColumn(\n",
    "    \"age_capped\",\n",
    "    when(col(\"age\") < lower_bound, lower_bound)\n",
    "    .when(col(\"age\") > upper_bound, upper_bound)\n",
    "    .otherwise(col(\"age\"))\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ CAP OUTLIERS:\")\n",
    "df_capped.select(\"customer_id\", \"age\", \"age_capped\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ **8. COMPLETE CLEANING PIPELINE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ BEFORE CLEANING:\n",
      "Rows: 16\n",
      "+-----------+---------------+-----------------+----+--------+-----------------+-------+\n",
      "|customer_id|name           |email            |age |salary  |registration_date|country|\n",
      "+-----------+---------------+-----------------+----+--------+-----------------+-------+\n",
      "|CUST001    |John Doe       |john@email.com   |25  |50000.0 |2024-01-01       |USA    |\n",
      "|CUST002    |Jane Smith     |JANE@EMAIL.COM   |30  |60000.0 |2024-01-02       |UK     |\n",
      "|CUST003    |  Bob Johnson  |bob@email.com    |NULL|55000.0 |2024-01-03       |Canada |\n",
      "|CUST004    |Alice Brown    |NULL             |28  |70000.0 |2024-01-04       |USA    |\n",
      "|CUST005    |Charlie Wilson |charlie@email.com|35  |NULL    |2024-01-05       |UK     |\n",
      "|CUST001    |John Doe       |john@email.com   |25  |50000.0 |2024-01-01       |USA    |\n",
      "|CUST006    |NULL           |david@email.com  |40  |80000.0 |2024-01-06       |Canada |\n",
      "|CUST007    |Eve Davis      |eve@email.com    |-5  |90000.0 |2024-01-07       |USA    |\n",
      "|CUST008    |Frank Miller   |frank@email.com  |150 |100000.0|2024-01-08       |UK     |\n",
      "|CUST009    |Grace Lee      |grace@email.com  |32  |-10000.0|2024-01-09       |Canada |\n",
      "|CUST010    |Henry Taylor   |HENRY@EMAIL.COM  |29  |65000.0 |invalid-date     |USA    |\n",
      "|CUST011    |Ivy Anderson   |ivy@email.com    |27  |58000.0 |2024-01-11       |NULL   |\n",
      "|CUST012    |Jack Thomas    |jack@email.com   |33  |72000.0 |2024-01-12       |  UK   |\n",
      "|CUST013    |KAREN JACKSON  |karen@email.com  |31  |68000.0 |2024-01-13       |usa    |\n",
      "|CUST014    |Leo White      |leo@email.com    |26  |54000.0 |2024-01-14       |Canada |\n",
      "|CUST015    |Mia Harris     |mia@email.com    |NULL|NULL    |NULL             |NULL   |\n",
      "+-----------+---------------+-----------------+----+--------+-----------------+-------+\n",
      "\n",
      "\n",
      "‚úÖ AFTER CLEANING:\n",
      "Rows: 15\n",
      "+-----------+--------------+--------------------+---+-----------------+-----------------+-------+\n",
      "|customer_id|name          |email               |age|salary           |registration_date|country|\n",
      "+-----------+--------------+--------------------+---+-----------------+-----------------+-------+\n",
      "|CUST001    |John Doe      |john@email.com      |25 |50000.0          |2024-01-01       |USA    |\n",
      "|CUST002    |Jane Smith    |jane@email.com      |30 |60000.0          |2024-01-02       |UK     |\n",
      "|CUST003    |Bob Johnson   |bob@email.com       |37 |55000.0          |2024-01-03       |CANADA |\n",
      "|CUST004    |Alice Brown   |no-email@example.com|28 |70000.0          |2024-01-04       |USA    |\n",
      "|CUST005    |Charlie Wilson|charlie@email.com   |35 |62461.53846153846|2024-01-05       |UK     |\n",
      "|CUST006    |Unknown       |david@email.com     |40 |80000.0          |2024-01-06       |CANADA |\n",
      "|CUST007    |Eve Davis     |eve@email.com       |0  |90000.0          |2024-01-07       |USA    |\n",
      "|CUST008    |Frank Miller  |frank@email.com     |120|100000.0         |2024-01-08       |UK     |\n",
      "|CUST009    |Grace Lee     |grace@email.com     |32 |0.0              |2024-01-09       |CANADA |\n",
      "|CUST010    |Henry Taylor  |henry@email.com     |29 |65000.0          |2026-01-04       |USA    |\n",
      "|CUST011    |Ivy Anderson  |ivy@email.com       |27 |58000.0          |2024-01-11       |USA    |\n",
      "|CUST012    |Jack Thomas   |jack@email.com      |33 |72000.0          |2024-01-12       |UK     |\n",
      "|CUST013    |Karen Jackson |karen@email.com     |31 |68000.0          |2024-01-13       |USA    |\n",
      "|CUST014    |Leo White     |leo@email.com       |26 |54000.0          |2024-01-14       |CANADA |\n",
      "|CUST015    |Mia Harris    |mia@email.com       |37 |62461.53846153846|2026-01-04       |USA    |\n",
      "+-----------+--------------+--------------------+---+-----------------+-----------------+-------+\n",
      "\n",
      "\n",
      "‚úÖ NULL CHECK AFTER CLEANING:\n",
      "+-----------+----+-----+---+------+-----------------+-------+\n",
      "|customer_id|name|email|age|salary|registration_date|country|\n",
      "+-----------+----+-----+---+------+-----------------+-------+\n",
      "|          0|   0|    0|  0|     0|                0|      0|\n",
      "+-----------+----+-----+---+------+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Complete cleaning pipeline\n",
    "def clean_customer_data(df):\n",
    "    \"\"\"\n",
    "    Complete data cleaning pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Remove exact duplicates\n",
    "    df = df.dropDuplicates([\"customer_id\"])\n",
    "    \n",
    "    # 2. String cleaning\n",
    "    df = df.withColumn(\"name\", trim(col(\"name\"))) \\\n",
    "        .withColumn(\"name\", initcap(col(\"name\"))) \\\n",
    "        .withColumn(\"email\", lower(trim(col(\"email\")))) \\\n",
    "        .withColumn(\"country\", upper(trim(col(\"country\"))))\n",
    "    \n",
    "    # 3. Handle missing values\n",
    "    # Calculate mean for numeric columns\n",
    "    stats = df.select(\n",
    "        mean(\"age\").alias(\"mean_age\"),\n",
    "        mean(\"salary\").alias(\"mean_salary\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    # Get mode for country\n",
    "    mode_country = df.groupBy(\"country\").count() \\\n",
    "        .orderBy(desc(\"count\")) \\\n",
    "        .first()[\"country\"]\n",
    "    \n",
    "    df = df.fillna({\n",
    "        \"name\": \"Unknown\",\n",
    "        \"email\": \"no-email@example.com\",\n",
    "        \"age\": int(stats[\"mean_age\"]),\n",
    "        \"salary\": stats[\"mean_salary\"],\n",
    "        \"country\": mode_country\n",
    "    })\n",
    "    \n",
    "    # 4. Data type conversions\n",
    "    df = df.withColumn(\n",
    "        \"registration_date\",\n",
    "        when(\n",
    "            to_date(col(\"registration_date\"), \"yyyy-MM-dd\").isNotNull(),\n",
    "            to_date(col(\"registration_date\"), \"yyyy-MM-dd\")\n",
    "        ).otherwise(current_date())\n",
    "    )\n",
    "    \n",
    "    # 5. Handle outliers (cap age)\n",
    "    df = df.withColumn(\n",
    "        \"age\",\n",
    "        when(col(\"age\") < 0, 0)\n",
    "        .when(col(\"age\") > 120, 120)\n",
    "        .otherwise(col(\"age\"))\n",
    "    )\n",
    "    \n",
    "    # 6. Handle negative salary\n",
    "    df = df.withColumn(\n",
    "        \"salary\",\n",
    "        when(col(\"salary\") < 0, 0)\n",
    "        .otherwise(col(\"salary\"))\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply cleaning pipeline\n",
    "print(\"üßπ BEFORE CLEANING:\")\n",
    "print(f\"Rows: {df.count()}\")\n",
    "df.show(truncate=False)\n",
    "\n",
    "df_clean = clean_customer_data(df)\n",
    "\n",
    "print(\"\\n‚úÖ AFTER CLEANING:\")\n",
    "print(f\"Rows: {df_clean.count()}\")\n",
    "df_clean.show(truncate=False)\n",
    "\n",
    "# Verify no nulls\n",
    "print(\"\\n‚úÖ NULL CHECK AFTER CLEANING:\")\n",
    "null_counts_after = df_clean.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in df_clean.columns\n",
    "])\n",
    "null_counts_after.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üíæ **9. SAVE CLEANED DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/04 17:51:10 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned data saved to: s3a://warehouse/cleaned_customers/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Verification: 15 rows loaded\n",
      "+-----------+--------------+--------------------+---+-----------------+-----------------+-------+\n",
      "|customer_id|          name|               email|age|           salary|registration_date|country|\n",
      "+-----------+--------------+--------------------+---+-----------------+-----------------+-------+\n",
      "|    CUST001|      John Doe|      john@email.com| 25|          50000.0|       2024-01-01|    USA|\n",
      "|    CUST004|   Alice Brown|no-email@example.com| 28|          70000.0|       2024-01-04|    USA|\n",
      "|    CUST007|     Eve Davis|       eve@email.com|  0|          90000.0|       2024-01-07|    USA|\n",
      "|    CUST010|  Henry Taylor|     henry@email.com| 29|          65000.0|       2026-01-04|    USA|\n",
      "|    CUST011|  Ivy Anderson|       ivy@email.com| 27|          58000.0|       2024-01-11|    USA|\n",
      "|    CUST013| Karen Jackson|     karen@email.com| 31|          68000.0|       2024-01-13|    USA|\n",
      "|    CUST015|    Mia Harris|       mia@email.com| 37|62461.53846153846|       2026-01-04|    USA|\n",
      "|    CUST002|    Jane Smith|      jane@email.com| 30|          60000.0|       2024-01-02|     UK|\n",
      "|    CUST005|Charlie Wilson|   charlie@email.com| 35|62461.53846153846|       2024-01-05|     UK|\n",
      "|    CUST008|  Frank Miller|     frank@email.com|120|         100000.0|       2024-01-08|     UK|\n",
      "|    CUST012|   Jack Thomas|      jack@email.com| 33|          72000.0|       2024-01-12|     UK|\n",
      "|    CUST003|   Bob Johnson|       bob@email.com| 37|          55000.0|       2024-01-03| CANADA|\n",
      "|    CUST006|       Unknown|     david@email.com| 40|          80000.0|       2024-01-06| CANADA|\n",
      "|    CUST009|     Grace Lee|     grace@email.com| 32|              0.0|       2024-01-09| CANADA|\n",
      "|    CUST014|     Leo White|       leo@email.com| 26|          54000.0|       2024-01-14| CANADA|\n",
      "+-----------+--------------+--------------------+---+-----------------+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save to MinIO\n",
    "output_path = \"s3a://warehouse/cleaned_customers/\"\n",
    "\n",
    "df_clean.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"country\") \\\n",
    "    .parquet(output_path)\n",
    "\n",
    "print(f\"‚úÖ Cleaned data saved to: {output_path}\")\n",
    "\n",
    "# Verify\n",
    "df_verify = spark.read.parquet(output_path)\n",
    "print(f\"\\n‚úÖ Verification: {df_verify.count()} rows loaded\")\n",
    "df_verify.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä **10. BEFORE/AFTER COMPARISON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DATA CLEANING REPORT\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£ ROW COUNT:\n",
      "   Before: 16\n",
      "   After:  15\n",
      "   Removed: 1\n",
      "\n",
      "2Ô∏è‚É£ NULL VALUES:\n",
      "   name: 1 ‚Üí 0\n",
      "   email: 1 ‚Üí 0\n",
      "   age: 2 ‚Üí 0\n",
      "   salary: 2 ‚Üí 0\n",
      "   registration_date: 1 ‚Üí 0\n",
      "   country: 2 ‚Üí 0\n",
      "\n",
      "3Ô∏è‚É£ DUPLICATES:\n",
      "   Before: 1\n",
      "   After:  0\n",
      "\n",
      "4Ô∏è‚É£ OUTLIERS (age < 0 or > 120):\n",
      "   Before: 2\n",
      "   After:  0\n",
      "\n",
      "============================================================\n",
      "‚úÖ CLEANING COMPLETED!\n"
     ]
    }
   ],
   "source": [
    "# Create comparison report\n",
    "print(\"üìä DATA CLEANING REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Row count\n",
    "print(f\"\\n1Ô∏è‚É£ ROW COUNT:\")\n",
    "print(f\"   Before: {df.count()}\")\n",
    "print(f\"   After:  {df_clean.count()}\")\n",
    "print(f\"   Removed: {df.count() - df_clean.count()}\")\n",
    "\n",
    "# Null count\n",
    "print(f\"\\n2Ô∏è‚É£ NULL VALUES:\")\n",
    "null_before = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).collect()[0]\n",
    "null_after = df_clean.select([count(when(col(c).isNull(), c)).alias(c) for c in df_clean.columns]).collect()[0]\n",
    "\n",
    "for col_name in df.columns:\n",
    "    before = null_before[col_name]\n",
    "    after = null_after[col_name]\n",
    "    if before > 0 or after > 0:\n",
    "        print(f\"   {col_name}: {before} ‚Üí {after}\")\n",
    "\n",
    "# Duplicates\n",
    "print(f\"\\n3Ô∏è‚É£ DUPLICATES:\")\n",
    "dup_before = df.count() - df.dropDuplicates([\"customer_id\"]).count()\n",
    "dup_after = df_clean.count() - df_clean.dropDuplicates([\"customer_id\"]).count()\n",
    "print(f\"   Before: {dup_before}\")\n",
    "print(f\"   After:  {dup_after}\")\n",
    "\n",
    "# Outliers\n",
    "print(f\"\\n4Ô∏è‚É£ OUTLIERS (age < 0 or > 120):\")\n",
    "outliers_before = df.filter((col(\"age\") < 0) | (col(\"age\") > 120)).count()\n",
    "outliers_after = df_clean.filter((col(\"age\") < 0) | (col(\"age\") > 120)).count()\n",
    "print(f\"   Before: {outliers_before}\")\n",
    "print(f\"   After:  {outliers_after}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ CLEANING COMPLETED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì **KEY TAKEAWAYS**\n",
    "\n",
    "### **‚úÖ Data Cleaning Best Practices:**\n",
    "\n",
    "1. **Always profile data first** - Understand the problems before fixing\n",
    "2. **Handle nulls strategically** - Drop, fill, or flag based on context\n",
    "3. **Remove duplicates early** - Prevents skewed analysis\n",
    "4. **Standardize strings** - Trim, case conversion, remove special chars\n",
    "5. **Validate data types** - Convert and handle invalid values\n",
    "6. **Handle outliers carefully** - Remove, cap, or flag based on domain\n",
    "7. **Create reusable pipelines** - Encapsulate cleaning logic in functions\n",
    "8. **Document transformations** - Track what was changed and why\n",
    "9. **Validate results** - Compare before/after metrics\n",
    "10. **Save cleaned data** - Separate raw and cleaned layers\n",
    "\n",
    "### **üöÄ Next Steps:**\n",
    "- **Day 2 - Notebook 4:** Data Quality & Validation\n",
    "- Learn data profiling, validation rules, and quality metrics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark session stopped\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "spark.stop()\n",
    "print(\"‚úÖ Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
