{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ SPARK SQL BASICS\n",
    "\n",
    "---\n",
    "\n",
    "## üìã **DAY 5 - LESSON 1: SQL BASICS**\n",
    "\n",
    "### **üéØ M·ª§C TI√äU:**\n",
    "\n",
    "1. **Spark SQL Overview** - T·∫°i sao d√πng SQL?\n",
    "2. **Temporary Views** - createOrReplaceTempView\n",
    "3. **Basic Queries** - SELECT, WHERE, GROUP BY\n",
    "4. **SQL Functions** - Built-in functions\n",
    "5. **Subqueries & CTEs** - WITH clause\n",
    "6. **SQL vs DataFrame API** - So s√°nh performance\n",
    "\n",
    "---\n",
    "\n",
    "## üí° **T·∫†I SAO D√ôNG SPARK SQL?**\n",
    "\n",
    "### **DataFrame API:**\n",
    "```python\n",
    "df.filter(col(\"age\") > 25) \\\n",
    "  .groupBy(\"country\") \\\n",
    "  .agg(F.avg(\"salary\").alias(\"avg_salary\")) \\\n",
    "  .orderBy(desc(\"avg_salary\"))\n",
    "```\n",
    "\n",
    "### **Spark SQL:**\n",
    "```sql\n",
    "SELECT country, AVG(salary) as avg_salary\n",
    "FROM employees\n",
    "WHERE age > 25\n",
    "GROUP BY country\n",
    "ORDER BY avg_salary DESC\n",
    "```\n",
    "\n",
    "### **∆Øu ƒëi·ªÉm:**\n",
    "- ‚úÖ D·ªÖ ƒë·ªçc, d·ªÖ hi·ªÉu (SQL standard)\n",
    "- ‚úÖ Quen thu·ªôc v·ªõi SQL developers\n",
    "- ‚úÖ Catalyst optimizer (same performance)\n",
    "- ‚úÖ T√≠ch h·ª£p v·ªõi BI tools\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß **SETUP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/11 15:29:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark Session Created\n",
      "Spark Version: 3.5.1\n",
      "Spark SQL Enabled: true\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, lit, when, desc, asc, count, sum, avg, max, min\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQL_Basics\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark Session Created\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark SQL Enabled: {spark.conf.get('spark.sql.adaptive.enabled')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä **1. T·∫†O DATA M·∫™U**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Generating sample data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 1,000 employees\n",
      "+-----------+----------+---+-----------+-------+-------------+------+--------+----------+\n",
      "|employee_id|      name|age| department|country|         city|salary|  status| hire_date|\n",
      "+-----------+----------+---+-----------+-------+-------------+------+--------+----------+\n",
      "|    EMP0001|Employee 1| 59|  Marketing| France|         Lyon| 80535|  Active|2020-06-29|\n",
      "|    EMP0002|Employee 2| 30|         HR| Canada|      Toronto| 83090|  Active|2021-10-04|\n",
      "|    EMP0003|Employee 3| 40|Engineering|    USA|San Francisco| 78789|Inactive|2022-04-07|\n",
      "|    EMP0004|Employee 4| 59|      Sales| Canada|      Toronto| 91691|  Active|2022-11-07|\n",
      "|    EMP0005|Employee 5| 50|      Sales|     UK|       London| 83566|  Active|2022-07-30|\n",
      "+-----------+----------+---+-----------+-------+-------------+------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "‚úÖ Generated 50 projects\n",
      "+----------+------------+----------+------+-----------+\n",
      "|project_id|project_name|department|budget|     status|\n",
      "+----------+------------+----------+------+-----------+\n",
      "|   PROJ001|   Project 1|   Finance|185879|In Progress|\n",
      "|   PROJ002|   Project 2|   Finance|273924|    On Hold|\n",
      "|   PROJ003|   Project 3|   Finance|286535|   Planning|\n",
      "|   PROJ004|   Project 4|   Finance|173967|    On Hold|\n",
      "|   PROJ005|   Project 5|     Sales|251135|  Completed|\n",
      "+----------+------------+----------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üîπ Generating sample data...\")\n",
    "\n",
    "# Employees data\n",
    "departments = [\"Engineering\", \"Sales\", \"Marketing\", \"HR\", \"Finance\"]\n",
    "countries = [\"USA\", \"UK\", \"Germany\", \"France\", \"Canada\"]\n",
    "cities = {\n",
    "    \"USA\": [\"New York\", \"San Francisco\", \"Seattle\"],\n",
    "    \"UK\": [\"London\", \"Manchester\"],\n",
    "    \"Germany\": [\"Berlin\", \"Munich\"],\n",
    "    \"France\": [\"Paris\", \"Lyon\"],\n",
    "    \"Canada\": [\"Toronto\", \"Vancouver\"]\n",
    "}\n",
    "\n",
    "employees_data = []\n",
    "for i in range(1, 1001):\n",
    "    country = random.choice(countries)\n",
    "    city = random.choice(cities[country])\n",
    "    dept = random.choice(departments)\n",
    "    \n",
    "    # Salary based on department\n",
    "    base_salary = {\n",
    "        \"Engineering\": 80000,\n",
    "        \"Sales\": 70000,\n",
    "        \"Marketing\": 65000,\n",
    "        \"HR\": 60000,\n",
    "        \"Finance\": 75000\n",
    "    }[dept]\n",
    "    \n",
    "    salary = base_salary + random.randint(-10000, 30000)\n",
    "    \n",
    "    employees_data.append((\n",
    "        f\"EMP{i:04d}\",\n",
    "        f\"Employee {i}\",\n",
    "        random.randint(22, 60),\n",
    "        dept,\n",
    "        country,\n",
    "        city,\n",
    "        salary,\n",
    "        random.choice([\"Active\", \"Active\", \"Active\", \"Inactive\"]),  # 75% active\n",
    "        (datetime(2020, 1, 1) + timedelta(days=random.randint(0, 1460))).strftime(\"%Y-%m-%d\")\n",
    "    ))\n",
    "\n",
    "employees = spark.createDataFrame(employees_data,\n",
    "    [\"employee_id\", \"name\", \"age\", \"department\", \"country\", \"city\", \"salary\", \"status\", \"hire_date\"])\n",
    "\n",
    "print(f\"‚úÖ Generated {employees.count():,} employees\")\n",
    "employees.show(5)\n",
    "\n",
    "# Projects data\n",
    "projects_data = []\n",
    "for i in range(1, 51):\n",
    "    dept = random.choice(departments)\n",
    "    budget = random.randint(50000, 500000)\n",
    "    \n",
    "    projects_data.append((\n",
    "        f\"PROJ{i:03d}\",\n",
    "        f\"Project {i}\",\n",
    "        dept,\n",
    "        budget,\n",
    "        random.choice([\"Planning\", \"In Progress\", \"Completed\", \"On Hold\"])\n",
    "    ))\n",
    "\n",
    "projects = spark.createDataFrame(projects_data,\n",
    "    [\"project_id\", \"project_name\", \"department\", \"budget\", \"status\"])\n",
    "\n",
    "print(f\"\\n‚úÖ Generated {projects.count():,} projects\")\n",
    "projects.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã **2. CREATE TEMPORARY VIEWS**\n",
    "\n",
    "### **Temporary View l√† g√¨?**\n",
    "- Table ·∫£o trong Spark SQL\n",
    "- Ch·ªâ t·ªìn t·∫°i trong session\n",
    "- Cho ph√©p query b·∫±ng SQL\n",
    "\n",
    "### **Syntax:**\n",
    "```python\n",
    "df.createOrReplaceTempView(\"table_name\")\n",
    "spark.sql(\"SELECT * FROM table_name\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîπ DEMO 1: Create Temporary Views\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Created temporary views:\n",
      "   - employees\n",
      "   - projects\n",
      "\n",
      "üìã Available tables:\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|         |employees|       true|\n",
      "|         | projects|       true|\n",
      "+---------+---------+-----------+\n",
      "\n",
      "\n",
      "üîπ Simple SQL query:\n",
      "+-----------+----------+---+-----------+-------+-------------+------+--------+----------+\n",
      "|employee_id|      name|age| department|country|         city|salary|  status| hire_date|\n",
      "+-----------+----------+---+-----------+-------+-------------+------+--------+----------+\n",
      "|    EMP0001|Employee 1| 59|  Marketing| France|         Lyon| 80535|  Active|2020-06-29|\n",
      "|    EMP0002|Employee 2| 30|         HR| Canada|      Toronto| 83090|  Active|2021-10-04|\n",
      "|    EMP0003|Employee 3| 40|Engineering|    USA|San Francisco| 78789|Inactive|2022-04-07|\n",
      "|    EMP0004|Employee 4| 59|      Sales| Canada|      Toronto| 91691|  Active|2022-11-07|\n",
      "|    EMP0005|Employee 5| 50|      Sales|     UK|       London| 83566|  Active|2022-07-30|\n",
      "+-----------+----------+---+-----------+-------+-------------+------+--------+----------+\n",
      "\n",
      "\n",
      "üí° KEY POINTS:\n",
      "   - createOrReplaceTempView() creates a temporary table\n",
      "   - Use spark.sql() to run SQL queries\n",
      "   - Returns a DataFrame\n",
      "   - View exists only in current session\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üîπ DEMO 1: Create Temporary Views\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create temporary views\n",
    "employees.createOrReplaceTempView(\"employees\")\n",
    "projects.createOrReplaceTempView(\"projects\")\n",
    "\n",
    "print(\"\\n‚úÖ Created temporary views:\")\n",
    "print(\"   - employees\")\n",
    "print(\"   - projects\")\n",
    "\n",
    "# List all tables\n",
    "print(\"\\nüìã Available tables:\")\n",
    "spark.sql(\"SHOW TABLES\").show()\n",
    "\n",
    "# Simple query\n",
    "print(\"\\nüîπ Simple SQL query:\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM employees \n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "result.show()\n",
    "\n",
    "print(\"\"\"\n",
    "üí° KEY POINTS:\n",
    "   - createOrReplaceTempView() creates a temporary table\n",
    "   - Use spark.sql() to run SQL queries\n",
    "   - Returns a DataFrame\n",
    "   - View exists only in current session\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç **3. BASIC SQL QUERIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîπ DEMO 2: Basic SQL Queries\n",
      "================================================================================\n",
      "\n",
      "üìä Query 1: High earners (salary > 80000)\n",
      "+-----------+------------+-----------+------+\n",
      "|employee_id|        name| department|salary|\n",
      "+-----------+------------+-----------+------+\n",
      "|    EMP0127|Employee 127|Engineering|109993|\n",
      "|    EMP0184|Employee 184|Engineering|109861|\n",
      "|    EMP0915|Employee 915|Engineering|109813|\n",
      "|    EMP0513|Employee 513|Engineering|109597|\n",
      "|    EMP0119|Employee 119|Engineering|109342|\n",
      "|    EMP0227|Employee 227|Engineering|109293|\n",
      "|    EMP0014| Employee 14|Engineering|109291|\n",
      "|    EMP0265|Employee 265|Engineering|109183|\n",
      "|    EMP0447|Employee 447|Engineering|108618|\n",
      "|    EMP0301|Employee 301|Engineering|108589|\n",
      "+-----------+------------+-----------+------+\n",
      "\n",
      "\n",
      "üìä Query 2: Average salary by department\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-----------------+----------+----------+\n",
      "| department|employee_count|       avg_salary|min_salary|max_salary|\n",
      "+-----------+--------------+-----------------+----------+----------+\n",
      "|Engineering|           128|    88664.8359375|     70683|    109861|\n",
      "|    Finance|           160|      83612.98125|     65354|    104792|\n",
      "|      Sales|           145|80713.99310344827|     60030|     99948|\n",
      "|  Marketing|           162|76166.91358024691|     55399|     94834|\n",
      "|         HR|           163|69340.77300613497|     50271|     89966|\n",
      "+-----------+--------------+-----------------+----------+----------+\n",
      "\n",
      "\n",
      "üìä Query 3: Engineering employees in USA\n",
      "+------------+-------------+------+\n",
      "|        name|         city|salary|\n",
      "+------------+-------------+------+\n",
      "|Employee 301|San Francisco|108589|\n",
      "|Employee 380|     New York|107733|\n",
      "|Employee 472|      Seattle|107447|\n",
      "|Employee 675|San Francisco|105027|\n",
      "|Employee 244|      Seattle|100634|\n",
      "|Employee 535|      Seattle| 97087|\n",
      "|Employee 249|     New York| 96215|\n",
      "|Employee 885|     New York| 95331|\n",
      "|Employee 286|     New York| 93762|\n",
      "|Employee 580|San Francisco| 92684|\n",
      "|Employee 678|San Francisco| 90675|\n",
      "|Employee 680|     New York| 89773|\n",
      "|Employee 316|San Francisco| 87371|\n",
      "|Employee 207|      Seattle| 86763|\n",
      "|Employee 299|      Seattle| 84959|\n",
      "|Employee 157|     New York| 84466|\n",
      "|Employee 900|     New York| 84341|\n",
      "|Employee 211|     New York| 80236|\n",
      "|Employee 549|     New York| 79793|\n",
      "|Employee 377|      Seattle| 79778|\n",
      "+------------+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "üìä Query 4: Departments with avg salary > 70000\n",
      "+-----------+-----+-----------------+\n",
      "| department|count|       avg_salary|\n",
      "+-----------+-----+-----------------+\n",
      "|Engineering|  179|89469.69832402235|\n",
      "|    Finance|  212|84517.98113207547|\n",
      "|      Sales|  189|80750.02645502645|\n",
      "|  Marketing|  213|75485.32863849765|\n",
      "+-----------+-----+-----------------+\n",
      "\n",
      "\n",
      "üí° SQL BASICS:\n",
      "   - SELECT: Choose columns\n",
      "   - WHERE: Filter rows\n",
      "   - GROUP BY: Aggregate data\n",
      "   - HAVING: Filter aggregated results\n",
      "   - ORDER BY: Sort results\n",
      "   - LIMIT: Limit number of rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üîπ DEMO 2: Basic SQL Queries\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Query 1: SELECT with WHERE\n",
    "print(\"\\nüìä Query 1: High earners (salary > 80000)\")\n",
    "result1 = spark.sql(\"\"\"\n",
    "    SELECT employee_id, name, department, salary\n",
    "    FROM employees\n",
    "    WHERE salary > 80000\n",
    "    ORDER BY salary DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "result1.show()\n",
    "\n",
    "# Query 2: GROUP BY with aggregations\n",
    "print(\"\\nüìä Query 2: Average salary by department\")\n",
    "result2 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        COUNT(*) as employee_count,\n",
    "        AVG(salary) as avg_salary,\n",
    "        MIN(salary) as min_salary,\n",
    "        MAX(salary) as max_salary\n",
    "    FROM employees\n",
    "    WHERE status = 'Active'\n",
    "    GROUP BY department\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "result2.show()\n",
    "\n",
    "# Query 3: Multiple conditions\n",
    "print(\"\\nüìä Query 3: Engineering employees in USA\")\n",
    "result3 = spark.sql(\"\"\"\n",
    "    SELECT name, city, salary\n",
    "    FROM employees\n",
    "    WHERE department = 'Engineering'\n",
    "      AND country = 'USA'\n",
    "      AND salary > 75000\n",
    "    ORDER BY salary DESC\n",
    "\"\"\")\n",
    "result3.show()\n",
    "\n",
    "# Query 4: HAVING clause\n",
    "print(\"\\nüìä Query 4: Departments with avg salary > 70000\")\n",
    "result4 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        COUNT(*) as count,\n",
    "        AVG(salary) as avg_salary\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "    HAVING AVG(salary) > 70000\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "result4.show()\n",
    "\n",
    "print(\"\"\"\n",
    "üí° SQL BASICS:\n",
    "   - SELECT: Choose columns\n",
    "   - WHERE: Filter rows\n",
    "   - GROUP BY: Aggregate data\n",
    "   - HAVING: Filter aggregated results\n",
    "   - ORDER BY: Sort results\n",
    "   - LIMIT: Limit number of rows\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß **4. SQL FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîπ DEMO 3: SQL Functions\n",
      "================================================================================\n",
      "\n",
      "üìä String Functions:\n",
      "+----------+----------+----------+-----------+----------+------------------------+\n",
      "|name      |upper_name|lower_name|name_length|short_name|full_info               |\n",
      "+----------+----------+----------+-----------+----------+------------------------+\n",
      "|Employee 1|EMPLOYEE 1|employee 1|10         |Employee  |Employee 1 - Marketing  |\n",
      "|Employee 2|EMPLOYEE 2|employee 2|10         |Employee  |Employee 2 - HR         |\n",
      "|Employee 3|EMPLOYEE 3|employee 3|10         |Employee  |Employee 3 - Engineering|\n",
      "|Employee 4|EMPLOYEE 4|employee 4|10         |Employee  |Employee 4 - Sales      |\n",
      "|Employee 5|EMPLOYEE 5|employee 5|10         |Employee  |Employee 5 - Sales      |\n",
      "+----------+----------+----------+-----------+----------+------------------------+\n",
      "\n",
      "\n",
      "üìä Date Functions:\n",
      "+----------+----------+---------+----------+-----------+-------------+-----------------+\n",
      "|      name| hire_date|hire_year|hire_month|day_of_week|days_employed|first_anniversary|\n",
      "+----------+----------+---------+----------+-----------+-------------+-----------------+\n",
      "|Employee 1|2020-06-29|     2020|         6|          2|         2022|       2021-06-29|\n",
      "|Employee 2|2021-10-04|     2021|        10|          2|         1560|       2022-10-04|\n",
      "|Employee 3|2022-04-07|     2022|         4|          5|         1375|       2023-04-07|\n",
      "|Employee 4|2022-11-07|     2022|        11|          2|         1161|       2023-11-07|\n",
      "|Employee 5|2022-07-30|     2022|         7|          7|         1261|       2023-07-30|\n",
      "+----------+----------+---------+----------+-----------+-------------+-----------------+\n",
      "\n",
      "\n",
      "üìä Math Functions:\n",
      "+----------+------+--------------+-----------------------+-----------------+-------------------+\n",
      "|      name|salary|monthly_salary|salary_with_10pct_raise|rounded_up_salary|rounded_down_salary|\n",
      "+----------+------+--------------+-----------------------+-----------------+-------------------+\n",
      "|Employee 1| 80535|       6711.25|                88588.5|            81000|              80000|\n",
      "|Employee 2| 83090|       6924.17|                91399.0|            84000|              83000|\n",
      "|Employee 3| 78789|       6565.75|                86667.9|            79000|              78000|\n",
      "|Employee 4| 91691|       7640.92|               100860.1|            92000|              91000|\n",
      "|Employee 5| 83566|       6963.83|                91922.6|            84000|              83000|\n",
      "+----------+------+--------------+-----------------------+-----------------+-------------------+\n",
      "\n",
      "\n",
      "üìä Conditional Functions (CASE WHEN):\n",
      "+-----------+---+------+------------+---------+\n",
      "|       name|age|salary|salary_level|seniority|\n",
      "+-----------+---+------+------------+---------+\n",
      "| Employee 1| 59| 80535|      Medium|   Senior|\n",
      "| Employee 2| 30| 83090|      Medium|Mid-level|\n",
      "| Employee 3| 40| 78789|      Medium|Mid-level|\n",
      "| Employee 4| 59| 91691|        High|   Senior|\n",
      "| Employee 5| 50| 83566|      Medium|   Senior|\n",
      "| Employee 6| 28| 93714|        High|   Junior|\n",
      "| Employee 7| 25|107032|        High|   Junior|\n",
      "| Employee 8| 34| 75455|      Medium|Mid-level|\n",
      "| Employee 9| 27| 73816|      Medium|   Junior|\n",
      "|Employee 10| 29| 80831|      Medium|   Junior|\n",
      "+-----------+---+------+------------+---------+\n",
      "\n",
      "\n",
      "üí° COMMON SQL FUNCTIONS:\n",
      "\n",
      "String:\n",
      "   - UPPER(), LOWER(), LENGTH()\n",
      "   - SUBSTRING(), CONCAT(), TRIM()\n",
      "\n",
      "Date:\n",
      "   - YEAR(), MONTH(), DAY()\n",
      "   - DATEDIFF(), DATE_ADD(), DATE_SUB()\n",
      "   - CURRENT_DATE(), CURRENT_TIMESTAMP()\n",
      "\n",
      "Math:\n",
      "   - ROUND(), CEIL(), FLOOR()\n",
      "   - ABS(), SQRT(), POW()\n",
      "\n",
      "Conditional:\n",
      "   - CASE WHEN ... THEN ... ELSE ... END\n",
      "   - IF(), COALESCE(), NULLIF()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üîπ DEMO 3: SQL Functions\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# String functions\n",
    "print(\"\\nüìä String Functions:\")\n",
    "result_str = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        UPPER(name) as upper_name,\n",
    "        LOWER(name) as lower_name,\n",
    "        LENGTH(name) as name_length,\n",
    "        SUBSTRING(name, 1, 8) as short_name,\n",
    "        CONCAT(name, ' - ', department) as full_info\n",
    "    FROM employees\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "result_str.show(truncate=False)\n",
    "\n",
    "# Date functions\n",
    "print(\"\\nüìä Date Functions:\")\n",
    "result_date = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        hire_date,\n",
    "        YEAR(hire_date) as hire_year,\n",
    "        MONTH(hire_date) as hire_month,\n",
    "        DAYOFWEEK(hire_date) as day_of_week,\n",
    "        DATEDIFF(CURRENT_DATE(), hire_date) as days_employed,\n",
    "        DATE_ADD(hire_date, 365) as first_anniversary\n",
    "    FROM employees\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "result_date.show()\n",
    "\n",
    "# Math functions\n",
    "print(\"\\nüìä Math Functions:\")\n",
    "result_math = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        salary,\n",
    "        ROUND(salary / 12, 2) as monthly_salary,\n",
    "        ROUND(salary * 1.1, 2) as salary_with_10pct_raise,\n",
    "        CEIL(salary / 1000) * 1000 as rounded_up_salary,\n",
    "        FLOOR(salary / 1000) * 1000 as rounded_down_salary\n",
    "    FROM employees\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "result_math.show()\n",
    "\n",
    "# Conditional functions\n",
    "print(\"\\nüìä Conditional Functions (CASE WHEN):\")\n",
    "result_case = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        age,\n",
    "        salary,\n",
    "        CASE \n",
    "            WHEN salary > 90000 THEN 'High'\n",
    "            WHEN salary > 70000 THEN 'Medium'\n",
    "            ELSE 'Low'\n",
    "        END as salary_level,\n",
    "        CASE \n",
    "            WHEN age < 30 THEN 'Junior'\n",
    "            WHEN age < 45 THEN 'Mid-level'\n",
    "            ELSE 'Senior'\n",
    "        END as seniority\n",
    "    FROM employees\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "result_case.show()\n",
    "\n",
    "print(\"\"\"\n",
    "üí° COMMON SQL FUNCTIONS:\n",
    "\n",
    "String:\n",
    "   - UPPER(), LOWER(), LENGTH()\n",
    "   - SUBSTRING(), CONCAT(), TRIM()\n",
    "\n",
    "Date:\n",
    "   - YEAR(), MONTH(), DAY()\n",
    "   - DATEDIFF(), DATE_ADD(), DATE_SUB()\n",
    "   - CURRENT_DATE(), CURRENT_TIMESTAMP()\n",
    "\n",
    "Math:\n",
    "   - ROUND(), CEIL(), FLOOR()\n",
    "   - ABS(), SQRT(), POW()\n",
    "\n",
    "Conditional:\n",
    "   - CASE WHEN ... THEN ... ELSE ... END\n",
    "   - IF(), COALESCE(), NULLIF()\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîó **5. SUBQUERIES & CTEs**\n",
    "\n",
    "### **Subquery:**\n",
    "```sql\n",
    "SELECT * FROM table1\n",
    "WHERE col1 IN (SELECT col2 FROM table2)\n",
    "```\n",
    "\n",
    "### **CTE (Common Table Expression):**\n",
    "```sql\n",
    "WITH temp_table AS (\n",
    "    SELECT * FROM table1\n",
    ")\n",
    "SELECT * FROM temp_table\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîπ DEMO 4: Subqueries & CTEs\n",
      "================================================================================\n",
      "\n",
      "üìä Subquery: Employees earning above department average\n",
      "+------------+-----------+------+-----------------+\n",
      "|        name| department|salary|  dept_avg_salary|\n",
      "+------------+-----------+------+-----------------+\n",
      "|Employee 127|Engineering|109993|89469.69832402235|\n",
      "|Employee 184|Engineering|109861|89469.69832402235|\n",
      "|Employee 915|Engineering|109813|89469.69832402235|\n",
      "|Employee 513|Engineering|109597|89469.69832402235|\n",
      "|Employee 119|Engineering|109342|89469.69832402235|\n",
      "|Employee 227|Engineering|109293|89469.69832402235|\n",
      "| Employee 14|Engineering|109291|89469.69832402235|\n",
      "|Employee 265|Engineering|109183|89469.69832402235|\n",
      "|Employee 447|Engineering|108618|89469.69832402235|\n",
      "|Employee 301|Engineering|108589|89469.69832402235|\n",
      "+------------+-----------+------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "üìä CTE Example 1: Department statistics\n",
      "+-----------+--------------+----------+----------+----------+\n",
      "| department|employee_count|avg_salary|max_salary|salary_gap|\n",
      "+-----------+--------------+----------+----------+----------+\n",
      "|Engineering|           128|  88664.84|    109861|  21196.16|\n",
      "|    Finance|           160|  83612.98|    104792|  21179.02|\n",
      "|      Sales|           145|  80713.99|     99948|  19234.01|\n",
      "|  Marketing|           162|  76166.91|     94834|  18667.09|\n",
      "|         HR|           163|  69340.77|     89966|  20625.23|\n",
      "+-----------+--------------+----------+----------+----------+\n",
      "\n",
      "\n",
      "üìä CTE Example 2: Multiple CTEs\n",
      "+-----------+-----------+-----------------+---------------+\n",
      "| department|total_count|high_earner_count|high_earner_pct|\n",
      "+-----------+-----------+-----------------+---------------+\n",
      "|Engineering|        179|              132|          73.74|\n",
      "|    Finance|        212|              122|          57.55|\n",
      "|      Sales|        189|              100|          52.91|\n",
      "|  Marketing|        213|               86|          40.38|\n",
      "|         HR|        207|               44|          21.26|\n",
      "+-----------+-----------+-----------------+---------------+\n",
      "\n",
      "\n",
      "üìä CTE Example 3: Salary percentiles\n",
      "+-----------+----------+-------------+----------+\n",
      "| department|p25_salary|median_salary|p75_salary|\n",
      "+-----------+----------+-------------+----------+\n",
      "|Engineering|  75643.69|     79985.02|  84318.47|\n",
      "|    Finance|  70080.04|     74321.01|  79279.53|\n",
      "|      Sales|  65568.75|     70686.67|  75841.32|\n",
      "|  Marketing|  60312.11|     65508.91|  70546.95|\n",
      "|         HR|  55214.85|      60203.9|  64893.21|\n",
      "+-----------+----------+-------------+----------+\n",
      "\n",
      "\n",
      "üí° SUBQUERIES vs CTEs:\n",
      "\n",
      "Subquery:\n",
      "   ‚úÖ Good for simple, one-time use\n",
      "   ‚ùå Can be hard to read when nested\n",
      "   ‚ùå Evaluated multiple times if used multiple times\n",
      "\n",
      "CTE (WITH clause):\n",
      "   ‚úÖ More readable and maintainable\n",
      "   ‚úÖ Can reference multiple times\n",
      "   ‚úÖ Better for complex queries\n",
      "   ‚úÖ Can chain multiple CTEs\n",
      "\n",
      "Best Practice:\n",
      "   - Use CTEs for complex queries\n",
      "   - Use subqueries for simple, one-time filters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üîπ DEMO 4: Subqueries & CTEs\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Subquery example\n",
    "print(\"\\nüìä Subquery: Employees earning above department average\")\n",
    "result_subquery = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e.name,\n",
    "        e.department,\n",
    "        e.salary,\n",
    "        dept_avg.avg_salary as dept_avg_salary\n",
    "    FROM employees e\n",
    "    JOIN (\n",
    "        SELECT department, AVG(salary) as avg_salary\n",
    "        FROM employees\n",
    "        GROUP BY department\n",
    "    ) dept_avg ON e.department = dept_avg.department\n",
    "    WHERE e.salary > dept_avg.avg_salary\n",
    "    ORDER BY e.department, e.salary DESC\n",
    "\"\"\")\n",
    "result_subquery.show(10)\n",
    "\n",
    "# CTE example 1: Single CTE\n",
    "print(\"\\nüìä CTE Example 1: Department statistics\")\n",
    "result_cte1 = spark.sql(\"\"\"\n",
    "    WITH dept_stats AS (\n",
    "        SELECT \n",
    "            department,\n",
    "            COUNT(*) as employee_count,\n",
    "            AVG(salary) as avg_salary,\n",
    "            MAX(salary) as max_salary\n",
    "        FROM employees\n",
    "        WHERE status = 'Active'\n",
    "        GROUP BY department\n",
    "    )\n",
    "    SELECT \n",
    "        department,\n",
    "        employee_count,\n",
    "        ROUND(avg_salary, 2) as avg_salary,\n",
    "        max_salary,\n",
    "        ROUND(max_salary - avg_salary, 2) as salary_gap\n",
    "    FROM dept_stats\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "result_cte1.show()\n",
    "\n",
    "# CTE example 2: Multiple CTEs\n",
    "print(\"\\nüìä CTE Example 2: Multiple CTEs\")\n",
    "result_cte2 = spark.sql(\"\"\"\n",
    "    WITH \n",
    "    high_earners AS (\n",
    "        SELECT department, COUNT(*) as high_earner_count\n",
    "        FROM employees\n",
    "        WHERE salary > 80000\n",
    "        GROUP BY department\n",
    "    ),\n",
    "    dept_totals AS (\n",
    "        SELECT department, COUNT(*) as total_count\n",
    "        FROM employees\n",
    "        GROUP BY department\n",
    "    )\n",
    "    SELECT \n",
    "        dt.department,\n",
    "        dt.total_count,\n",
    "        COALESCE(he.high_earner_count, 0) as high_earner_count,\n",
    "        ROUND(COALESCE(he.high_earner_count, 0) * 100.0 / dt.total_count, 2) as high_earner_pct\n",
    "    FROM dept_totals dt\n",
    "    LEFT JOIN high_earners he ON dt.department = he.department\n",
    "    ORDER BY high_earner_pct DESC\n",
    "\"\"\")\n",
    "result_cte2.show()\n",
    "\n",
    "# CTE example 3: Recursive-like query\n",
    "print(\"\\nüìä CTE Example 3: Salary percentiles\")\n",
    "result_cte3 = spark.sql(\"\"\"\n",
    "    WITH salary_stats AS (\n",
    "        SELECT \n",
    "            department,\n",
    "            salary,\n",
    "            PERCENT_RANK() OVER (PARTITION BY department ORDER BY salary) as percentile\n",
    "        FROM employees\n",
    "    )\n",
    "    SELECT \n",
    "        department,\n",
    "        ROUND(AVG(CASE WHEN percentile <= 0.25 THEN salary END), 2) as p25_salary,\n",
    "        ROUND(AVG(CASE WHEN percentile <= 0.50 THEN salary END), 2) as median_salary,\n",
    "        ROUND(AVG(CASE WHEN percentile <= 0.75 THEN salary END), 2) as p75_salary\n",
    "    FROM salary_stats\n",
    "    GROUP BY department\n",
    "    ORDER BY median_salary DESC\n",
    "\"\"\")\n",
    "result_cte3.show()\n",
    "\n",
    "print(\"\"\"\n",
    "üí° SUBQUERIES vs CTEs:\n",
    "\n",
    "Subquery:\n",
    "   ‚úÖ Good for simple, one-time use\n",
    "   ‚ùå Can be hard to read when nested\n",
    "   ‚ùå Evaluated multiple times if used multiple times\n",
    "\n",
    "CTE (WITH clause):\n",
    "   ‚úÖ More readable and maintainable\n",
    "   ‚úÖ Can reference multiple times\n",
    "   ‚úÖ Better for complex queries\n",
    "   ‚úÖ Can chain multiple CTEs\n",
    "\n",
    "Best Practice:\n",
    "   - Use CTEs for complex queries\n",
    "   - Use subqueries for simple, one-time filters\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö° **6. SQL vs DATAFRAME API - PERFORMANCE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîπ DEMO 5: SQL vs DataFrame API Performance\n",
      "================================================================================\n",
      "\n",
      "üìä Method 1: Using SQL\n",
      "+-----------+-------+--------------+-----------------+----------+\n",
      "| department|country|employee_count|       avg_salary|max_salary|\n",
      "+-----------+-------+--------------+-----------------+----------+\n",
      "|Engineering|Germany|            31|91465.19354838709|    109861|\n",
      "|Engineering| France|            24|90679.04166666667|    109813|\n",
      "|Engineering|     UK|            27|88126.81481481482|    109293|\n",
      "|Engineering| Canada|            27|86416.14814814815|    108068|\n",
      "|Engineering|    USA|            19|85511.63157894737|    108589|\n",
      "|    Finance|Germany|            29|85195.06896551725|    104792|\n",
      "|    Finance|     UK|            39|84679.56410256411|    104053|\n",
      "|    Finance|    USA|            37|83775.18918918919|    104261|\n",
      "|    Finance| Canada|            31|83401.45161290323|    104504|\n",
      "|    Finance| France|            24|         79991.25|    103168|\n",
      "+-----------+-------+--------------+-----------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "‚è±Ô∏è  SQL Time: 0.782s\n",
      "\n",
      "üìä Method 2: Using DataFrame API\n",
      "+-----------+-------+--------------+-----------------+----------+\n",
      "| department|country|employee_count|       avg_salary|max_salary|\n",
      "+-----------+-------+--------------+-----------------+----------+\n",
      "|Engineering|Germany|            31|91465.19354838709|    109861|\n",
      "|Engineering| France|            24|90679.04166666667|    109813|\n",
      "|Engineering|     UK|            27|88126.81481481482|    109293|\n",
      "|Engineering| Canada|            27|86416.14814814815|    108068|\n",
      "|Engineering|    USA|            19|85511.63157894737|    108589|\n",
      "|    Finance|Germany|            29|85195.06896551725|    104792|\n",
      "|    Finance|     UK|            39|84679.56410256411|    104053|\n",
      "|    Finance|    USA|            37|83775.18918918919|    104261|\n",
      "|    Finance| Canada|            31|83401.45161290323|    104504|\n",
      "|    Finance| France|            24|         79991.25|    103168|\n",
      "+-----------+-------+--------------+-----------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "‚è±Ô∏è  DataFrame Time: 0.774s\n",
      "\n",
      "üìä Execution Plans:\n",
      "\n",
      "üîπ SQL Plan:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [department#3 ASC NULLS FIRST, avg_salary#617 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(department#3 ASC NULLS FIRST, avg_salary#617 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=964]\n",
      "      +- Filter (employee_count#616L > 5)\n",
      "         +- HashAggregate(keys=[department#3, country#4], functions=[count(1), avg(salary#6L), max(salary#6L)])\n",
      "            +- Exchange hashpartitioning(department#3, country#4, 200), ENSURE_REQUIREMENTS, [plan_id=960]\n",
      "               +- HashAggregate(keys=[department#3, country#4], functions=[partial_count(1), partial_avg(salary#6L), partial_max(salary#6L)])\n",
      "                  +- Project [department#3, country#4, salary#6L]\n",
      "                     +- Filter (isnotnull(status#7) AND (status#7 = Active))\n",
      "                        +- Scan ExistingRDD[employee_id#0,name#1,age#2L,department#3,country#4,city#5,salary#6L,status#7,hire_date#8]\n",
      "\n",
      "\n",
      "\n",
      "üîπ DataFrame Plan:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [department#3 ASC NULLS FIRST, avg_salary#672 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(department#3 ASC NULLS FIRST, avg_salary#672 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=997]\n",
      "      +- Filter (employee_count#670L > 5)\n",
      "         +- HashAggregate(keys=[department#3, country#4], functions=[count(1), avg(salary#6L), max(salary#6L)])\n",
      "            +- Exchange hashpartitioning(department#3, country#4, 200), ENSURE_REQUIREMENTS, [plan_id=993]\n",
      "               +- HashAggregate(keys=[department#3, country#4], functions=[partial_count(1), partial_avg(salary#6L), partial_max(salary#6L)])\n",
      "                  +- Project [department#3, country#4, salary#6L]\n",
      "                     +- Filter (isnotnull(status#7) AND (status#7 = Active))\n",
      "                        +- Scan ExistingRDD[employee_id#0,name#1,age#2L,department#3,country#4,city#5,salary#6L,status#7,hire_date#8]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä PERFORMANCE COMPARISON\n",
      "================================================================================\n",
      "+-------------+------------------+-----------------+\n",
      "|Method       |Time (s)          |Note             |\n",
      "+-------------+------------------+-----------------+\n",
      "|SQL          |0.7821173667907715|More readable    |\n",
      "|DataFrame API|0.7736492156982422|More programmatic|\n",
      "+-------------+------------------+-----------------+\n",
      "\n",
      "\n",
      "üí° KEY INSIGHTS:\n",
      "\n",
      "Performance:\n",
      "   - Both use Catalyst optimizer\n",
      "   - Same execution plan\n",
      "   - Same performance!\n",
      "\n",
      "When to use SQL:\n",
      "   ‚úÖ Complex analytical queries\n",
      "   ‚úÖ Team familiar with SQL\n",
      "   ‚úÖ Ad-hoc analysis\n",
      "   ‚úÖ BI tool integration\n",
      "\n",
      "When to use DataFrame API:\n",
      "   ‚úÖ Programmatic data processing\n",
      "   ‚úÖ Type safety (compile-time checks)\n",
      "   ‚úÖ IDE autocomplete\n",
      "   ‚úÖ Complex transformations\n",
      "\n",
      "Best Practice:\n",
      "   - Use both! Mix and match as needed\n",
      "   - SQL for queries, DataFrame for transformations\n",
      "   - Choose based on readability and team preference\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üîπ DEMO 5: SQL vs DataFrame API Performance\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Query: Average salary by department and country\n",
    "\n",
    "# Method 1: SQL\n",
    "print(\"\\nüìä Method 1: Using SQL\")\n",
    "start = time.time()\n",
    "result_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        country,\n",
    "        COUNT(*) as employee_count,\n",
    "        AVG(salary) as avg_salary,\n",
    "        MAX(salary) as max_salary\n",
    "    FROM employees\n",
    "    WHERE status = 'Active'\n",
    "    GROUP BY department, country\n",
    "    HAVING COUNT(*) > 5\n",
    "    ORDER BY department, avg_salary DESC\n",
    "\"\"\")\n",
    "result_sql.show(10)\n",
    "time_sql = time.time() - start\n",
    "print(f\"‚è±Ô∏è  SQL Time: {time_sql:.3f}s\")\n",
    "\n",
    "# Method 2: DataFrame API\n",
    "print(\"\\nüìä Method 2: Using DataFrame API\")\n",
    "start = time.time()\n",
    "result_df = employees \\\n",
    "    .filter(col(\"status\") == \"Active\") \\\n",
    "    .groupBy(\"department\", \"country\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"employee_count\"),\n",
    "        F.avg(\"salary\").alias(\"avg_salary\"),\n",
    "        F.max(\"salary\").alias(\"max_salary\")\n",
    "    ) \\\n",
    "    .filter(col(\"employee_count\") > 5) \\\n",
    "    .orderBy(\"department\", desc(\"avg_salary\"))\n",
    "result_df.show(10)\n",
    "time_df = time.time() - start\n",
    "print(f\"‚è±Ô∏è  DataFrame Time: {time_df:.3f}s\")\n",
    "\n",
    "# Compare execution plans\n",
    "print(\"\\nüìä Execution Plans:\")\n",
    "print(\"\\nüîπ SQL Plan:\")\n",
    "result_sql.explain()\n",
    "\n",
    "print(\"\\nüîπ DataFrame Plan:\")\n",
    "result_df.explain()\n",
    "\n",
    "# Comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison = [\n",
    "    (\"SQL\", time_sql, \"More readable\"),\n",
    "    (\"DataFrame API\", time_df, \"More programmatic\")\n",
    "]\n",
    "\n",
    "comparison_df = spark.createDataFrame(comparison,\n",
    "    [\"Method\", \"Time (s)\", \"Note\"])\n",
    "comparison_df.show(truncate=False)\n",
    "\n",
    "print(\"\"\"\n",
    "üí° KEY INSIGHTS:\n",
    "\n",
    "Performance:\n",
    "   - Both use Catalyst optimizer\n",
    "   - Same execution plan\n",
    "   - Same performance!\n",
    "\n",
    "When to use SQL:\n",
    "   ‚úÖ Complex analytical queries\n",
    "   ‚úÖ Team familiar with SQL\n",
    "   ‚úÖ Ad-hoc analysis\n",
    "   ‚úÖ BI tool integration\n",
    "\n",
    "When to use DataFrame API:\n",
    "   ‚úÖ Programmatic data processing\n",
    "   ‚úÖ Type safety (compile-time checks)\n",
    "   ‚úÖ IDE autocomplete\n",
    "   ‚úÖ Complex transformations\n",
    "\n",
    "Best Practice:\n",
    "   - Use both! Mix and match as needed\n",
    "   - SQL for queries, DataFrame for transformations\n",
    "   - Choose based on readability and team preference\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì **KEY TAKEAWAYS**\n",
    "\n",
    "### **‚úÖ What You Learned:**\n",
    "\n",
    "1. **Temporary Views**\n",
    "   - createOrReplaceTempView()\n",
    "   - Query with spark.sql()\n",
    "   - Session-scoped\n",
    "\n",
    "2. **Basic SQL**\n",
    "   - SELECT, WHERE, GROUP BY\n",
    "   - HAVING, ORDER BY, LIMIT\n",
    "   - Aggregations (COUNT, AVG, SUM, etc.)\n",
    "\n",
    "3. **SQL Functions**\n",
    "   - String: UPPER, LOWER, CONCAT\n",
    "   - Date: YEAR, MONTH, DATEDIFF\n",
    "   - Math: ROUND, CEIL, FLOOR\n",
    "   - Conditional: CASE WHEN\n",
    "\n",
    "4. **Subqueries & CTEs**\n",
    "   - Subqueries for simple filters\n",
    "   - CTEs (WITH) for complex queries\n",
    "   - Multiple CTEs for readability\n",
    "\n",
    "5. **SQL vs DataFrame API**\n",
    "   - Same performance (Catalyst optimizer)\n",
    "   - SQL: More readable for queries\n",
    "   - DataFrame: More programmatic\n",
    "   - Use both as needed!\n",
    "\n",
    "### **üìä Quick Reference:**\n",
    "\n",
    "```python\n",
    "# Create view\n",
    "df.createOrReplaceTempView(\"table_name\")\n",
    "\n",
    "# Run SQL\n",
    "result = spark.sql(\"SELECT * FROM table_name\")\n",
    "\n",
    "# CTE\n",
    "spark.sql(\"\"\"\n",
    "    WITH temp AS (\n",
    "        SELECT * FROM table\n",
    "    )\n",
    "    SELECT * FROM temp\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "### **üöÄ Next:** Day 5 - Lesson 2: Advanced SQL\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark session stopped\n",
      "\n",
      "üéâ DAY 5 - LESSON 1 COMPLETED!\n",
      "\n",
      "üí° Remember:\n",
      "   - SQL and DataFrame API have same performance\n",
      "   - Use createOrReplaceTempView() for SQL queries\n",
      "   - CTEs make complex queries readable\n",
      "   - Mix SQL and DataFrame API as needed\n",
      "\n",
      "üî• Quote: 'SQL is not dead, it's just distributed!' üöÄ\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "spark.catalog.clearCache()\n",
    "spark.stop()\n",
    "\n",
    "print(\"‚úÖ Spark session stopped\")\n",
    "print(\"\\nüéâ DAY 5 - LESSON 1 COMPLETED!\")\n",
    "print(\"\\nüí° Remember:\")\n",
    "print(\"   - SQL and DataFrame API have same performance\")\n",
    "print(\"   - Use createOrReplaceTempView() for SQL queries\")\n",
    "print(\"   - CTEs make complex queries readable\")\n",
    "print(\"   - Mix SQL and DataFrame API as needed\")\n",
    "print(\"\\nüî• Quote: 'SQL is not dead, it's just distributed!' üöÄ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
