{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö° SQL OPTIMIZATION\n",
    "\n",
    "---\n",
    "\n",
    "## üìã **DAY 5 - LESSON 4: SQL OPTIMIZATION**\n",
    "\n",
    "### **üéØ M·ª§C TI√äU:**\n",
    "\n",
    "1. **Query Execution Plans** - Understand EXPLAIN\n",
    "2. **Cost-Based Optimization** - Statistics and CBO\n",
    "3. **Predicate Pushdown** - Filter early\n",
    "4. **Join Optimization** - Broadcast, Sort-Merge, Shuffle Hash\n",
    "5. **Partition Pruning** - Skip unnecessary partitions\n",
    "6. **Query Hints** - Control execution\n",
    "7. **Common Anti-Patterns** - What to avoid\n",
    "8. **Performance Tuning** - Best practices\n",
    "\n",
    "---\n",
    "\n",
    "## üí° **SQL OPTIMIZATION:**\n",
    "\n",
    "- Understand how Spark executes queries\n",
    "- Use EXPLAIN to analyze plans\n",
    "- Optimize joins and filters\n",
    "- Avoid common mistakes\n",
    "- Tune for performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß **SETUP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/11 16:49:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/01/11 16:49:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark Session Created\n",
      "Spark Version: 3.5.1\n",
      "Adaptive Execution: true\n",
      "CBO Enabled: true\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, lit, when, desc, asc, broadcast\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SQLOptimization\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\") \\\n",
    "    .config(\"spark.sql.cbo.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.statistics.histogram.enabled\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark Session Created\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Adaptive Execution: {spark.conf.get('spark.sql.adaptive.enabled')}\")\n",
    "print(f\"CBO Enabled: {spark.conf.get('spark.sql.cbo.enabled')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä **1. T·∫†O DATA M·∫™U**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìä 1. GENERATING SAMPLE DATA\n",
      "================================================================================\n",
      "\n",
      "üîπ Generating 10,000 employees...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 16:50:05 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:50:20 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:50:35 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:50:50 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:51:05 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:51:20 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:51:35 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:51:50 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:52:05 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:52:20 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:52:35 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:52:50 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:53:05 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:53:20 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:53:35 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:53:50 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:54:05 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:54:20 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:54:35 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:54:50 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:55:05 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:55:20 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:55:35 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:55:50 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:56:05 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:56:20 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:56:35 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:56:50 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:57:05 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:57:20 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:57:35 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:57:50 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:58:05 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:58:20 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:58:35 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:58:50 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:59:05 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:59:20 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:59:35 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 16:59:50 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 17:00:05 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 17:00:20 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 17:00:35 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 17:00:50 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 17:01:05 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 17:01:20 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 17:01:35 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/11 17:01:50 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "[Stage 0:>                                                          (0 + 0) / 2]\r"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üìä 1. GENERATING SAMPLE DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Large employees dataset\n",
    "print(\"\\nüîπ Generating 10,000 employees...\")\n",
    "\n",
    "departments = [\"Engineering\", \"Sales\", \"Marketing\", \"HR\", \"Finance\"]\n",
    "countries = [\"USA\", \"UK\", \"Germany\", \"France\", \"Canada\"]\n",
    "cities = {\n",
    "    \"USA\": [\"New York\", \"San Francisco\", \"Seattle\"],\n",
    "    \"UK\": [\"London\", \"Manchester\"],\n",
    "    \"Germany\": [\"Berlin\", \"Munich\"],\n",
    "    \"France\": [\"Paris\", \"Lyon\"],\n",
    "    \"Canada\": [\"Toronto\", \"Vancouver\"]\n",
    "}\n",
    "\n",
    "employees_data = []\n",
    "for i in range(1, 10001):\n",
    "    country = random.choice(countries)\n",
    "    city = random.choice(cities[country])\n",
    "    dept = random.choice(departments)\n",
    "    \n",
    "    base_salary = {\n",
    "        \"Engineering\": 80000,\n",
    "        \"Sales\": 70000,\n",
    "        \"Marketing\": 65000,\n",
    "        \"HR\": 60000,\n",
    "        \"Finance\": 75000\n",
    "    }[dept]\n",
    "    \n",
    "    employees_data.append((\n",
    "        f\"EMP{i:05d}\",\n",
    "        f\"Employee {i}\",\n",
    "        random.randint(22, 60),\n",
    "        dept,\n",
    "        country,\n",
    "        city,\n",
    "        base_salary + random.randint(-10000, 30000),\n",
    "        random.choice([\"Active\", \"Active\", \"Active\", \"Inactive\"]),\n",
    "        (datetime(2020, 1, 1) + timedelta(days=random.randint(0, 1460))).strftime(\"%Y-%m-%d\")\n",
    "    ))\n",
    "\n",
    "employees = spark.createDataFrame(employees_data,\n",
    "    [\"employee_id\", \"name\", \"age\", \"department\", \"country\", \"city\", \"salary\", \"status\", \"hire_date\"])\n",
    "\n",
    "print(f\"‚úÖ Generated {employees.count():,} employees\")\n",
    "\n",
    "# Small departments lookup table\n",
    "print(\"\\nüîπ Generating departments lookup...\")\n",
    "\n",
    "departments_data = [\n",
    "    (\"Engineering\", \"Tech\", \"John Doe\"),\n",
    "    (\"Sales\", \"Business\", \"Jane Smith\"),\n",
    "    (\"Marketing\", \"Business\", \"Bob Johnson\"),\n",
    "    (\"HR\", \"Support\", \"Alice Brown\"),\n",
    "    (\"Finance\", \"Support\", \"Charlie Wilson\")\n",
    "]\n",
    "\n",
    "departments_lookup = spark.createDataFrame(departments_data,\n",
    "    [\"department\", \"division\", \"manager\"])\n",
    "\n",
    "print(f\"‚úÖ Generated {departments_lookup.count()} departments\")\n",
    "\n",
    "# Create temporary views\n",
    "employees.createOrReplaceTempView(\"employees\")\n",
    "departments_lookup.createOrReplaceTempView(\"departments\")\n",
    "\n",
    "print(\"\\n‚úÖ Created temporary views\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ **2. QUERY EXECUTION PLANS**\n",
    "\n",
    "### **Understanding EXPLAIN:**\n",
    "- **Parsed Logical Plan**: SQL ‚Üí Logical plan\n",
    "- **Analyzed Logical Plan**: Resolve columns, tables\n",
    "- **Optimized Logical Plan**: Apply optimizations\n",
    "- **Physical Plan**: How to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üìñ 2. QUERY EXECUTION PLANS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Simple query\n",
    "query = \"\"\"\n",
    "    SELECT department, COUNT(*) as count, AVG(salary) as avg_salary\n",
    "    FROM employees\n",
    "    WHERE status = 'Active'\n",
    "    GROUP BY department\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "\n",
    "print(\"\\nüìä A. SIMPLE EXPLAIN\")\n",
    "print(\"-\" * 80)\n",
    "result.explain()\n",
    "\n",
    "print(\"\\nüìä B. EXTENDED EXPLAIN (All stages)\")\n",
    "print(\"-\" * 80)\n",
    "result.explain(extended=True)\n",
    "\n",
    "print(\"\\nüìä C. FORMATTED EXPLAIN (Easy to read)\")\n",
    "print(\"-\" * 80)\n",
    "result.explain(mode=\"formatted\")\n",
    "\n",
    "print(\"\\nüìä D. COST EXPLAIN (With statistics)\")\n",
    "print(\"-\" * 80)\n",
    "result.explain(mode=\"cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain modes comparison\n",
    "print(\"\\nüìä EXPLAIN MODES COMPARISON\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "üí° EXPLAIN MODES:\n",
    "\n",
    "1. explain() - Simple\n",
    "   - Shows physical plan only\n",
    "   - Easy to read\n",
    "   - Good for quick check\n",
    "\n",
    "2. explain(extended=True) - Extended\n",
    "   - Shows all 4 stages:\n",
    "     * Parsed Logical Plan\n",
    "     * Analyzed Logical Plan\n",
    "     * Optimized Logical Plan\n",
    "     * Physical Plan\n",
    "   - Good for debugging\n",
    "\n",
    "3. explain(mode=\"formatted\") - Formatted\n",
    "   - Tree structure\n",
    "   - Shows node IDs\n",
    "   - Easy to understand flow\n",
    "\n",
    "4. explain(mode=\"cost\") - Cost\n",
    "   - Shows statistics\n",
    "   - Row counts, data sizes\n",
    "   - Good for optimization\n",
    "\n",
    "5. explain(mode=\"codegen\") - Code Generation\n",
    "   - Shows generated Java code\n",
    "   - Advanced debugging\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä **3. COST-BASED OPTIMIZATION (CBO)**\n",
    "\n",
    "### **What is CBO?**\n",
    "- Uses statistics to choose best execution plan\n",
    "- Estimates cost of different strategies\n",
    "- Chooses cheapest plan\n",
    "\n",
    "### **Statistics:**\n",
    "- Table statistics: row count, size\n",
    "- Column statistics: min, max, distinct count, nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üìä 3. COST-BASED OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# A. Compute statistics\n",
    "print(\"\\nüìä A. COMPUTE STATISTICS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Table statistics\n",
    "print(\"\\n1. Computing table statistics...\")\n",
    "spark.sql(\"ANALYZE TABLE employees COMPUTE STATISTICS\")\n",
    "print(\"‚úÖ Table statistics computed\")\n",
    "\n",
    "# Column statistics\n",
    "print(\"\\n2. Computing column statistics...\")\n",
    "spark.sql(\"\"\"\n",
    "    ANALYZE TABLE employees \n",
    "    COMPUTE STATISTICS FOR COLUMNS \n",
    "    department, country, salary, age, status\n",
    "\"\"\")\n",
    "print(\"‚úÖ Column statistics computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B. View statistics\n",
    "print(\"\\nüìä B. VIEW STATISTICS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Table stats\n",
    "print(\"\\n1. Table statistics:\")\n",
    "spark.sql(\"DESCRIBE EXTENDED employees\").filter(\n",
    "    col(\"col_name\").contains(\"Statistics\")\n",
    ").show(truncate=False)\n",
    "\n",
    "# Column stats\n",
    "print(\"\\n2. Column statistics (salary):\")\n",
    "spark.sql(\"DESCRIBE EXTENDED employees salary\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C. Compare with and without statistics\n",
    "print(\"\\nüìä C. COMPARE PLANS (With vs Without Statistics)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT e.department, d.division, COUNT(*) as count\n",
    "    FROM employees e\n",
    "    JOIN departments d ON e.department = d.department\n",
    "    WHERE e.salary > 80000\n",
    "    GROUP BY e.department, d.division\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nWith statistics (CBO enabled):\")\n",
    "spark.sql(query).explain(mode=\"cost\")\n",
    "\n",
    "print(\"\"\"\n",
    "üí° CBO Benefits:\n",
    "   - Better join strategy selection\n",
    "   - Accurate cardinality estimation\n",
    "   - Optimal filter ordering\n",
    "   - Better resource allocation\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç **4. PREDICATE PUSHDOWN**\n",
    "\n",
    "### **What is Predicate Pushdown?**\n",
    "- Push filters as early as possible\n",
    "- Reduce data processed\n",
    "- Happens automatically in Spark\n",
    "\n",
    "### **Benefits:**\n",
    "- Less data to read\n",
    "- Less data to shuffle\n",
    "- Faster queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üîç 4. PREDICATE PUSHDOWN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Bad: Filter after aggregation\n",
    "print(\"\\n‚ùå BAD: Filter after aggregation\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "bad_query = spark.sql(\"\"\"\n",
    "    SELECT department, COUNT(*) as count\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "\"\"\").filter(col(\"count\") > 1000)\n",
    "\n",
    "print(\"Plan:\")\n",
    "bad_query.explain()\n",
    "\n",
    "start = time.time()\n",
    "bad_result = bad_query.collect()\n",
    "bad_time = time.time() - start\n",
    "print(f\"\\nTime: {bad_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good: Filter before aggregation\n",
    "print(\"\\n‚úÖ GOOD: Filter before aggregation\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "good_query = spark.sql(\"\"\"\n",
    "    SELECT department, COUNT(*) as count\n",
    "    FROM employees\n",
    "    WHERE status = 'Active'\n",
    "    GROUP BY department\n",
    "    HAVING COUNT(*) > 1000\n",
    "\"\"\")\n",
    "\n",
    "print(\"Plan:\")\n",
    "good_query.explain()\n",
    "\n",
    "start = time.time()\n",
    "good_result = good_query.collect()\n",
    "good_time = time.time() - start\n",
    "print(f\"\\nTime: {good_time:.3f}s\")\n",
    "\n",
    "print(f\"\\nüìä Speedup: {bad_time/good_time:.2f}x faster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicate pushdown examples\n",
    "print(\"\\nüìä PREDICATE PUSHDOWN EXAMPLES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "üí° PREDICATE PUSHDOWN:\n",
    "\n",
    "1. Column Pruning\n",
    "   ‚ùå SELECT * FROM table\n",
    "   ‚úÖ SELECT col1, col2 FROM table\n",
    "\n",
    "2. Filter Pushdown\n",
    "   ‚ùå SELECT * FROM table ‚Üí Filter in Python\n",
    "   ‚úÖ SELECT * FROM table WHERE condition\n",
    "\n",
    "3. Projection Pushdown\n",
    "   ‚ùå Read all columns ‚Üí Select needed\n",
    "   ‚úÖ Read only needed columns\n",
    "\n",
    "4. Partition Pruning\n",
    "   ‚ùå Scan all partitions\n",
    "   ‚úÖ Scan only matching partitions\n",
    "\n",
    "Spark does this automatically!\n",
    "But you can help by:\n",
    "   - Filtering early\n",
    "   - Selecting only needed columns\n",
    "   - Using partition columns in filters\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîó **5. JOIN OPTIMIZATION**\n",
    "\n",
    "### **Join Strategies:**\n",
    "1. **Broadcast Hash Join** - Small table broadcast to all nodes\n",
    "2. **Sort-Merge Join** - Sort both sides, then merge\n",
    "3. **Shuffle Hash Join** - Hash partition both sides\n",
    "\n",
    "### **When to use:**\n",
    "- Broadcast: One table < 10MB\n",
    "- Sort-Merge: Large tables, sorted data\n",
    "- Shuffle Hash: Large tables, not sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üîó 5. JOIN OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# A. Broadcast Join (Small table)\n",
    "print(\"\\nüìä A. BROADCAST JOIN\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Without hint\n",
    "print(\"\\n1. Without broadcast hint:\")\n",
    "query_no_hint = spark.sql(\"\"\"\n",
    "    SELECT e.*, d.division, d.manager\n",
    "    FROM employees e\n",
    "    JOIN departments d ON e.department = d.department\n",
    "    WHERE e.salary > 80000\n",
    "\"\"\")\n",
    "\n",
    "query_no_hint.explain()\n",
    "\n",
    "start = time.time()\n",
    "count_no_hint = query_no_hint.count()\n",
    "time_no_hint = time.time() - start\n",
    "print(f\"\\nTime: {time_no_hint:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With broadcast hint\n",
    "print(\"\\n2. With broadcast hint:\")\n",
    "query_with_hint = spark.sql(\"\"\"\n",
    "    SELECT /*+ BROADCAST(d) */ e.*, d.division, d.manager\n",
    "    FROM employees e\n",
    "    JOIN departments d ON e.department = d.department\n",
    "    WHERE e.salary > 80000\n",
    "\"\"\")\n",
    "\n",
    "query_with_hint.explain()\n",
    "\n",
    "start = time.time()\n",
    "count_with_hint = query_with_hint.count()\n",
    "time_with_hint = time.time() - start\n",
    "print(f\"\\nTime: {time_with_hint:.3f}s\")\n",
    "\n",
    "if time_no_hint > time_with_hint:\n",
    "    print(f\"\\n‚úÖ Broadcast join is {time_no_hint/time_with_hint:.2f}x faster!\")\n",
    "else:\n",
    "    print(f\"\\nüí° Similar performance (Spark may auto-broadcast)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B. DataFrame API broadcast\n",
    "print(\"\\nüìä B. BROADCAST IN DATAFRAME API\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Using broadcast function\n",
    "result_broadcast = employees.join(\n",
    "    broadcast(departments_lookup),\n",
    "    \"department\"\n",
    ").filter(col(\"salary\") > 80000)\n",
    "\n",
    "print(\"\\nPlan with broadcast():\")\n",
    "result_broadcast.explain()\n",
    "\n",
    "print(\"\"\"\n",
    "üí° BROADCAST JOIN:\n",
    "   - Best for small tables (< 10MB)\n",
    "   - No shuffle needed\n",
    "   - Much faster\n",
    "   - Use /*+ BROADCAST(table) */ in SQL\n",
    "   - Use broadcast(df) in DataFrame API\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C. Join strategies comparison\n",
    "print(\"\\nüìä C. JOIN STRATEGIES COMPARISON\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "üí° JOIN STRATEGIES:\n",
    "\n",
    "1. BROADCAST HASH JOIN\n",
    "   When: One table < 10MB (default threshold)\n",
    "   How: Broadcast small table to all executors\n",
    "   Pros: No shuffle, very fast\n",
    "   Cons: Limited by broadcast size\n",
    "   \n",
    "2. SORT-MERGE JOIN\n",
    "   When: Both tables large, sorted\n",
    "   How: Sort both sides, merge sorted data\n",
    "   Pros: Good for large tables\n",
    "   Cons: Requires sort (expensive)\n",
    "   \n",
    "3. SHUFFLE HASH JOIN\n",
    "   When: Both tables large, not sorted\n",
    "   How: Hash partition both sides\n",
    "   Pros: No sort needed\n",
    "   Cons: Requires shuffle\n",
    "\n",
    "Spark chooses automatically based on:\n",
    "   - Table sizes\n",
    "   - Statistics\n",
    "   - Sort order\n",
    "   - Available memory\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ **6. QUERY HINTS**\n",
    "\n",
    "### **Available Hints:**\n",
    "- **BROADCAST**: Force broadcast join\n",
    "- **MERGE**: Force sort-merge join\n",
    "- **SHUFFLE_HASH**: Force shuffle hash join\n",
    "- **SHUFFLE_REPLICATE_NL**: Force shuffle replicate nested loop join\n",
    "- **COALESCE**: Reduce number of partitions\n",
    "- **REPARTITION**: Increase number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üéØ 6. QUERY HINTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# A. Broadcast hint\n",
    "print(\"\\nüìä A. BROADCAST HINT\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT /*+ BROADCAST(d) */ *\n",
    "    FROM employees e\n",
    "    JOIN departments d ON e.department = d.department\n",
    "\"\"\").explain()\n",
    "\n",
    "# B. Merge hint\n",
    "print(\"\\nüìä B. MERGE HINT (Sort-Merge Join)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT /*+ MERGE(e, d) */ *\n",
    "    FROM employees e\n",
    "    JOIN departments d ON e.department = d.department\n",
    "\"\"\").explain()\n",
    "\n",
    "# C. Shuffle hash hint\n",
    "print(\"\\nüìä C. SHUFFLE_HASH HINT\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT /*+ SHUFFLE_HASH(e, d) */ *\n",
    "    FROM employees e\n",
    "    JOIN departments d ON e.department = d.department\n",
    "\"\"\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D. Coalesce and Repartition hints\n",
    "print(\"\\nüìä D. COALESCE AND REPARTITION HINTS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Coalesce hint\n",
    "print(\"\\n1. COALESCE hint (reduce partitions):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT /*+ COALESCE(2) */ department, COUNT(*) as count\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "\"\"\").explain()\n",
    "\n",
    "# Repartition hint\n",
    "print(\"\\n2. REPARTITION hint (increase partitions):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT /*+ REPARTITION(10) */ department, COUNT(*) as count\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "\"\"\").explain()\n",
    "\n",
    "# Repartition by column\n",
    "print(\"\\n3. REPARTITION by column:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT /*+ REPARTITION(department) */ *\n",
    "    FROM employees\n",
    "\"\"\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E. Multiple hints\n",
    "print(\"\\nüìä E. MULTIPLE HINTS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT /*+ BROADCAST(d), COALESCE(2) */ \n",
    "        e.department, d.division, COUNT(*) as count\n",
    "    FROM employees e\n",
    "    JOIN departments d ON e.department = d.department\n",
    "    GROUP BY e.department, d.division\n",
    "\"\"\").explain()\n",
    "\n",
    "print(\"\"\"\n",
    "üí° QUERY HINTS:\n",
    "   - Use hints to control execution\n",
    "   - Spark usually chooses well automatically\n",
    "   - Use hints when you know better\n",
    "   - Test with and without hints\n",
    "   - Hints are suggestions, not commands\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ùå **7. COMMON ANTI-PATTERNS**\n",
    "\n",
    "### **What to Avoid:**\n",
    "1. SELECT *\n",
    "2. No filters\n",
    "3. Cross joins\n",
    "4. UDFs instead of built-in functions\n",
    "5. Collecting large datasets\n",
    "6. Not using partitioning\n",
    "7. Skewed data\n",
    "8. Too many small files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"‚ùå 7. COMMON ANTI-PATTERNS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "üí° COMMON ANTI-PATTERNS:\n",
    "\n",
    "1. SELECT * (Column Pruning)\n",
    "   ‚ùå SELECT * FROM large_table\n",
    "   ‚úÖ SELECT col1, col2 FROM large_table\n",
    "   Why: Reads unnecessary data\n",
    "\n",
    "2. No Filters (Predicate Pushdown)\n",
    "   ‚ùå SELECT * FROM table ‚Üí Filter in Python\n",
    "   ‚úÖ SELECT * FROM table WHERE condition\n",
    "   Why: Processes all data\n",
    "\n",
    "3. Cross Joins\n",
    "   ‚ùå SELECT * FROM t1 CROSS JOIN t2\n",
    "   ‚úÖ SELECT * FROM t1 JOIN t2 ON t1.id = t2.id\n",
    "   Why: Cartesian product is huge\n",
    "\n",
    "4. UDFs Instead of Built-in Functions\n",
    "   ‚ùå udf(lambda x: x.upper())\n",
    "   ‚úÖ F.upper(col(\"name\"))\n",
    "   Why: UDFs are slow (Python ‚Üí JVM)\n",
    "\n",
    "5. Collecting Large Datasets\n",
    "   ‚ùå df.collect()  # 1TB data\n",
    "   ‚úÖ df.limit(100).collect()\n",
    "   Why: OOM error\n",
    "\n",
    "6. Not Using Partitioning\n",
    "   ‚ùå df.write.parquet(\"path\")\n",
    "   ‚úÖ df.write.partitionBy(\"date\").parquet(\"path\")\n",
    "   Why: Slow queries\n",
    "\n",
    "7. Skewed Data\n",
    "   ‚ùå One partition has 90% of data\n",
    "   ‚úÖ Use salting or AQE\n",
    "   Why: One task is slow\n",
    "\n",
    "8. Too Many Small Files\n",
    "   ‚ùå 10,000 files of 1KB each\n",
    "   ‚úÖ 100 files of 100KB each\n",
    "   Why: Metadata overhead\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: SELECT * vs SELECT columns\n",
    "print(\"\\nüìä DEMO: SELECT * vs SELECT COLUMNS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Bad: SELECT *\n",
    "print(\"\\n‚ùå Bad: SELECT *\")\n",
    "start = time.time()\n",
    "result_bad = spark.sql(\"SELECT * FROM employees WHERE salary > 80000\")\n",
    "count_bad = result_bad.count()\n",
    "time_bad = time.time() - start\n",
    "print(f\"Time: {time_bad:.3f}s\")\n",
    "\n",
    "# Good: SELECT specific columns\n",
    "print(\"\\n‚úÖ Good: SELECT specific columns\")\n",
    "start = time.time()\n",
    "result_good = spark.sql(\"\"\"\n",
    "    SELECT employee_id, name, department, salary \n",
    "    FROM employees \n",
    "    WHERE salary > 80000\n",
    "\"\"\")\n",
    "count_good = result_good.count()\n",
    "time_good = time.time() - start\n",
    "print(f\"Time: {time_good:.3f}s\")\n",
    "\n",
    "if time_bad > time_good:\n",
    "    print(f\"\\n‚úÖ Selecting columns is {time_bad/time_good:.2f}x faster!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Built-in function vs UDF\n",
    "print(\"\\nüìä DEMO: BUILT-IN FUNCTION vs UDF\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Bad: UDF\n",
    "print(\"\\n‚ùå Bad: UDF\")\n",
    "upper_udf = udf(lambda x: x.upper() if x else None, StringType())\n",
    "\n",
    "start = time.time()\n",
    "result_udf = employees.withColumn(\"name_upper\", upper_udf(col(\"name\")))\n",
    "count_udf = result_udf.count()\n",
    "time_udf = time.time() - start\n",
    "print(f\"Time: {time_udf:.3f}s\")\n",
    "\n",
    "# Good: Built-in function\n",
    "print(\"\\n‚úÖ Good: Built-in function\")\n",
    "start = time.time()\n",
    "result_builtin = employees.withColumn(\"name_upper\", F.upper(col(\"name\")))\n",
    "count_builtin = result_builtin.count()\n",
    "time_builtin = time.time() - start\n",
    "print(f\"Time: {time_builtin:.3f}s\")\n",
    "\n",
    "if time_udf > time_builtin:\n",
    "    print(f\"\\n‚úÖ Built-in function is {time_udf/time_builtin:.2f}x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö° **8. PERFORMANCE TUNING BEST PRACTICES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"‚ö° 8. PERFORMANCE TUNING BEST PRACTICES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "üí° PERFORMANCE TUNING CHECKLIST:\n",
    "\n",
    "1. DATA READING\n",
    "   ‚úÖ Use columnar formats (Parquet, ORC)\n",
    "   ‚úÖ Partition data by frequently filtered columns\n",
    "   ‚úÖ Use predicate pushdown (filter early)\n",
    "   ‚úÖ Select only needed columns\n",
    "   ‚úÖ Use compression (snappy, gzip)\n",
    "\n",
    "2. JOINS\n",
    "   ‚úÖ Broadcast small tables (< 10MB)\n",
    "   ‚úÖ Use appropriate join strategy\n",
    "   ‚úÖ Filter before joining\n",
    "   ‚úÖ Join on partition keys when possible\n",
    "   ‚úÖ Avoid cross joins\n",
    "\n",
    "3. AGGREGATIONS\n",
    "   ‚úÖ Filter before aggregating\n",
    "   ‚úÖ Use partial aggregations\n",
    "   ‚úÖ Consider approximate aggregations (approx_count_distinct)\n",
    "   ‚úÖ Use appropriate number of partitions\n",
    "\n",
    "4. SHUFFLES\n",
    "   ‚úÖ Minimize shuffles\n",
    "   ‚úÖ Use appropriate partition size (128MB-256MB)\n",
    "   ‚úÖ Coalesce after filtering\n",
    "   ‚úÖ Repartition before expensive operations\n",
    "\n",
    "5. CACHING\n",
    "   ‚úÖ Cache frequently accessed data\n",
    "   ‚úÖ Use appropriate storage level\n",
    "   ‚úÖ Unpersist when done\n",
    "   ‚úÖ Monitor cache memory usage\n",
    "\n",
    "6. STATISTICS\n",
    "   ‚úÖ Compute table statistics\n",
    "   ‚úÖ Compute column statistics\n",
    "   ‚úÖ Enable CBO\n",
    "   ‚úÖ Update statistics regularly\n",
    "\n",
    "7. CONFIGURATION\n",
    "   ‚úÖ Enable Adaptive Query Execution (AQE)\n",
    "   ‚úÖ Set appropriate memory\n",
    "   ‚úÖ Configure parallelism\n",
    "   ‚úÖ Tune broadcast threshold\n",
    "\n",
    "8. MONITORING\n",
    "   ‚úÖ Use Spark UI\n",
    "   ‚úÖ Check execution plans\n",
    "   ‚úÖ Monitor stage times\n",
    "   ‚úÖ Identify bottlenecks\n",
    "   ‚úÖ Profile queries\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration recommendations\n",
    "print(\"\\nüìä RECOMMENDED SPARK CONFIGURATIONS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "üí° RECOMMENDED CONFIGURATIONS:\n",
    "\n",
    "# Adaptive Query Execution\n",
    "spark.sql.adaptive.enabled = true\n",
    "spark.sql.adaptive.coalescePartitions.enabled = true\n",
    "spark.sql.adaptive.skewJoin.enabled = true\n",
    "\n",
    "# Cost-Based Optimization\n",
    "spark.sql.cbo.enabled = true\n",
    "spark.sql.statistics.histogram.enabled = true\n",
    "\n",
    "# Broadcast\n",
    "spark.sql.autoBroadcastJoinThreshold = 10485760  # 10MB\n",
    "\n",
    "# Shuffle\n",
    "spark.sql.shuffle.partitions = 200  # Adjust based on data size\n",
    "spark.sql.files.maxPartitionBytes = 134217728  # 128MB\n",
    "\n",
    "# Memory\n",
    "spark.executor.memory = 4g\n",
    "spark.driver.memory = 2g\n",
    "spark.memory.fraction = 0.6\n",
    "\n",
    "# Compression\n",
    "spark.sql.parquet.compression.codec = snappy\n",
    "\n",
    "# Dynamic Allocation\n",
    "spark.dynamicAllocation.enabled = true\n",
    "spark.dynamicAllocation.minExecutors = 1\n",
    "spark.dynamicAllocation.maxExecutors = 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì **KEY TAKEAWAYS**\n",
    "\n",
    "### **‚úÖ What You Learned:**\n",
    "\n",
    "1. **Query Execution Plans**\n",
    "   - Use EXPLAIN to understand queries\n",
    "   - 4 stages: Parsed, Analyzed, Optimized, Physical\n",
    "   - Different explain modes\n",
    "\n",
    "2. **Cost-Based Optimization**\n",
    "   - Compute statistics for better plans\n",
    "   - Table and column statistics\n",
    "   - Enable CBO\n",
    "\n",
    "3. **Predicate Pushdown**\n",
    "   - Filter early\n",
    "   - Select only needed columns\n",
    "   - Spark does this automatically\n",
    "\n",
    "4. **Join Optimization**\n",
    "   - Broadcast small tables\n",
    "   - Choose appropriate join strategy\n",
    "   - Use hints when needed\n",
    "\n",
    "5. **Query Hints**\n",
    "   - BROADCAST, MERGE, SHUFFLE_HASH\n",
    "   - COALESCE, REPARTITION\n",
    "   - Use sparingly\n",
    "\n",
    "6. **Anti-Patterns**\n",
    "   - Avoid SELECT *\n",
    "   - Filter early\n",
    "   - Use built-in functions\n",
    "   - Avoid cross joins\n",
    "\n",
    "7. **Performance Tuning**\n",
    "   - Enable AQE and CBO\n",
    "   - Compute statistics\n",
    "   - Monitor with Spark UI\n",
    "   - Tune configurations\n",
    "\n",
    "### **üìä Quick Reference:**\n",
    "\n",
    "```python\n",
    "# Explain query\n",
    "df.explain()\n",
    "df.explain(extended=True)\n",
    "df.explain(mode=\"formatted\")\n",
    "df.explain(mode=\"cost\")\n",
    "\n",
    "# Compute statistics\n",
    "spark.sql(\"ANALYZE TABLE table_name COMPUTE STATISTICS\")\n",
    "spark.sql(\"ANALYZE TABLE table_name COMPUTE STATISTICS FOR COLUMNS col1, col2\")\n",
    "\n",
    "# Broadcast join\n",
    "df1.join(broadcast(df2), \"key\")\n",
    "spark.sql(\"SELECT /*+ BROADCAST(t2) */ * FROM t1 JOIN t2 ON t1.id = t2.id\")\n",
    "\n",
    "# Query hints\n",
    "/*+ BROADCAST(table) */\n",
    "/*+ MERGE(t1, t2) */\n",
    "/*+ SHUFFLE_HASH(t1, t2) */\n",
    "/*+ COALESCE(n) */\n",
    "/*+ REPARTITION(n) */\n",
    "```\n",
    "\n",
    "### **üöÄ Congratulations!**\n",
    "\n",
    "You've completed **DAY 5: SPARK SQL**!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.catalog.clearCache()\n",
    "spark.stop()\n",
    "\n",
    "print(\"‚úÖ Spark session stopped\")\n",
    "print(\"\\nüéâ DAY 5 - LESSON 4 COMPLETED!\")\n",
    "print(\"\\nüí° Remember:\")\n",
    "print(\"   - Always use EXPLAIN to understand queries\")\n",
    "print(\"   - Compute statistics for better optimization\")\n",
    "print(\"   - Broadcast small tables\")\n",
    "print(\"   - Filter early, select only needed columns\")\n",
    "print(\"   - Avoid anti-patterns\")\n",
    "print(\"   - Monitor with Spark UI\")\n",
    "print(\"\\nüî• Quote: 'Premature optimization is the root of all evil, but knowing how to optimize is essential!' ‚ö°\")\n",
    "print(\"\\nüéä CONGRATULATIONS! You've completed DAY 5: SPARK SQL! üéä\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
