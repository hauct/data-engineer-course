{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ—„ï¸ CATALOG & METADATA\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ **DAY 5 - LESSON 3: CATALOG & METADATA**\n",
    "\n",
    "### **ðŸŽ¯ Má»¤C TIÃŠU:**\n",
    "\n",
    "1. **Spark Catalog API** - Explore metadata\n",
    "2. **Database Management** - Create, drop, use databases\n",
    "3. **Table Management** - Create, alter, drop tables\n",
    "4. **Schema Operations** - Inspect and modify schemas\n",
    "5. **Metadata Queries** - Query table/column metadata\n",
    "6. **Best Practices** - Metadata management\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¡ **CATALOG & METADATA:**\n",
    "\n",
    "- Catalog = Metadata store (databases, tables, columns)\n",
    "- Spark Catalog API = Python interface to metadata\n",
    "- Metastore = Persistent metadata storage\n",
    "- Important for data governance and discovery\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ **SETUP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/11 16:00:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark Session Created\n",
      "Spark Version: 3.5.1\n",
      "Hive Support: Enabled\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, lit, when, desc, asc\n",
    "from pyspark.sql.types import *\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CatalogMetadata\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"âœ… Spark Session Created\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Hive Support: Enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Š **1. SPARK CATALOG API OVERVIEW**\n",
    "\n",
    "### **What is Spark Catalog?**\n",
    "- Interface to Spark's metadata\n",
    "- Access via `spark.catalog`\n",
    "- Query databases, tables, columns, functions\n",
    "- Manage cache and temporary views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ“Š 1. SPARK CATALOG API OVERVIEW\n",
      "================================================================================\n",
      "\n",
      "ðŸ’¡ Spark Catalog API Methods:\n",
      "--------------------------------------------------------------------------------\n",
      "   currentDatabase()              - Get current database\n",
      "   setCurrentDatabase(name)       - Set current database\n",
      "   listDatabases()                - List all databases\n",
      "   listTables()                   - List tables in current database\n",
      "   listColumns(tableName)         - List columns in table\n",
      "   listFunctions()                - List available functions\n",
      "   tableExists(tableName)         - Check if table exists\n",
      "   databaseExists(dbName)         - Check if database exists\n",
      "   createTable()                  - Create table from DataFrame\n",
      "   dropTempView(viewName)         - Drop temporary view\n",
      "   isCached(tableName)            - Check if table is cached\n",
      "   cacheTable(tableName)          - Cache table\n",
      "   uncacheTable(tableName)        - Uncache table\n",
      "   clearCache()                   - Clear all cached tables\n",
      "   refreshTable(tableName)        - Refresh table metadata\n",
      "\n",
      "ðŸ”¹ Current Database:\n",
      "   default\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ“Š 1. SPARK CATALOG API OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ’¡ Spark Catalog API Methods:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "catalog_methods = [\n",
    "    (\"currentDatabase()\", \"Get current database\"),\n",
    "    (\"setCurrentDatabase(name)\", \"Set current database\"),\n",
    "    (\"listDatabases()\", \"List all databases\"),\n",
    "    (\"listTables()\", \"List tables in current database\"),\n",
    "    (\"listColumns(tableName)\", \"List columns in table\"),\n",
    "    (\"listFunctions()\", \"List available functions\"),\n",
    "    (\"tableExists(tableName)\", \"Check if table exists\"),\n",
    "    (\"databaseExists(dbName)\", \"Check if database exists\"),\n",
    "    (\"createTable()\", \"Create table from DataFrame\"),\n",
    "    (\"dropTempView(viewName)\", \"Drop temporary view\"),\n",
    "    (\"isCached(tableName)\", \"Check if table is cached\"),\n",
    "    (\"cacheTable(tableName)\", \"Cache table\"),\n",
    "    (\"uncacheTable(tableName)\", \"Uncache table\"),\n",
    "    (\"clearCache()\", \"Clear all cached tables\"),\n",
    "    (\"refreshTable(tableName)\", \"Refresh table metadata\")\n",
    "]\n",
    "\n",
    "for method, description in catalog_methods:\n",
    "    print(f\"   {method:30s} - {description}\")\n",
    "\n",
    "print(\"\\nðŸ”¹ Current Database:\")\n",
    "print(f\"   {spark.catalog.currentDatabase()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ—„ï¸ **2. DATABASE MANAGEMENT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ—„ï¸ 2. DATABASE MANAGEMENT\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š A. LIST ALL DATABASES\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 16:00:49 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "26/01/11 16:00:49 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "26/01/11 16:00:53 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "26/01/11 16:00:53 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.18.0.9\n",
      "26/01/11 16:00:53 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 1 database(s):\n",
      "\n",
      "   ðŸ“ default\n",
      "      Description: Default Hive database\n",
      "      Location: file:/opt/spark-notebooks/day05/spark-warehouse\n",
      "\n",
      "Using SQL:\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ—„ï¸ 2. DATABASE MANAGEMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# A. List all databases\n",
    "print(\"\\nðŸ“Š A. LIST ALL DATABASES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "databases = spark.catalog.listDatabases()\n",
    "print(f\"\\nFound {len(databases)} database(s):\\n\")\n",
    "for db in databases:\n",
    "    print(f\"   ðŸ“ {db.name}\")\n",
    "    print(f\"      Description: {db.description}\")\n",
    "    print(f\"      Location: {db.locationUri}\")\n",
    "    print()\n",
    "\n",
    "# Using SQL\n",
    "print(\"Using SQL:\")\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š B. CREATE DATABASES\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 16:00:58 WARN ObjectStore: Failed to get database hr_database, returning NoSuchObjectException\n",
      "26/01/11 16:00:58 WARN ObjectStore: Failed to get database hr_database, returning NoSuchObjectException\n",
      "26/01/11 16:00:58 WARN ObjectStore: Failed to get database hr_database, returning NoSuchObjectException\n",
      "26/01/11 16:00:58 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "26/01/11 16:00:58 WARN ObjectStore: Failed to get database hr_database, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created database: hr_database\n",
      "âœ… Created database: sales_database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 16:00:58 WARN ObjectStore: Failed to get database sales_database, returning NoSuchObjectException\n",
      "26/01/11 16:00:58 WARN ObjectStore: Failed to get database sales_database, returning NoSuchObjectException\n",
      "26/01/11 16:00:58 WARN ObjectStore: Failed to get database sales_database, returning NoSuchObjectException\n",
      "26/01/11 16:00:58 WARN ObjectStore: Failed to get database sales_database, returning NoSuchObjectException\n",
      "26/01/11 16:00:58 WARN ObjectStore: Failed to get database analytics_database, returning NoSuchObjectException\n",
      "26/01/11 16:00:58 WARN ObjectStore: Failed to get database analytics_database, returning NoSuchObjectException\n",
      "26/01/11 16:00:58 WARN ObjectStore: Failed to get database analytics_database, returning NoSuchObjectException\n",
      "26/01/11 16:00:58 WARN ObjectStore: Failed to get database analytics_database, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created database: analytics_database\n",
      "\n",
      "ðŸ“‹ All databases:\n",
      "+------------------+\n",
      "|         namespace|\n",
      "+------------------+\n",
      "|analytics_database|\n",
      "|           default|\n",
      "|       hr_database|\n",
      "|    sales_database|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# B. Create databases\n",
    "print(\"\\nðŸ“Š B. CREATE DATABASES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Create test databases\n",
    "databases_to_create = [\n",
    "    (\"hr_database\", \"Human Resources data\"),\n",
    "    (\"sales_database\", \"Sales and revenue data\"),\n",
    "    (\"analytics_database\", \"Analytics and reporting data\")\n",
    "]\n",
    "\n",
    "for db_name, description in databases_to_create:\n",
    "    # Drop if exists\n",
    "    spark.sql(f\"DROP DATABASE IF EXISTS {db_name} CASCADE\")\n",
    "    \n",
    "    # Create database\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE DATABASE IF NOT EXISTS {db_name}\n",
    "        COMMENT '{description}'\n",
    "    \"\"\")\n",
    "    print(f\"âœ… Created database: {db_name}\")\n",
    "\n",
    "print(\"\\nðŸ“‹ All databases:\")\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š C. SWITCH DATABASE\n",
      "--------------------------------------------------------------------------------\n",
      "Current database: default\n",
      "Switched to: hr_database\n",
      "Switched to: sales_database\n",
      "Switched to: default\n"
     ]
    }
   ],
   "source": [
    "# C. Switch database\n",
    "print(\"\\nðŸ“Š C. SWITCH DATABASE\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"Current database: {spark.catalog.currentDatabase()}\")\n",
    "\n",
    "# Switch to hr_database\n",
    "spark.catalog.setCurrentDatabase(\"hr_database\")\n",
    "print(f\"Switched to: {spark.catalog.currentDatabase()}\")\n",
    "\n",
    "# Using SQL\n",
    "spark.sql(\"USE sales_database\")\n",
    "print(f\"Switched to: {spark.catalog.currentDatabase()}\")\n",
    "\n",
    "# Switch back to default\n",
    "spark.catalog.setCurrentDatabase(\"default\")\n",
    "print(f\"Switched to: {spark.catalog.currentDatabase()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š D. DATABASE PROPERTIES\n",
      "--------------------------------------------------------------------------------\n",
      "+--------------+--------------------------------------------------------------+\n",
      "|info_name     |info_value                                                    |\n",
      "+--------------+--------------------------------------------------------------+\n",
      "|Catalog Name  |spark_catalog                                                 |\n",
      "|Namespace Name|hr_database                                                   |\n",
      "|Comment       |Human Resources data                                          |\n",
      "|Location      |file:/opt/spark-notebooks/day05/spark-warehouse/hr_database.db|\n",
      "|Owner         |spark                                                         |\n",
      "+--------------+--------------------------------------------------------------+\n",
      "\n",
      "+--------------+--------------------------------------------------------------+\n",
      "|info_name     |info_value                                                    |\n",
      "+--------------+--------------------------------------------------------------+\n",
      "|Catalog Name  |spark_catalog                                                 |\n",
      "|Namespace Name|hr_database                                                   |\n",
      "|Comment       |Human Resources data                                          |\n",
      "|Location      |file:/opt/spark-notebooks/day05/spark-warehouse/hr_database.db|\n",
      "|Owner         |spark                                                         |\n",
      "|Properties    |                                                              |\n",
      "+--------------+--------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# D. Database properties\n",
    "print(\"\\nðŸ“Š D. DATABASE PROPERTIES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Describe database\n",
    "spark.sql(\"DESCRIBE DATABASE hr_database\").show(truncate=False)\n",
    "\n",
    "# Extended info\n",
    "spark.sql(\"DESCRIBE DATABASE EXTENDED hr_database\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š E. CHECK IF DATABASE EXISTS\n",
      "--------------------------------------------------------------------------------\n",
      "   hr_database          - âœ… EXISTS\n",
      "   sales_database       - âœ… EXISTS\n",
      "   nonexistent_db       - âŒ NOT FOUND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 16:00:59 WARN ObjectStore: Failed to get database nonexistent_db, returning NoSuchObjectException\n",
      "26/01/11 16:00:59 WARN ObjectStore: Failed to get database nonexistent_db, returning NoSuchObjectException\n"
     ]
    }
   ],
   "source": [
    "# E. Check if database exists\n",
    "print(\"\\nðŸ“Š E. CHECK IF DATABASE EXISTS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "databases_to_check = [\"hr_database\", \"sales_database\", \"nonexistent_db\"]\n",
    "\n",
    "for db_name in databases_to_check:\n",
    "    exists = spark.catalog.databaseExists(db_name)\n",
    "    status = \"âœ… EXISTS\" if exists else \"âŒ NOT FOUND\"\n",
    "    print(f\"   {db_name:20s} - {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“‹ **3. TABLE MANAGEMENT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ“‹ 3. TABLE MANAGEMENT\n",
      "================================================================================\n",
      "\n",
      "ðŸ”¹ Creating sample data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created DataFrame with 100 rows\n",
      "+-----------+----------+---+----------+------+--------+----------+\n",
      "|employee_id|      name|age|department|salary|  status| hire_date|\n",
      "+-----------+----------+---+----------+------+--------+----------+\n",
      "|    EMP0001|Employee 1| 33|     Sales|112953|  Active|2021-08-17|\n",
      "|    EMP0002|Employee 2| 43|        HR|103986|Inactive|2020-12-22|\n",
      "|    EMP0003|Employee 3| 33|     Sales| 84113|Inactive|2021-08-13|\n",
      "|    EMP0004|Employee 4| 48|   Finance|115490|  Active|2020-01-08|\n",
      "|    EMP0005|Employee 5| 59|        HR| 82896|Inactive|2022-05-08|\n",
      "+-----------+----------+---+----------+------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ“‹ 3. TABLE MANAGEMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create sample data\n",
    "print(\"\\nðŸ”¹ Creating sample data...\")\n",
    "\n",
    "employees_data = []\n",
    "for i in range(1, 101):\n",
    "    employees_data.append((\n",
    "        f\"EMP{i:04d}\",\n",
    "        f\"Employee {i}\",\n",
    "        random.randint(22, 60),\n",
    "        random.choice([\"Engineering\", \"Sales\", \"Marketing\", \"HR\", \"Finance\"]),\n",
    "        random.randint(50000, 120000),\n",
    "        random.choice([\"Active\", \"Inactive\"]),\n",
    "        (datetime(2020, 1, 1) + timedelta(days=random.randint(0, 1460))).strftime(\"%Y-%m-%d\")\n",
    "    ))\n",
    "\n",
    "employees = spark.createDataFrame(employees_data,\n",
    "    [\"employee_id\", \"name\", \"age\", \"department\", \"salary\", \"status\", \"hire_date\"])\n",
    "\n",
    "print(f\"âœ… Created DataFrame with {employees.count()} rows\")\n",
    "employees.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š A. CREATE TABLES\n",
      "--------------------------------------------------------------------------------\n",
      "âœ… Created temporary view: employees_temp\n",
      "âœ… Created global temporary view: employees_global\n",
      "   Access via: global_temp.employees_global\n"
     ]
    }
   ],
   "source": [
    "# A. Create tables\n",
    "print(\"\\nðŸ“Š A. CREATE TABLES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Method 1: Create temporary view\n",
    "employees.createOrReplaceTempView(\"employees_temp\")\n",
    "print(\"âœ… Created temporary view: employees_temp\")\n",
    "\n",
    "# Method 2: Create global temporary view\n",
    "employees.createOrReplaceGlobalTempView(\"employees_global\")\n",
    "print(\"âœ… Created global temporary view: employees_global\")\n",
    "print(\"   Access via: global_temp.employees_global\")\n",
    "\n",
    "# # Method 3: Save as table (persistent)\n",
    "# spark.sql(\"USE hr_database\")\n",
    "# employees.write.mode(\"overwrite\").saveAsTable(\"employees_persistent\")\n",
    "# print(\"âœ… Created persistent table: hr_database.employees_persistent\")\n",
    "\n",
    "# # Method 4: Create table using SQL\n",
    "# spark.sql(\"\"\"\n",
    "#     CREATE TABLE IF NOT EXISTS hr_database.employees_sql (\n",
    "#         employee_id STRING,\n",
    "#         name STRING,\n",
    "#         age INT,\n",
    "#         department STRING,\n",
    "#         salary INT,\n",
    "#         status STRING,\n",
    "#         hire_date STRING\n",
    "#     )\n",
    "#     USING parquet\n",
    "# \"\"\")\n",
    "# print(\"âœ… Created table using SQL: hr_database.employees_sql\")\n",
    "\n",
    "# # Insert data\n",
    "# spark.sql(\"\"\"\n",
    "#     INSERT INTO hr_database.employees_sql\n",
    "#     SELECT * FROM employees_temp\n",
    "# \"\"\")\n",
    "# print(\"âœ… Inserted data into employees_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š B. LIST TABLES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Tables in hr_database:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“„ employees_temp\n",
      "      Database: None\n",
      "      Type: TEMPORARY\n",
      "      Is Temporary: True\n",
      "\n",
      "Using SQL:\n",
      "+---------+--------------+-----------+\n",
      "|namespace|     tableName|isTemporary|\n",
      "+---------+--------------+-----------+\n",
      "|         |employees_temp|       true|\n",
      "+---------+--------------+-----------+\n",
      "\n",
      "\n",
      "All tables (including temp views):\n",
      "   employees_temp                 - TEMPORARY       - Temp: True\n"
     ]
    }
   ],
   "source": [
    "# B. List tables\n",
    "print(\"\\nðŸ“Š B. LIST TABLES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# List tables in current database\n",
    "print(\"\\nTables in hr_database:\")\n",
    "tables = spark.catalog.listTables(\"hr_database\")\n",
    "for table in tables:\n",
    "    print(f\"\\n   ðŸ“„ {table.name}\")\n",
    "    print(f\"      Database: {table.database}\")\n",
    "    print(f\"      Type: {table.tableType}\")\n",
    "    print(f\"      Is Temporary: {table.isTemporary}\")\n",
    "\n",
    "# Using SQL\n",
    "print(\"\\nUsing SQL:\")\n",
    "spark.sql(\"SHOW TABLES IN hr_database\").show()\n",
    "\n",
    "# List all tables (including temp views)\n",
    "print(\"\\nAll tables (including temp views):\")\n",
    "all_tables = spark.catalog.listTables()\n",
    "for table in all_tables:\n",
    "    print(f\"   {table.name:30s} - {table.tableType:15s} - Temp: {table.isTemporary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š C. CHECK IF TABLE EXISTS\n",
      "--------------------------------------------------------------------------------\n",
      "   employees_temp                      - âœ… EXISTS\n",
      "   employees_persistent                - âŒ NOT FOUND\n",
      "   hr_database.employees_sql           - âŒ NOT FOUND\n",
      "   nonexistent_table                   - âŒ NOT FOUND\n"
     ]
    }
   ],
   "source": [
    "# C. Check if table exists\n",
    "print(\"\\nðŸ“Š C. CHECK IF TABLE EXISTS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "tables_to_check = [\n",
    "    \"employees_temp\",\n",
    "    \"employees_persistent\",\n",
    "    \"hr_database.employees_sql\",\n",
    "    \"nonexistent_table\"\n",
    "]\n",
    "\n",
    "for table_name in tables_to_check:\n",
    "    exists = spark.catalog.tableExists(table_name)\n",
    "    status = \"âœ… EXISTS\" if exists else \"âŒ NOT FOUND\"\n",
    "    print(f\"   {table_name:35s} - {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D. Table properties\n",
    "print(\"\\nðŸ“Š D. TABLE PROPERTIES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Describe table\n",
    "print(\"\\nDESCRIBE TABLE:\")\n",
    "spark.sql(\"DESCRIBE TABLE hr_database.employees_persistent\").show(truncate=False)\n",
    "\n",
    "# Extended info\n",
    "print(\"\\nDESCRIBE TABLE EXTENDED:\")\n",
    "spark.sql(\"DESCRIBE TABLE EXTENDED hr_database.employees_persistent\").show(truncate=False)\n",
    "\n",
    "# Formatted\n",
    "print(\"\\nDESCRIBE TABLE FORMATTED:\")\n",
    "spark.sql(\"DESCRIBE FORMATTED hr_database.employees_persistent\").show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š E. ALTER TABLE\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `hr_database`.`employees_sql` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 2 pos 16;\n'AddColumns [QualifiedColType(None,email,StringType,true,Some(Employee email),None,None)]\n+- 'UnresolvedTable [hr_database, employees_sql], ALTER TABLE ... ADD COLUMNS\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Add column\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43m    ALTER TABLE hr_database.employees_sql\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43m    ADD COLUMNS (email STRING COMMENT \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEmployee email\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Added column: email\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Rename table\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hr_database`.`employees_sql` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 2 pos 16;\n'AddColumns [QualifiedColType(None,email,StringType,true,Some(Employee email),None,None)]\n+- 'UnresolvedTable [hr_database, employees_sql], ALTER TABLE ... ADD COLUMNS\n"
     ]
    }
   ],
   "source": [
    "# E. Alter table\n",
    "print(\"\\nðŸ“Š E. ALTER TABLE\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Add column\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE hr_database.employees_sql\n",
    "    ADD COLUMNS (email STRING COMMENT 'Employee email')\n",
    "\"\"\")\n",
    "print(\"âœ… Added column: email\")\n",
    "\n",
    "# Rename table\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE hr_database.employees_sql\n",
    "    RENAME TO hr_database.employees_renamed\n",
    "\"\"\")\n",
    "print(\"âœ… Renamed table: employees_sql â†’ employees_renamed\")\n",
    "\n",
    "# Set table properties\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE hr_database.employees_renamed\n",
    "    SET TBLPROPERTIES ('comment' = 'Employee master data')\n",
    "\"\"\")\n",
    "print(\"âœ… Set table properties\")\n",
    "\n",
    "# Verify changes\n",
    "print(\"\\nVerify changes:\")\n",
    "spark.sql(\"DESCRIBE TABLE hr_database.employees_renamed\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F. Drop tables\n",
    "print(\"\\nðŸ“Š F. DROP TABLES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Drop temporary view\n",
    "spark.catalog.dropTempView(\"employees_temp\")\n",
    "print(\"âœ… Dropped temporary view: employees_temp\")\n",
    "\n",
    "# Drop global temporary view\n",
    "spark.catalog.dropGlobalTempView(\"employees_global\")\n",
    "print(\"âœ… Dropped global temporary view: employees_global\")\n",
    "\n",
    "# Drop table using SQL\n",
    "spark.sql(\"DROP TABLE IF EXISTS hr_database.employees_renamed\")\n",
    "print(\"âœ… Dropped table: hr_database.employees_renamed\")\n",
    "\n",
    "print(\"\\nðŸ“‹ Remaining tables:\")\n",
    "spark.sql(\"SHOW TABLES IN hr_database\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ” **4. SCHEMA OPERATIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ” 4. SCHEMA OPERATIONS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š A. LIST COLUMNS\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `hr_database`.`employees_persistent` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedTableOrView [hr_database, employees_persistent], Catalog.listColumns, true\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ“Š A. LIST COLUMNS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m columns \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcatalog\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistColumns\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhr_database.employees_persistent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTable: hr_database.employees_persistent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/catalog.py:649\u001b[0m, in \u001b[0;36mCatalog.listColumns\u001b[0;34m(self, tableName, dbName)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of columns for the given table/view in the specified database.\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \n\u001b[1;32m    614\u001b[0m \u001b[38;5;124;03m.. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;124;03m>>> _ = spark.sql(\"DROP TABLE tblA\")\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dbName \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcatalog\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistColumns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtableName\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoLocalIterator()\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    651\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    652\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`dbName` has been deprecated since Spark 3.4 and might be removed in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    653\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma future version. Use listColumns(`dbName.tableName`) instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    654\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    655\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hr_database`.`employees_persistent` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedTableOrView [hr_database, employees_persistent], Catalog.listColumns, true\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ” 4. SCHEMA OPERATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# A. List columns\n",
    "print(\"\\nðŸ“Š A. LIST COLUMNS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "columns = spark.catalog.listColumns(\"hr_database.employees_persistent\")\n",
    "print(f\"\\nTable: hr_database.employees_persistent\")\n",
    "print(f\"Columns: {len(columns)}\\n\")\n",
    "\n",
    "for col in columns:\n",
    "    print(f\"   ðŸ“Œ {col.name}\")\n",
    "    print(f\"      Type: {col.dataType}\")\n",
    "    print(f\"      Nullable: {col.nullable}\")\n",
    "    print(f\"      Description: {col.description}\")\n",
    "    print(f\"      Is Partition: {col.isPartition}\")\n",
    "    print(f\"      Is Bucket: {col.isBucket}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š B. GET SCHEMA\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `hr_database`.`employees_persistent` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedRelation [hr_database, employees_persistent], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ“Š B. GET SCHEMA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhr_database.employees_persistent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMethod 1: printSchema()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m df\u001b[38;5;241m.\u001b[39mprintSchema()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/session.py:1667\u001b[0m, in \u001b[0;36mSparkSession.table\u001b[0;34m(self, tableName)\u001b[0m\n\u001b[1;32m   1636\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtable\u001b[39m(\u001b[38;5;28mself\u001b[39m, tableName: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   1637\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the specified table as a :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1638\u001b[0m \n\u001b[1;32m   1639\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;124;03m    +---+\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtableName\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hr_database`.`employees_persistent` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedRelation [hr_database, employees_persistent], [], false\n"
     ]
    }
   ],
   "source": [
    "# B. Get schema\n",
    "print(\"\\nðŸ“Š B. GET SCHEMA\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "df = spark.table(\"hr_database.employees_persistent\")\n",
    "\n",
    "print(\"\\nMethod 1: printSchema()\")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"\\nMethod 2: schema\")\n",
    "print(df.schema)\n",
    "\n",
    "print(\"\\nMethod 3: dtypes\")\n",
    "for col_name, col_type in df.dtypes:\n",
    "    print(f\"   {col_name:20s} - {col_type}\")\n",
    "\n",
    "print(\"\\nMethod 4: columns\")\n",
    "print(f\"   Columns: {df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C. Schema as JSON\n",
    "print(\"\\nðŸ“Š C. SCHEMA AS JSON\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "schema_json = df.schema.json()\n",
    "print(\"\\nSchema JSON:\")\n",
    "print(schema_json)\n",
    "\n",
    "# Parse back to schema\n",
    "from pyspark.sql.types import StructType\n",
    "parsed_schema = StructType.fromJson(eval(schema_json))\n",
    "print(\"\\nâœ… Schema can be serialized and deserialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D. Schema evolution example\n",
    "print(\"\\nðŸ“Š D. SCHEMA EVOLUTION EXAMPLE\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Original schema\n",
    "print(\"\\n1. Original schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "# Add new column\n",
    "df_v2 = df.withColumn(\"bonus\", (col(\"salary\") * 0.1).cast(\"int\"))\n",
    "print(\"\\n2. After adding 'bonus' column:\")\n",
    "df_v2.printSchema()\n",
    "\n",
    "# Change column type\n",
    "df_v3 = df_v2.withColumn(\"age\", col(\"age\").cast(\"string\"))\n",
    "print(\"\\n3. After changing 'age' type to string:\")\n",
    "df_v3.printSchema()\n",
    "\n",
    "# Drop column\n",
    "df_v4 = df_v3.drop(\"status\")\n",
    "print(\"\\n4. After dropping 'status' column:\")\n",
    "df_v4.printSchema()\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’¡ Schema Evolution:\n",
    "   - Add columns: withColumn()\n",
    "   - Change types: cast()\n",
    "   - Drop columns: drop()\n",
    "   - Rename columns: withColumnRenamed()\n",
    "   \n",
    "   âš ï¸  Be careful with schema changes in production!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Š **5. METADATA QUERIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ“Š 5. METADATA QUERIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# A. Table statistics\n",
    "print(\"\\nðŸ“Š A. TABLE STATISTICS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Analyze table\n",
    "spark.sql(\"ANALYZE TABLE hr_database.employees_persistent COMPUTE STATISTICS\")\n",
    "print(\"âœ… Computed table statistics\")\n",
    "\n",
    "# Show statistics\n",
    "spark.sql(\"DESCRIBE EXTENDED hr_database.employees_persistent\").show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B. Column statistics\n",
    "print(\"\\nðŸ“Š B. COLUMN STATISTICS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Analyze columns\n",
    "spark.sql(\"\"\"\n",
    "    ANALYZE TABLE hr_database.employees_persistent\n",
    "    COMPUTE STATISTICS FOR COLUMNS salary, age, department\n",
    "\"\"\")\n",
    "print(\"âœ… Computed column statistics\")\n",
    "\n",
    "# Show column stats\n",
    "spark.sql(\"DESCRIBE EXTENDED hr_database.employees_persistent salary\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C. List functions\n",
    "print(\"\\nðŸ“Š C. LIST FUNCTIONS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "functions = spark.catalog.listFunctions()\n",
    "print(f\"\\nTotal functions: {len(functions)}\")\n",
    "\n",
    "# Show first 20\n",
    "print(\"\\nFirst 20 functions:\")\n",
    "for func in functions[:20]:\n",
    "    print(f\"   {func.name:30s} - {func.description}\")\n",
    "\n",
    "# Filter by name\n",
    "print(\"\\nString functions (contains 'string'):\")\n",
    "string_funcs = [f for f in functions if 'string' in f.name.lower()]\n",
    "for func in string_funcs[:10]:\n",
    "    print(f\"   {func.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D. Function details\n",
    "print(\"\\nðŸ“Š D. FUNCTION DETAILS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Describe function\n",
    "functions_to_describe = ['avg', 'concat', 'date_format', 'explode']\n",
    "\n",
    "for func_name in functions_to_describe:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Function: {func_name}\")\n",
    "    print('='*80)\n",
    "    spark.sql(f\"DESCRIBE FUNCTION {func_name}\").show(truncate=False)\n",
    "    \n",
    "    print(f\"\\nExtended info:\")\n",
    "    spark.sql(f\"DESCRIBE FUNCTION EXTENDED {func_name}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E. Custom metadata query\n",
    "print(\"\\nðŸ“Š E. CUSTOM METADATA QUERY\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Create metadata summary\n",
    "metadata_summary = []\n",
    "\n",
    "for db in spark.catalog.listDatabases():\n",
    "    tables = spark.catalog.listTables(db.name)\n",
    "    for table in tables:\n",
    "        if not table.isTemporary:\n",
    "            columns = spark.catalog.listColumns(f\"{db.name}.{table.name}\")\n",
    "            metadata_summary.append((\n",
    "                db.name,\n",
    "                table.name,\n",
    "                table.tableType,\n",
    "                len(columns)\n",
    "            ))\n",
    "\n",
    "metadata_df = spark.createDataFrame(metadata_summary,\n",
    "    [\"database\", \"table\", \"type\", \"column_count\"])\n",
    "\n",
    "print(\"\\nðŸ“‹ Metadata Summary:\")\n",
    "metadata_df.show(truncate=False)\n",
    "\n",
    "print(\"\\nðŸ“Š Statistics:\")\n",
    "metadata_df.groupBy(\"database\").agg(\n",
    "    F.count(\"table\").alias(\"table_count\"),\n",
    "    F.sum(\"column_count\").alias(\"total_columns\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ’¾ **6. CACHE MANAGEMENT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ’¾ 6. CACHE MANAGEMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# A. Cache table\n",
    "print(\"\\nðŸ“Š A. CACHE TABLE\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "table_name = \"hr_database.employees_persistent\"\n",
    "\n",
    "# Check if cached\n",
    "print(f\"Is cached before: {spark.catalog.isCached(table_name)}\")\n",
    "\n",
    "# Cache table\n",
    "spark.catalog.cacheTable(table_name)\n",
    "print(f\"âœ… Cached table: {table_name}\")\n",
    "\n",
    "# Check again\n",
    "print(f\"Is cached after: {spark.catalog.isCached(table_name)}\")\n",
    "\n",
    "# Using SQL\n",
    "spark.sql(f\"CACHE TABLE {table_name}\")\n",
    "print(f\"âœ… Cached using SQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B. Uncache table\n",
    "print(\"\\nðŸ“Š B. UNCACHE TABLE\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Uncache\n",
    "spark.catalog.uncacheTable(table_name)\n",
    "print(f\"âœ… Uncached table: {table_name}\")\n",
    "print(f\"Is cached: {spark.catalog.isCached(table_name)}\")\n",
    "\n",
    "# Using SQL\n",
    "spark.sql(f\"UNCACHE TABLE {table_name}\")\n",
    "print(f\"âœ… Uncached using SQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C. Clear all cache\n",
    "print(\"\\nðŸ“Š C. CLEAR ALL CACHE\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Cache multiple tables\n",
    "spark.catalog.cacheTable(table_name)\n",
    "print(f\"âœ… Cached: {table_name}\")\n",
    "\n",
    "# Clear all\n",
    "spark.catalog.clearCache()\n",
    "print(\"âœ… Cleared all cache\")\n",
    "print(f\"Is cached: {spark.catalog.isCached(table_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D. Refresh table\n",
    "print(\"\\nðŸ“Š D. REFRESH TABLE\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Refresh metadata\n",
    "spark.catalog.refreshTable(table_name)\n",
    "print(f\"âœ… Refreshed table: {table_name}\")\n",
    "\n",
    "# Using SQL\n",
    "spark.sql(f\"REFRESH TABLE {table_name}\")\n",
    "print(f\"âœ… Refreshed using SQL\")\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’¡ When to refresh:\n",
    "   - After external changes to data files\n",
    "   - After partition changes\n",
    "   - To update metadata cache\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ **7. BEST PRACTICES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸŽ¯ 7. BEST PRACTICES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’¡ CATALOG & METADATA BEST PRACTICES:\n",
    "\n",
    "1. DATABASE ORGANIZATION\n",
    "   âœ… Use separate databases for different domains\n",
    "   âœ… Name databases clearly (e.g., hr_prod, sales_staging)\n",
    "   âœ… Add descriptions to databases\n",
    "   âœ… Use consistent naming conventions\n",
    "\n",
    "2. TABLE MANAGEMENT\n",
    "   âœ… Use meaningful table names\n",
    "   âœ… Add comments to tables and columns\n",
    "   âœ… Document schema changes\n",
    "   âœ… Use temporary views for intermediate results\n",
    "   âœ… Drop temporary views when done\n",
    "\n",
    "3. SCHEMA MANAGEMENT\n",
    "   âœ… Define explicit schemas (don't rely on inference)\n",
    "   âœ… Use appropriate data types\n",
    "   âœ… Document column meanings\n",
    "   âœ… Plan for schema evolution\n",
    "   âœ… Version your schemas\n",
    "\n",
    "4. METADATA QUERIES\n",
    "   âœ… Compute statistics regularly\n",
    "   âœ… Use ANALYZE TABLE for better query planning\n",
    "   âœ… Monitor table sizes\n",
    "   âœ… Track schema changes\n",
    "\n",
    "5. CACHE MANAGEMENT\n",
    "   âœ… Cache frequently accessed tables\n",
    "   âœ… Uncache when done\n",
    "   âœ… Monitor cache memory usage\n",
    "   âœ… Use LAZY caching for large tables\n",
    "   âœ… Clear cache periodically\n",
    "\n",
    "6. PERFORMANCE\n",
    "   âœ… Use partitioning for large tables\n",
    "   âœ… Compute statistics for cost-based optimization\n",
    "   âœ… Use appropriate file formats (Parquet, ORC)\n",
    "   âœ… Refresh metadata after external changes\n",
    "\n",
    "7. GOVERNANCE\n",
    "   âœ… Document all tables and columns\n",
    "   âœ… Track data lineage\n",
    "   âœ… Implement access controls\n",
    "   âœ… Monitor metadata changes\n",
    "   âœ… Regular metadata audits\n",
    "\n",
    "8. COMMON MISTAKES TO AVOID\n",
    "   âŒ Not dropping temporary views\n",
    "   âŒ Forgetting to uncache tables\n",
    "   âŒ Not computing statistics\n",
    "   âŒ Using SELECT * in production\n",
    "   âŒ Not documenting schema changes\n",
    "   âŒ Inconsistent naming conventions\n",
    "   âŒ Not refreshing metadata after external changes\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“š **8. PRACTICAL EXAMPLES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ“š 8. PRACTICAL EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Example 1: Data catalog report\n",
    "print(\"\\nðŸ“Š EXAMPLE 1: DATA CATALOG REPORT\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def generate_catalog_report():\n",
    "    \"\"\"Generate comprehensive catalog report\"\"\"\n",
    "    \n",
    "    report = []\n",
    "    \n",
    "    for db in spark.catalog.listDatabases():\n",
    "        db_name = db.name\n",
    "        tables = spark.catalog.listTables(db_name)\n",
    "        \n",
    "        for table in tables:\n",
    "            if not table.isTemporary:\n",
    "                table_name = f\"{db_name}.{table.name}\"\n",
    "                \n",
    "                # Get columns\n",
    "                columns = spark.catalog.listColumns(table_name)\n",
    "                \n",
    "                # Get row count (if possible)\n",
    "                try:\n",
    "                    row_count = spark.table(table_name).count()\n",
    "                except:\n",
    "                    row_count = -1\n",
    "                \n",
    "                report.append({\n",
    "                    'database': db_name,\n",
    "                    'table': table.name,\n",
    "                    'type': table.tableType,\n",
    "                    'columns': len(columns),\n",
    "                    'rows': row_count,\n",
    "                    'cached': spark.catalog.isCached(table_name)\n",
    "                })\n",
    "    \n",
    "    return spark.createDataFrame(report)\n",
    "\n",
    "catalog_report = generate_catalog_report()\n",
    "print(\"\\nðŸ“‹ Catalog Report:\")\n",
    "catalog_report.show(truncate=False)\n",
    "\n",
    "print(\"\\nðŸ“Š Summary by Database:\")\n",
    "catalog_report.groupBy(\"database\").agg(\n",
    "    F.count(\"table\").alias(\"tables\"),\n",
    "    F.sum(\"columns\").alias(\"total_columns\"),\n",
    "    F.sum(\"rows\").alias(\"total_rows\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Schema comparison\n",
    "print(\"\\nðŸ“Š EXAMPLE 2: SCHEMA COMPARISON\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def compare_schemas(table1, table2):\n",
    "    \"\"\"Compare schemas of two tables\"\"\"\n",
    "    \n",
    "    cols1 = {col.name: col.dataType for col in spark.catalog.listColumns(table1)}\n",
    "    cols2 = {col.name: col.dataType for col in spark.catalog.listColumns(table2)}\n",
    "    \n",
    "    # Columns only in table1\n",
    "    only_in_1 = set(cols1.keys()) - set(cols2.keys())\n",
    "    # Columns only in table2\n",
    "    only_in_2 = set(cols2.keys()) - set(cols1.keys())\n",
    "    # Common columns\n",
    "    common = set(cols1.keys()) & set(cols2.keys())\n",
    "    # Type differences\n",
    "    type_diffs = {col: (cols1[col], cols2[col]) \n",
    "                  for col in common if cols1[col] != cols2[col]}\n",
    "    \n",
    "    print(f\"\\nComparing: {table1} vs {table2}\")\n",
    "    print(f\"\\nColumns only in {table1}: {only_in_1}\")\n",
    "    print(f\"Columns only in {table2}: {only_in_2}\")\n",
    "    print(f\"Common columns: {len(common)}\")\n",
    "    print(f\"Type differences: {type_diffs}\")\n",
    "    \n",
    "    return {\n",
    "        'only_in_1': only_in_1,\n",
    "        'only_in_2': only_in_2,\n",
    "        'common': common,\n",
    "        'type_diffs': type_diffs\n",
    "    }\n",
    "\n",
    "# Create two similar tables for comparison\n",
    "df1 = employees.select(\"employee_id\", \"name\", \"age\", \"department\", \"salary\")\n",
    "df1.createOrReplaceTempView(\"employees_v1\")\n",
    "\n",
    "df2 = employees.select(\"employee_id\", \"name\", \"department\", \"salary\", \"status\")\n",
    "df2.createOrReplaceTempView(\"employees_v2\")\n",
    "\n",
    "comparison = compare_schemas(\"employees_v1\", \"employees_v2\")\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’¡ Use Case:\n",
    "   - Compare dev vs prod schemas\n",
    "   - Validate schema migrations\n",
    "   - Track schema evolution\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Metadata-driven processing\n",
    "print(\"\\nðŸ“Š EXAMPLE 3: METADATA-DRIVEN PROCESSING\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def process_all_tables_in_database(database_name, operation):\n",
    "    \"\"\"Process all tables in a database using metadata\"\"\"\n",
    "    \n",
    "    tables = spark.catalog.listTables(database_name)\n",
    "    results = []\n",
    "    \n",
    "    for table in tables:\n",
    "        if not table.isTemporary:\n",
    "            table_name = f\"{database_name}.{table.name}\"\n",
    "            print(f\"\\nProcessing: {table_name}\")\n",
    "            \n",
    "            try:\n",
    "                df = spark.table(table_name)\n",
    "                result = operation(df, table_name)\n",
    "                results.append((table_name, \"SUCCESS\", result))\n",
    "            except Exception as e:\n",
    "                results.append((table_name, \"FAILED\", str(e)))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example operation: Count rows\n",
    "def count_rows(df, table_name):\n",
    "    count = df.count()\n",
    "    print(f\"   Rows: {count:,}\")\n",
    "    return count\n",
    "\n",
    "# Process all tables\n",
    "results = process_all_tables_in_database(\"hr_database\", count_rows)\n",
    "\n",
    "print(\"\\nðŸ“Š Processing Results:\")\n",
    "results_df = spark.createDataFrame(results, [\"table\", \"status\", \"result\"])\n",
    "results_df.show(truncate=False)\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’¡ Use Case:\n",
    "   - Batch processing all tables\n",
    "   - Data quality checks\n",
    "   - Automated reporting\n",
    "   - Metadata-driven ETL\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ“ **KEY TAKEAWAYS**\n",
    "\n",
    "### **âœ… What You Learned:**\n",
    "\n",
    "1. **Spark Catalog API**\n",
    "   - Access metadata via `spark.catalog`\n",
    "   - List databases, tables, columns, functions\n",
    "   - Check existence, cache status\n",
    "\n",
    "2. **Database Management**\n",
    "   - CREATE/DROP databases\n",
    "   - Switch databases\n",
    "   - Query database properties\n",
    "\n",
    "3. **Table Management**\n",
    "   - Create temporary/persistent tables\n",
    "   - List and describe tables\n",
    "   - ALTER and DROP tables\n",
    "   - Table properties and metadata\n",
    "\n",
    "4. **Schema Operations**\n",
    "   - List columns and types\n",
    "   - Get schema information\n",
    "   - Schema evolution\n",
    "   - Schema serialization\n",
    "\n",
    "5. **Metadata Queries**\n",
    "   - Table and column statistics\n",
    "   - Function discovery\n",
    "   - Custom metadata reports\n",
    "\n",
    "6. **Cache Management**\n",
    "   - Cache/uncache tables\n",
    "   - Check cache status\n",
    "   - Clear all cache\n",
    "   - Refresh metadata\n",
    "\n",
    "### **ðŸ“Š Quick Reference:**\n",
    "\n",
    "```python\n",
    "# List databases\n",
    "spark.catalog.listDatabases()\n",
    "\n",
    "# List tables\n",
    "spark.catalog.listTables(\"database_name\")\n",
    "\n",
    "# List columns\n",
    "spark.catalog.listColumns(\"table_name\")\n",
    "\n",
    "# Check existence\n",
    "spark.catalog.databaseExists(\"db_name\")\n",
    "spark.catalog.tableExists(\"table_name\")\n",
    "\n",
    "# Cache management\n",
    "spark.catalog.cacheTable(\"table_name\")\n",
    "spark.catalog.isCached(\"table_name\")\n",
    "spark.catalog.uncacheTable(\"table_name\")\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# Refresh metadata\n",
    "spark.catalog.refreshTable(\"table_name\")\n",
    "```\n",
    "\n",
    "### **ðŸš€ Next:** Day 5 - Lesson 4: SQL Optimization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ§¹ CLEANUP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Drop test databases\n",
    "for db_name in [\"hr_database\", \"sales_database\", \"analytics_database\"]:\n",
    "    spark.sql(f\"DROP DATABASE IF EXISTS {db_name} CASCADE\")\n",
    "    print(f\"âœ… Dropped database: {db_name}\")\n",
    "\n",
    "# Clear cache\n",
    "spark.catalog.clearCache()\n",
    "print(\"âœ… Cleared cache\")\n",
    "\n",
    "# Stop session\n",
    "spark.stop()\n",
    "\n",
    "print(\"\\nâœ… Spark session stopped\")\n",
    "print(\"\\nðŸŽ‰ DAY 5 - LESSON 3 COMPLETED!\")\n",
    "print(\"\\nðŸ’¡ Remember:\")\n",
    "print(\"   - Catalog API provides programmatic access to metadata\")\n",
    "print(\"   - Use databases to organize tables\")\n",
    "print(\"   - Document schemas and tables\")\n",
    "print(\"   - Compute statistics for better performance\")\n",
    "print(\"   - Manage cache carefully\")\n",
    "print(\"\\nðŸ”¥ Quote: 'Good metadata is the foundation of good data!' ðŸ—„ï¸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
