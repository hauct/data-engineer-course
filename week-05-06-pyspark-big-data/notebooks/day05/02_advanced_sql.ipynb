{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ ADVANCED SQL\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ **DAY 5 - LESSON 2: ADVANCED SQL**\n",
    "\n",
    "### **ðŸŽ¯ Má»¤C TIÃŠU:**\n",
    "\n",
    "1. **Window Functions** - ROW_NUMBER, RANK, LAG, LEAD\n",
    "2. **Complex Joins** - Self-join, Cross join\n",
    "3. **Set Operations** - UNION, INTERSECT, EXCEPT\n",
    "4. **Pivot & Unpivot** - Transform data\n",
    "5. **Advanced Aggregations** - ROLLUP, CUBE, GROUPING SETS\n",
    "6. **Performance Tips** - Optimize SQL queries\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¡ **ADVANCED SQL:**\n",
    "\n",
    "- Window functions cho analytics\n",
    "- Complex joins cho hierarchical data\n",
    "- Set operations cho data comparison\n",
    "- Pivot/Unpivot cho reporting\n",
    "- Advanced aggregations cho subtotals\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ **SETUP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/11 15:44:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark Session Created\n",
      "Spark Version: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, lit, when, desc, asc\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank, percent_rank, ntile\n",
    "from pyspark.sql.functions import lag, lead, first, last\n",
    "from pyspark.sql.functions import sum as spark_sum, avg as spark_avg, count as spark_count\n",
    "from pyspark.sql.functions import round as spark_round\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AdvancedSQL\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"âœ… Spark Session Created\")\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Š **1. Táº O DATA MáºªU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ“Š 1. GENERATING SAMPLE DATA\n",
      "================================================================================\n",
      "\n",
      "ðŸ”¹ Generating employees data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated 500 employees\n",
      "+-----------+----------+---+----------+-------+------+----------+--------+----------+\n",
      "|employee_id|      name|age|department|country|salary|manager_id|  status| hire_date|\n",
      "+-----------+----------+---+----------+-------+------+----------+--------+----------+\n",
      "|    EMP0001|Employee 1| 33|     Sales|    USA| 81958|      NULL|  Active|2022-07-30|\n",
      "|    EMP0002|Employee 2| 43|        HR|Germany| 63411|      NULL|  Active|2020-12-12|\n",
      "|    EMP0003|Employee 3| 22|   Finance| France| 70931|      NULL|Inactive|2021-01-11|\n",
      "|    EMP0004|Employee 4| 33| Marketing|     UK| 60074|      NULL|  Active|2023-08-09|\n",
      "|    EMP0005|Employee 5| 48| Marketing| Canada| 87114|      NULL|  Active|2022-06-20|\n",
      "+-----------+----------+---+----------+-------+------+----------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "ðŸ”¹ Generating sales data...\n",
      "âœ… Generated 480 sales records\n",
      "+---------+-----------+----------+------+---------+\n",
      "|  sale_id|employee_id| sale_date|amount|  product|\n",
      "+---------+-----------+----------+------+---------+\n",
      "|SALE00001|    EMP0003|2024-01-01| 22472|Product A|\n",
      "|SALE00002|    EMP0067|2024-01-01| 31633|Product B|\n",
      "|SALE00003|    EMP0061|2024-01-01| 34789|Product B|\n",
      "|SALE00004|    EMP0060|2024-01-01| 29017|Product B|\n",
      "|SALE00005|    EMP0037|2024-01-01| 28487|Product C|\n",
      "+---------+-----------+----------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "âœ… Created temporary views: employees, sales\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ“Š 1. GENERATING SAMPLE DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Employees data\n",
    "departments = [\"Engineering\", \"Sales\", \"Marketing\", \"HR\", \"Finance\"]\n",
    "countries = [\"USA\", \"UK\", \"Germany\", \"France\", \"Canada\"]\n",
    "\n",
    "print(\"\\nðŸ”¹ Generating employees data...\")\n",
    "\n",
    "employees_data = []\n",
    "for i in range(1, 501):\n",
    "    dept = random.choice(departments)\n",
    "    country = random.choice(countries)\n",
    "    \n",
    "    base_salary = {\n",
    "        \"Engineering\": 80000,\n",
    "        \"Sales\": 70000,\n",
    "        \"Marketing\": 65000,\n",
    "        \"HR\": 60000,\n",
    "        \"Finance\": 75000\n",
    "    }[dept]\n",
    "    \n",
    "    salary = base_salary + random.randint(-10000, 30000)\n",
    "    \n",
    "    # Manager ID (some employees have managers)\n",
    "    manager_id = None\n",
    "    if i > 50:  # First 50 are managers\n",
    "        manager_id = f\"EMP{random.randint(1, 50):04d}\"\n",
    "    \n",
    "    employees_data.append((\n",
    "        f\"EMP{i:04d}\",\n",
    "        f\"Employee {i}\",\n",
    "        random.randint(22, 60),\n",
    "        dept,\n",
    "        country,\n",
    "        salary,\n",
    "        manager_id,\n",
    "        random.choice([\"Active\", \"Active\", \"Active\", \"Inactive\"]),\n",
    "        (datetime(2020, 1, 1) + timedelta(days=random.randint(0, 1460))).strftime(\"%Y-%m-%d\")\n",
    "    ))\n",
    "\n",
    "employees = spark.createDataFrame(employees_data,\n",
    "    [\"employee_id\", \"name\", \"age\", \"department\", \"country\", \"salary\", \"manager_id\", \"status\", \"hire_date\"])\n",
    "\n",
    "print(f\"âœ… Generated {employees.count():,} employees\")\n",
    "employees.show(5)\n",
    "\n",
    "# Sales data\n",
    "print(\"\\nðŸ”¹ Generating sales data...\")\n",
    "\n",
    "sales_data = []\n",
    "sale_id = 1\n",
    "for month in range(1, 13):\n",
    "    for day in [1, 15]:\n",
    "        for emp_id in random.sample(range(1, 101), 20):\n",
    "            sales_data.append((\n",
    "                f\"SALE{sale_id:05d}\",\n",
    "                f\"EMP{emp_id:04d}\",\n",
    "                f\"2024-{month:02d}-{day:02d}\",\n",
    "                random.randint(1000, 50000),\n",
    "                random.choice([\"Product A\", \"Product B\", \"Product C\", \"Product D\"])\n",
    "            ))\n",
    "            sale_id += 1\n",
    "\n",
    "sales = spark.createDataFrame(sales_data,\n",
    "    [\"sale_id\", \"employee_id\", \"sale_date\", \"amount\", \"product\"])\n",
    "\n",
    "print(f\"âœ… Generated {sales.count():,} sales records\")\n",
    "sales.show(5)\n",
    "\n",
    "# Create temporary views\n",
    "employees.createOrReplaceTempView(\"employees\")\n",
    "sales.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "print(\"\\nâœ… Created temporary views: employees, sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸªŸ **2. WINDOW FUNCTIONS**\n",
    "\n",
    "### **Window Functions lÃ  gÃ¬?**\n",
    "- TÃ­nh toÃ¡n trÃªn má»™t \"cá»­a sá»•\" (window) cá»§a rows\n",
    "- KhÃ´ng lÃ m giáº£m sá»‘ rows (khÃ¡c vá»›i GROUP BY)\n",
    "- Syntax: `function() OVER (PARTITION BY ... ORDER BY ...)`\n",
    "\n",
    "### **Common Window Functions:**\n",
    "- ROW_NUMBER(), RANK(), DENSE_RANK()\n",
    "- LAG(), LEAD()\n",
    "- SUM(), AVG(), COUNT() OVER (...)\n",
    "- FIRST_VALUE(), LAST_VALUE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸªŸ 2. WINDOW FUNCTIONS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š A. RANKING FUNCTIONS\n",
      "--------------------------------------------------------------------------------\n",
      "Top 3 earners per department:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----------+------+-------+----+----------+\n",
      "|employee_id|        name| department|salary|row_num|rank|dense_rank|\n",
      "+-----------+------------+-----------+------+-------+----+----------+\n",
      "|    EMP0322|Employee 322|Engineering|109070|      1|   1|         1|\n",
      "|    EMP0338|Employee 338|Engineering|107167|      2|   2|         2|\n",
      "|    EMP0323|Employee 323|Engineering|106531|      3|   3|         3|\n",
      "|    EMP0446|Employee 446|    Finance|104748|      1|   1|         1|\n",
      "|    EMP0203|Employee 203|    Finance|104252|      2|   2|         2|\n",
      "|    EMP0339|Employee 339|    Finance|103886|      3|   3|         3|\n",
      "|    EMP0493|Employee 493|         HR| 89950|      1|   1|         1|\n",
      "|    EMP0213|Employee 213|         HR| 87834|      2|   2|         2|\n",
      "|    EMP0411|Employee 411|         HR| 87120|      3|   3|         3|\n",
      "|    EMP0397|Employee 397|  Marketing| 94545|      1|   1|         1|\n",
      "|    EMP0200|Employee 200|  Marketing| 94533|      2|   2|         2|\n",
      "|    EMP0126|Employee 126|  Marketing| 94230|      3|   3|         3|\n",
      "|    EMP0247|Employee 247|      Sales| 99978|      1|   1|         1|\n",
      "|    EMP0098| Employee 98|      Sales| 99825|      2|   2|         2|\n",
      "|    EMP0256|Employee 256|      Sales| 97067|      3|   3|         3|\n",
      "+-----------+------------+-----------+------+-------+----+----------+\n",
      "\n",
      "\n",
      "ðŸ’¡ Difference:\n",
      "   - ROW_NUMBER: 1, 2, 3, 4, 5 (unique)\n",
      "   - RANK:       1, 2, 2, 4, 5 (skip after tie)\n",
      "   - DENSE_RANK: 1, 2, 2, 3, 4 (no skip)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸªŸ 2. WINDOW FUNCTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# A. Ranking Functions\n",
    "print(\"\\nðŸ“Š A. RANKING FUNCTIONS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "result_rank = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        employee_id,\n",
    "        name,\n",
    "        department,\n",
    "        salary,\n",
    "        ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as row_num,\n",
    "        RANK() OVER (PARTITION BY department ORDER BY salary DESC) as rank,\n",
    "        DENSE_RANK() OVER (PARTITION BY department ORDER BY salary DESC) as dense_rank\n",
    "    FROM employees\n",
    "    WHERE status = 'Active'\n",
    "    ORDER BY department, salary DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Top 3 earners per department:\")\n",
    "result_rank.filter(col(\"row_num\") <= 3).show(15)\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’¡ Difference:\n",
    "   - ROW_NUMBER: 1, 2, 3, 4, 5 (unique)\n",
    "   - RANK:       1, 2, 2, 4, 5 (skip after tie)\n",
    "   - DENSE_RANK: 1, 2, 2, 3, 4 (no skip)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š B. LAG and LEAD (Compare with previous/next row)\n",
      "--------------------------------------------------------------------------------\n",
      "+-----------+------------+-----------+------+-----------+-----------+--------------+\n",
      "|employee_id|        name| department|salary|prev_salary|next_salary|diff_from_prev|\n",
      "+-----------+------------+-----------+------+-----------+-----------+--------------+\n",
      "|    EMP0101|Employee 101|Engineering| 71279|       NULL|      71747|          NULL|\n",
      "|    EMP0122|Employee 122|Engineering| 71747|      71279|      73136|           468|\n",
      "|    EMP0369|Employee 369|Engineering| 73136|      71747|      73613|          1389|\n",
      "|    EMP0489|Employee 489|Engineering| 73613|      73136|      75724|           477|\n",
      "|    EMP0281|Employee 281|Engineering| 75724|      73613|      77262|          2111|\n",
      "|    EMP0381|Employee 381|Engineering| 77262|      75724|      79116|          1538|\n",
      "|    EMP0110|Employee 110|Engineering| 79116|      77262|      79542|          1854|\n",
      "|    EMP0456|Employee 456|Engineering| 79542|      79116|      80719|           426|\n",
      "|    EMP0267|Employee 267|Engineering| 80719|      79542|      81633|          1177|\n",
      "|    EMP0227|Employee 227|Engineering| 81633|      80719|      81704|           914|\n",
      "+-----------+------------+-----------+------+-----------+-----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "ðŸ’¡ LAG and LEAD:\n",
      "   - LAG(col, n):  Get value from n rows BEFORE\n",
      "   - LEAD(col, n): Get value from n rows AFTER\n",
      "   - Useful for: Time series, comparing with previous/next\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# B. LAG and LEAD\n",
    "print(\"\\nðŸ“Š B. LAG and LEAD (Compare with previous/next row)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "result_lag_lead = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        employee_id,\n",
    "        name,\n",
    "        department,\n",
    "        salary,\n",
    "        LAG(salary, 1) OVER (PARTITION BY department ORDER BY salary) as prev_salary,\n",
    "        LEAD(salary, 1) OVER (PARTITION BY department ORDER BY salary) as next_salary,\n",
    "        salary - LAG(salary, 1) OVER (PARTITION BY department ORDER BY salary) as diff_from_prev\n",
    "    FROM employees\n",
    "    WHERE status = 'Active'\n",
    "    ORDER BY department, salary\n",
    "\"\"\")\n",
    "\n",
    "result_lag_lead.show(10)\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’¡ LAG and LEAD:\n",
    "   - LAG(col, n):  Get value from n rows BEFORE\n",
    "   - LEAD(col, n): Get value from n rows AFTER\n",
    "   - Useful for: Time series, comparing with previous/next\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š C. RUNNING TOTALS and MOVING AVERAGES\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 15:44:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/11 15:44:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/11 15:44:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/11 15:44:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/11 15:44:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------------+------------------+----------------+\n",
      "| sale_date|amount|running_total|      moving_avg_3|cumulative_count|\n",
      "+----------+------+-------------+------------------+----------------+\n",
      "|2024-01-01| 22472|       538931|           22472.0|              20|\n",
      "|2024-01-01| 31633|       538931|           27052.5|              20|\n",
      "|2024-01-01| 34789|       538931|29631.333333333332|              20|\n",
      "|2024-01-01| 29017|       538931|           31813.0|              20|\n",
      "|2024-01-01| 28487|       538931|30764.333333333332|              20|\n",
      "|2024-01-01| 23666|       538931|27056.666666666668|              20|\n",
      "|2024-01-01| 22697|       538931|           24950.0|              20|\n",
      "|2024-01-01| 42407|       538931|           29590.0|              20|\n",
      "|2024-01-01| 14777|       538931|           26627.0|              20|\n",
      "|2024-01-01| 13785|       538931|23656.333333333332|              20|\n",
      "|2024-01-01| 36268|       538931|           21610.0|              20|\n",
      "|2024-01-01| 26194|       538931|25415.666666666668|              20|\n",
      "|2024-01-01| 27640|       538931|           30034.0|              20|\n",
      "|2024-01-01| 43232|       538931|32355.333333333332|              20|\n",
      "|2024-01-01| 30325|       538931|33732.333333333336|              20|\n",
      "|2024-01-01|  3292|       538931|25616.333333333332|              20|\n",
      "|2024-01-01| 44338|       538931|           25985.0|              20|\n",
      "|2024-01-01| 24982|       538931|           24204.0|              20|\n",
      "|2024-01-01| 27929|       538931|32416.333333333332|              20|\n",
      "|2024-01-01| 11001|       538931|           21304.0|              20|\n",
      "+----------+------+-------------+------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "ðŸ’¡ Window Frame:\n",
      "   - ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n",
      "     â†’ Last 3 rows (including current)\n",
      "   \n",
      "   - ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
      "     â†’ All rows from start to current (running total)\n",
      "   \n",
      "   - ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING\n",
      "     â†’ Previous, current, and next row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# C. Running Totals and Moving Averages\n",
    "print(\"\\nðŸ“Š C. RUNNING TOTALS and MOVING AVERAGES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "result_running = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        sale_date,\n",
    "        amount,\n",
    "        SUM(amount) OVER (ORDER BY sale_date) as running_total,\n",
    "        AVG(amount) OVER (ORDER BY sale_date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) as moving_avg_3,\n",
    "        COUNT(*) OVER (ORDER BY sale_date) as cumulative_count\n",
    "    FROM sales\n",
    "    ORDER BY sale_date\n",
    "\"\"\")\n",
    "\n",
    "result_running.show(20)\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’¡ Window Frame:\n",
    "   - ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n",
    "     â†’ Last 3 rows (including current)\n",
    "   \n",
    "   - ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "     â†’ All rows from start to current (running total)\n",
    "   \n",
    "   - ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING\n",
    "     â†’ Previous, current, and next row\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š D. FIRST_VALUE and LAST_VALUE\n",
      "--------------------------------------------------------------------------------\n",
      "+-----------+------------+------+------------+------------+----------+-------------+\n",
      "| department|        name|salary|highest_paid| lowest_paid|max_salary|diff_from_max|\n",
      "+-----------+------------+------+------------+------------+----------+-------------+\n",
      "|Engineering|Employee 322|109070|Employee 322|Employee 101|    109070|            0|\n",
      "|Engineering|Employee 338|107167|Employee 322|Employee 101|    109070|        -1903|\n",
      "|Engineering|Employee 323|106531|Employee 322|Employee 101|    109070|        -2539|\n",
      "|Engineering|Employee 320|106428|Employee 322|Employee 101|    109070|        -2642|\n",
      "|Engineering|Employee 180|106326|Employee 322|Employee 101|    109070|        -2744|\n",
      "|Engineering| Employee 38|106167|Employee 322|Employee 101|    109070|        -2903|\n",
      "|Engineering| Employee 64|104672|Employee 322|Employee 101|    109070|        -4398|\n",
      "|Engineering|Employee 166|103831|Employee 322|Employee 101|    109070|        -5239|\n",
      "|Engineering|Employee 270|103587|Employee 322|Employee 101|    109070|        -5483|\n",
      "|Engineering|Employee 471|103462|Employee 322|Employee 101|    109070|        -5608|\n",
      "|Engineering|Employee 453|103321|Employee 322|Employee 101|    109070|        -5749|\n",
      "|Engineering|Employee 443|103161|Employee 322|Employee 101|    109070|        -5909|\n",
      "|Engineering|Employee 104|102964|Employee 322|Employee 101|    109070|        -6106|\n",
      "|Engineering|Employee 209|102950|Employee 322|Employee 101|    109070|        -6120|\n",
      "|Engineering|Employee 354|102097|Employee 322|Employee 101|    109070|        -6973|\n",
      "+-----------+------------+------+------------+------------+----------+-------------+\n",
      "only showing top 15 rows\n",
      "\n",
      "\n",
      "ðŸ’¡ FIRST_VALUE and LAST_VALUE:\n",
      "   - FIRST_VALUE: First value in window\n",
      "   - LAST_VALUE: Last value in window\n",
      "   - Need ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n",
      "     for LAST_VALUE to work correctly\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# D. FIRST_VALUE and LAST_VALUE\n",
    "print(\"\\nðŸ“Š D. FIRST_VALUE and LAST_VALUE\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "result_first_last = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        name,\n",
    "        salary,\n",
    "        FIRST_VALUE(name) OVER (PARTITION BY department ORDER BY salary DESC) as highest_paid,\n",
    "        LAST_VALUE(name) OVER (PARTITION BY department ORDER BY salary DESC \n",
    "                               ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as lowest_paid,\n",
    "        FIRST_VALUE(salary) OVER (PARTITION BY department ORDER BY salary DESC) as max_salary,\n",
    "        salary - FIRST_VALUE(salary) OVER (PARTITION BY department ORDER BY salary DESC) as diff_from_max\n",
    "    FROM employees\n",
    "    WHERE status = 'Active'\n",
    "    ORDER BY department, salary DESC\n",
    "\"\"\")\n",
    "\n",
    "result_first_last.show(15)\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’¡ FIRST_VALUE and LAST_VALUE:\n",
    "   - FIRST_VALUE: First value in window\n",
    "   - LAST_VALUE: Last value in window\n",
    "   - Need ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n",
    "     for LAST_VALUE to work correctly\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š E. NTILE (Divide into buckets)\n",
      "--------------------------------------------------------------------------------\n",
      "+-----------+------------+-----------+------+--------+--------------+\n",
      "|employee_id|        name| department|salary|quartile|salary_bracket|\n",
      "+-----------+------------+-----------+------+--------+--------------+\n",
      "|    EMP0322|Employee 322|Engineering|109070|       1|       Top 25%|\n",
      "|    EMP0338|Employee 338|Engineering|107167|       1|       Top 25%|\n",
      "|    EMP0323|Employee 323|Engineering|106531|       1|       Top 25%|\n",
      "|    EMP0320|Employee 320|Engineering|106428|       1|       Top 25%|\n",
      "|    EMP0180|Employee 180|Engineering|106326|       1|       Top 25%|\n",
      "|    EMP0038| Employee 38|Engineering|106167|       1|       Top 25%|\n",
      "|    EMP0064| Employee 64|Engineering|104672|       1|       Top 25%|\n",
      "|    EMP0166|Employee 166|Engineering|103831|       1|       Top 25%|\n",
      "|    EMP0270|Employee 270|Engineering|103587|       1|       Top 25%|\n",
      "|    EMP0471|Employee 471|Engineering|103462|       1|       Top 25%|\n",
      "|    EMP0453|Employee 453|Engineering|103321|       1|       Top 25%|\n",
      "|    EMP0443|Employee 443|Engineering|103161|       1|       Top 25%|\n",
      "|    EMP0104|Employee 104|Engineering|102964|       1|       Top 25%|\n",
      "|    EMP0209|Employee 209|Engineering|102950|       1|       Top 25%|\n",
      "|    EMP0354|Employee 354|Engineering|102097|       1|       Top 25%|\n",
      "|    EMP0179|Employee 179|Engineering|102072|       1|       Top 25%|\n",
      "|    EMP0243|Employee 243|Engineering|101986|       1|       Top 25%|\n",
      "|    EMP0069| Employee 69|Engineering|101961|       1|       Top 25%|\n",
      "|    EMP0378|Employee 378|Engineering|101648|       2|       Top 50%|\n",
      "|    EMP0131|Employee 131|Engineering|100633|       2|       Top 50%|\n",
      "+-----------+------------+-----------+------+--------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "ðŸ’¡ NTILE(n):\n",
      "   - Divide rows into n buckets\n",
      "   - NTILE(4) = Quartiles (4 buckets)\n",
      "   - NTILE(10) = Deciles (10 buckets)\n",
      "   - NTILE(100) = Percentiles (100 buckets)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# E. NTILE\n",
    "print(\"\\nðŸ“Š E. NTILE (Divide into buckets)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "result_ntile = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        employee_id,\n",
    "        name,\n",
    "        department,\n",
    "        salary,\n",
    "        NTILE(4) OVER (PARTITION BY department ORDER BY salary DESC) as quartile,\n",
    "        CASE \n",
    "            WHEN NTILE(4) OVER (PARTITION BY department ORDER BY salary DESC) = 1 THEN 'Top 25%'\n",
    "            WHEN NTILE(4) OVER (PARTITION BY department ORDER BY salary DESC) = 2 THEN 'Top 50%'\n",
    "            WHEN NTILE(4) OVER (PARTITION BY department ORDER BY salary DESC) = 3 THEN 'Top 75%'\n",
    "            ELSE 'Bottom 25%'\n",
    "        END as salary_bracket\n",
    "    FROM employees\n",
    "    WHERE status = 'Active'\n",
    "    ORDER BY department, salary DESC\n",
    "\"\"\")\n",
    "\n",
    "result_ntile.show(20)\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’¡ NTILE(n):\n",
    "   - Divide rows into n buckets\n",
    "   - NTILE(4) = Quartiles (4 buckets)\n",
    "   - NTILE(10) = Deciles (10 buckets)\n",
    "   - NTILE(100) = Percentiles (100 buckets)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š F. PERCENT_RANK (Percentile)\n",
      "--------------------------------------------------------------------------------\n",
      "+-----------+------------+-----------+------+------------------+--------------+\n",
      "|employee_id|        name| department|salary|        percentile|percentile_pct|\n",
      "+-----------+------------+-----------+------+------------------+--------------+\n",
      "|    EMP0322|Employee 322|Engineering|109070|               1.0|         100.0|\n",
      "|    EMP0338|Employee 338|Engineering|107167|0.9852941176470589|         98.53|\n",
      "|    EMP0323|Employee 323|Engineering|106531|0.9705882352941176|         97.06|\n",
      "|    EMP0320|Employee 320|Engineering|106428|0.9558823529411765|         95.59|\n",
      "|    EMP0180|Employee 180|Engineering|106326|0.9411764705882353|         94.12|\n",
      "|    EMP0038| Employee 38|Engineering|106167|0.9264705882352942|         92.65|\n",
      "|    EMP0064| Employee 64|Engineering|104672|0.9117647058823529|         91.18|\n",
      "|    EMP0166|Employee 166|Engineering|103831|0.8970588235294118|         89.71|\n",
      "|    EMP0270|Employee 270|Engineering|103587|0.8823529411764706|         88.24|\n",
      "|    EMP0471|Employee 471|Engineering|103462|0.8676470588235294|         86.76|\n",
      "|    EMP0453|Employee 453|Engineering|103321|0.8529411764705882|         85.29|\n",
      "|    EMP0443|Employee 443|Engineering|103161|0.8382352941176471|         83.82|\n",
      "|    EMP0104|Employee 104|Engineering|102964|0.8235294117647058|         82.35|\n",
      "|    EMP0209|Employee 209|Engineering|102950|0.8088235294117647|         80.88|\n",
      "|    EMP0354|Employee 354|Engineering|102097|0.7941176470588235|         79.41|\n",
      "+-----------+------------+-----------+------+------------------+--------------+\n",
      "only showing top 15 rows\n",
      "\n",
      "\n",
      "ðŸ’¡ PERCENT_RANK():\n",
      "   - Returns percentile (0.0 to 1.0)\n",
      "   - 0.0 = lowest value\n",
      "   - 1.0 = highest value\n",
      "   - Multiply by 100 for percentage\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# F. PERCENT_RANK\n",
    "print(\"\\nðŸ“Š F. PERCENT_RANK (Percentile)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "result_percent = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        employee_id,\n",
    "        name,\n",
    "        department,\n",
    "        salary,\n",
    "        PERCENT_RANK() OVER (PARTITION BY department ORDER BY salary) as percentile,\n",
    "        ROUND(PERCENT_RANK() OVER (PARTITION BY department ORDER BY salary) * 100, 2) as percentile_pct\n",
    "    FROM employees\n",
    "    WHERE status = 'Active'\n",
    "    ORDER BY department, salary DESC\n",
    "\"\"\")\n",
    "\n",
    "result_percent.show(15)\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’¡ PERCENT_RANK():\n",
    "   - Returns percentile (0.0 to 1.0)\n",
    "   - 0.0 = lowest value\n",
    "   - 1.0 = highest value\n",
    "   - Multiply by 100 for percentage\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”— **3. COMPLEX JOINS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ”— 3. COMPLEX JOINS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š A. SELF JOIN (Employees and their managers)\n",
      "--------------------------------------------------------------------------------\n",
      "+-----------+-------------+-----------+---------------+----------+------------+--------------+\n",
      "|employee_id|employee_name| department|employee_salary|manager_id|manager_name|manager_salary|\n",
      "+-----------+-------------+-----------+---------------+----------+------------+--------------+\n",
      "|    EMP0027|  Employee 27|Engineering|          85504|      NULL|        NULL|          NULL|\n",
      "|    EMP0033|  Employee 33|Engineering|          90435|      NULL|        NULL|          NULL|\n",
      "|    EMP0037|  Employee 37|Engineering|          88755|      NULL|        NULL|          NULL|\n",
      "|    EMP0038|  Employee 38|Engineering|         106167|      NULL|        NULL|          NULL|\n",
      "|    EMP0040|  Employee 40|Engineering|          98947|      NULL|        NULL|          NULL|\n",
      "|    EMP0048|  Employee 48|Engineering|          99595|      NULL|        NULL|          NULL|\n",
      "|    EMP0052|  Employee 52|Engineering|          81918|   EMP0021| Employee 21|        100352|\n",
      "|    EMP0061|  Employee 61|Engineering|         100139|   EMP0047| Employee 47|        102179|\n",
      "|    EMP0064|  Employee 64|Engineering|         104672|   EMP0016| Employee 16|         87283|\n",
      "|    EMP0069|  Employee 69|Engineering|         101961|   EMP0025| Employee 25|         90013|\n",
      "+-----------+-------------+-----------+---------------+----------+------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "ðŸ’¡ Self Join:\n",
      "   - Join table with itself\n",
      "   - Use aliases (e, m) to distinguish\n",
      "   - Common use: Hierarchical data (employee-manager)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ”— 3. COMPLEX JOINS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# A. Self Join\n",
    "print(\"\\nðŸ“Š A. SELF JOIN (Employees and their managers)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "result_self_join = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e.employee_id,\n",
    "        e.name as employee_name,\n",
    "        e.department,\n",
    "        e.salary as employee_salary,\n",
    "        m.employee_id as manager_id,\n",
    "        m.name as manager_name,\n",
    "        m.salary as manager_salary\n",
    "    FROM employees e\n",
    "    LEFT JOIN employees m ON e.manager_id = m.employee_id\n",
    "    WHERE e.status = 'Active'\n",
    "    ORDER BY e.department, e.employee_id\n",
    "\"\"\")\n",
    "\n",
    "result_self_join.show(10)\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’¡ Self Join:\n",
    "   - Join table with itself\n",
    "   - Use aliases (e, m) to distinguish\n",
    "   - Common use: Hierarchical data (employee-manager)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š B. Employees earning more than their manager\n",
      "--------------------------------------------------------------------------------\n",
      "+-----------+-------------+---------------+------------+--------------+-----------+\n",
      "|employee_id|employee_name|employee_salary|manager_name|manager_salary|salary_diff|\n",
      "+-----------+-------------+---------------+------------+--------------+-----------+\n",
      "|    EMP0322| Employee 322|         109070| Employee 36|         59643|      49427|\n",
      "|    EMP0240| Employee 240|         107751| Employee 22|         59347|      48404|\n",
      "|    EMP0338| Employee 338|         107167|  Employee 4|         60074|      47093|\n",
      "|    EMP0320| Employee 320|         106428|  Employee 4|         60074|      46354|\n",
      "|    EMP0179| Employee 179|         102072| Employee 22|         59347|      42725|\n",
      "|    EMP0247| Employee 247|          99978| Employee 22|         59347|      40631|\n",
      "|    EMP0166| Employee 166|         103831| Employee 50|         63660|      40171|\n",
      "|    EMP0346| Employee 346|          99723| Employee 35|         59740|      39983|\n",
      "|    EMP0465| Employee 465|          98473|  Employee 4|         60074|      38399|\n",
      "|    EMP0256| Employee 256|          97067| Employee 22|         59347|      37720|\n",
      "+-----------+-------------+---------------+------------+--------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# B. Employees earning more than manager\n",
    "print(\"\\nðŸ“Š B. Employees earning more than their manager\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "result_higher_salary = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e.employee_id,\n",
    "        e.name as employee_name,\n",
    "        e.salary as employee_salary,\n",
    "        m.name as manager_name,\n",
    "        m.salary as manager_salary,\n",
    "        e.salary - m.salary as salary_diff\n",
    "    FROM employees e\n",
    "    INNER JOIN employees m ON e.manager_id = m.employee_id\n",
    "    WHERE e.salary > m.salary\n",
    "    ORDER BY salary_diff DESC\n",
    "\"\"\")\n",
    "\n",
    "result_higher_salary.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š C. CROSS JOIN (Cartesian product)\n",
      "--------------------------------------------------------------------------------\n",
      "All combinations of departments and categories:\n",
      "+-----------+----------+\n",
      "| department|  category|\n",
      "+-----------+----------+\n",
      "|Engineering|Category A|\n",
      "|Engineering|Category B|\n",
      "|Engineering|Category C|\n",
      "|    Finance|Category A|\n",
      "|    Finance|Category B|\n",
      "|    Finance|Category C|\n",
      "|         HR|Category A|\n",
      "|         HR|Category B|\n",
      "|         HR|Category C|\n",
      "|  Marketing|Category A|\n",
      "|  Marketing|Category B|\n",
      "|  Marketing|Category C|\n",
      "|      Sales|Category A|\n",
      "|      Sales|Category B|\n",
      "|      Sales|Category C|\n",
      "+-----------+----------+\n",
      "\n",
      "\n",
      "ðŸ’¡ Cross Join:\n",
      "   - Cartesian product (all combinations)\n",
      "   - Result size = table1_rows Ã— table2_rows\n",
      "   - âš ï¸  Use carefully! Can be very large\n",
      "   - Use case: Generate all combinations\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# C. Cross Join\n",
    "print(\"\\nðŸ“Š C. CROSS JOIN (Cartesian product)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Create small lookup table\n",
    "categories = spark.createDataFrame([\n",
    "    (\"Category A\",),\n",
    "    (\"Category B\",),\n",
    "    (\"Category C\",)\n",
    "], [\"category\"])\n",
    "\n",
    "categories.createOrReplaceTempView(\"categories\")\n",
    "\n",
    "result_cross = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        d.department,\n",
    "        c.category\n",
    "    FROM (SELECT DISTINCT department FROM employees) d\n",
    "    CROSS JOIN categories c\n",
    "    ORDER BY d.department, c.category\n",
    "\"\"\")\n",
    "\n",
    "print(\"All combinations of departments and categories:\")\n",
    "result_cross.show()\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’¡ Cross Join:\n",
    "   - Cartesian product (all combinations)\n",
    "   - Result size = table1_rows Ã— table2_rows\n",
    "   - âš ï¸  Use carefully! Can be very large\n",
    "   - Use case: Generate all combinations\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š D. MULTIPLE JOINS\n",
      "--------------------------------------------------------------------------------\n",
      "+-----------+-----------+-----------+------+-----------+-------------+------------+\n",
      "|employee_id|       name| department|salary|total_sales|total_revenue|manager_name|\n",
      "+-----------+-----------+-----------+------+-----------+-------------+------------+\n",
      "|    EMP0078|Employee 78|      Sales| 64876|          8|       318770|  Employee 8|\n",
      "|    EMP0033|Employee 33|Engineering| 90435|          8|       223157|        NULL|\n",
      "|    EMP0013|Employee 13|      Sales| 90946|          7|       214662|        NULL|\n",
      "|    EMP0060|Employee 60|  Marketing| 66665|          8|       211087| Employee 38|\n",
      "|    EMP0032|Employee 32|  Marketing| 88322|          7|       205087|        NULL|\n",
      "|    EMP0057|Employee 57|    Finance| 92738|          7|       202772| Employee 43|\n",
      "|    EMP0056|Employee 56|  Marketing| 89386|          6|       202474|  Employee 9|\n",
      "|    EMP0018|Employee 18|      Sales| 88504|          7|       196280|        NULL|\n",
      "|    EMP0045|Employee 45|    Finance| 78590|          6|       191146|        NULL|\n",
      "|    EMP0028|Employee 28|      Sales| 88537|          8|       187786|        NULL|\n",
      "+-----------+-----------+-----------+------+-----------+-------------+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "ðŸ’¡ Multiple Joins:\n",
      "   - Join multiple tables in one query\n",
      "   - Use LEFT JOIN to keep all employees (even without sales)\n",
      "   - Use COALESCE to handle NULLs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# D. Multiple Joins\n",
    "print(\"\\nðŸ“Š D. MULTIPLE JOINS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "result_multiple = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e.employee_id,\n",
    "        e.name,\n",
    "        e.department,\n",
    "        e.salary,\n",
    "        COUNT(s.sale_id) as total_sales,\n",
    "        COALESCE(SUM(s.amount), 0) as total_revenue,\n",
    "        m.name as manager_name\n",
    "    FROM employees e\n",
    "    LEFT JOIN sales s ON e.employee_id = s.employee_id\n",
    "    LEFT JOIN employees m ON e.manager_id = m.employee_id\n",
    "    WHERE e.status = 'Active'\n",
    "    GROUP BY e.employee_id, e.name, e.department, e.salary, m.name\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "result_multiple.show(10)\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’¡ Multiple Joins:\n",
    "   - Join multiple tables in one query\n",
    "   - Use LEFT JOIN to keep all employees (even without sales)\n",
    "   - Use COALESCE to handle NULLs\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”€ **4. SET OPERATIONS**\n",
    "\n",
    "### **Set Operations:**\n",
    "- **UNION**: Combine results (remove duplicates)\n",
    "- **UNION ALL**: Combine results (keep duplicates)\n",
    "- **INTERSECT**: Common rows\n",
    "- **EXCEPT**: Rows in first but not in second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ”€ 4. SET OPERATIONS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š A. UNION (Combine results)\n",
      "--------------------------------------------------------------------------------\n",
      "+-----------+-----------+--------------+\n",
      "|employee_id|       name|      category|\n",
      "+-----------+-----------+--------------+\n",
      "|    EMP0003| Employee 3|Young Employee|\n",
      "|    EMP0005| Employee 5|   High Earner|\n",
      "|    EMP0007| Employee 7|   High Earner|\n",
      "|    EMP0008| Employee 8|   High Earner|\n",
      "|    EMP0009| Employee 9|   High Earner|\n",
      "|    EMP0012|Employee 12|   High Earner|\n",
      "|    EMP0012|Employee 12|Young Employee|\n",
      "|    EMP0013|Employee 13|   High Earner|\n",
      "|    EMP0014|Employee 14|   High Earner|\n",
      "|    EMP0015|Employee 15|   High Earner|\n",
      "+-----------+-----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ”€ 4. SET OPERATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# A. UNION\n",
    "print(\"\\nðŸ“Š A. UNION (Combine results)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "result_union = spark.sql(\"\"\"\n",
    "    SELECT employee_id, name, 'High Earner' as category\n",
    "    FROM employees\n",
    "    WHERE salary > 85000\n",
    "    \n",
    "    UNION\n",
    "    \n",
    "    SELECT employee_id, name, 'Young Employee' as category\n",
    "    FROM employees\n",
    "    WHERE age < 30\n",
    "    \n",
    "    ORDER BY employee_id\n",
    "\"\"\")\n",
    "\n",
    "result_union.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š B. UNION ALL (Keep duplicates)\n",
      "--------------------------------------------------------------------------------\n",
      "UNION result: 318 rows (duplicates removed)\n",
      "UNION ALL result: 318 rows (duplicates kept)\n",
      "+-----------+-----------+--------------+\n",
      "|employee_id|       name|      category|\n",
      "+-----------+-----------+--------------+\n",
      "|    EMP0003| Employee 3|Young Employee|\n",
      "|    EMP0005| Employee 5|   High Earner|\n",
      "|    EMP0007| Employee 7|   High Earner|\n",
      "|    EMP0008| Employee 8|   High Earner|\n",
      "|    EMP0009| Employee 9|   High Earner|\n",
      "|    EMP0012|Employee 12|Young Employee|\n",
      "|    EMP0012|Employee 12|   High Earner|\n",
      "|    EMP0013|Employee 13|   High Earner|\n",
      "|    EMP0014|Employee 14|   High Earner|\n",
      "|    EMP0015|Employee 15|   High Earner|\n",
      "+-----------+-----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# B. UNION ALL\n",
    "print(\"\\nðŸ“Š B. UNION ALL (Keep duplicates)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "result_union_all = spark.sql(\"\"\"\n",
    "    SELECT employee_id, name, 'High Earner' as category\n",
    "    FROM employees\n",
    "    WHERE salary > 85000\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT employee_id, name, 'Young Employee' as category\n",
    "    FROM employees\n",
    "    WHERE age < 30\n",
    "    \n",
    "    ORDER BY employee_id\n",
    "\"\"\")\n",
    "\n",
    "print(f\"UNION result: {result_union.count()} rows (duplicates removed)\")\n",
    "print(f\"UNION ALL result: {result_union_all.count()} rows (duplicates kept)\")\n",
    "result_union_all.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š C. INTERSECT (Common rows)\n",
      "--------------------------------------------------------------------------------\n",
      "Employees who are both high earners AND young:\n",
      "+-----------+------------+\n",
      "|employee_id|        name|\n",
      "+-----------+------------+\n",
      "|    EMP0392|Employee 392|\n",
      "|    EMP0455|Employee 455|\n",
      "|    EMP0409|Employee 409|\n",
      "|    EMP0423|Employee 423|\n",
      "|    EMP0319|Employee 319|\n",
      "|    EMP0149|Employee 149|\n",
      "|    EMP0170|Employee 170|\n",
      "|    EMP0362|Employee 362|\n",
      "|    EMP0118|Employee 118|\n",
      "|    EMP0396|Employee 396|\n",
      "+-----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# C. INTERSECT\n",
    "print(\"\\nðŸ“Š C. INTERSECT (Common rows)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "result_intersect = spark.sql(\"\"\"\n",
    "    SELECT employee_id, name\n",
    "    FROM employees\n",
    "    WHERE salary > 80000\n",
    "    \n",
    "    INTERSECT\n",
    "    \n",
    "    SELECT employee_id, name\n",
    "    FROM employees\n",
    "    WHERE age < 35\n",
    "\"\"\")\n",
    "\n",
    "print(\"Employees who are both high earners AND young:\")\n",
    "result_intersect.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š D. EXCEPT (Difference)\n",
      "--------------------------------------------------------------------------------\n",
      "Engineering employees who are NOT high earners:\n",
      "+-----------+------------+------+\n",
      "|employee_id|        name|salary|\n",
      "+-----------+------------+------+\n",
      "|    EMP0052| Employee 52| 81918|\n",
      "|    EMP0215|Employee 215| 82603|\n",
      "|    EMP0197|Employee 197| 81704|\n",
      "|    EMP0122|Employee 122| 71747|\n",
      "|    EMP0227|Employee 227| 81633|\n",
      "|    EMP0235|Employee 235| 76550|\n",
      "|    EMP0080| Employee 80| 75704|\n",
      "|    EMP0121|Employee 121| 70183|\n",
      "|    EMP0110|Employee 110| 79116|\n",
      "|    EMP0142|Employee 142| 84455|\n",
      "+-----------+------------+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "ðŸ’¡ Set Operations Summary:\n",
      "   - UNION: A âˆª B (remove duplicates)\n",
      "   - UNION ALL: A + B (keep duplicates)\n",
      "   - INTERSECT: A âˆ© B (common)\n",
      "   - EXCEPT: A - B (difference)\n",
      "   \n",
      "   âš ï¸  All queries must have same number of columns and types\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# D. EXCEPT\n",
    "print(\"\\nðŸ“Š D. EXCEPT (Difference)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "result_except = spark.sql(\"\"\"\n",
    "    SELECT employee_id, name, salary\n",
    "    FROM employees\n",
    "    WHERE department = 'Engineering'\n",
    "    \n",
    "    EXCEPT\n",
    "    \n",
    "    SELECT employee_id, name, salary\n",
    "    FROM employees\n",
    "    WHERE salary > 85000\n",
    "\"\"\")\n",
    "\n",
    "print(\"Engineering employees who are NOT high earners:\")\n",
    "result_except.show(10)\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’¡ Set Operations Summary:\n",
    "   - UNION: A âˆª B (remove duplicates)\n",
    "   - UNION ALL: A + B (keep duplicates)\n",
    "   - INTERSECT: A âˆ© B (common)\n",
    "   - EXCEPT: A - B (difference)\n",
    "   \n",
    "   âš ï¸  All queries must have same number of columns and types\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”„ **5. PIVOT & UNPIVOT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ”„ 5. PIVOT & UNPIVOT\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š A. PIVOT (Transform rows to columns)\n",
      "--------------------------------------------------------------------------------\n",
      "Original data (long format):\n",
      "+-----------+-------+--------------+\n",
      "| department|country|employee_count|\n",
      "+-----------+-------+--------------+\n",
      "|Engineering| Canada|            19|\n",
      "|Engineering| France|            22|\n",
      "|Engineering|Germany|            13|\n",
      "|Engineering|     UK|            19|\n",
      "|Engineering|    USA|            26|\n",
      "|    Finance| Canada|            26|\n",
      "|    Finance| France|            30|\n",
      "|    Finance|Germany|            15|\n",
      "|    Finance|     UK|            17|\n",
      "|    Finance|    USA|            15|\n",
      "+-----------+-------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Pivoted data (wide format):\n",
      "+-----------+---------+--------------+--------+-------------+-------------+------------------+------------+-----------------+------------+-----------------+\n",
      "| department|USA_count|USA_avg_salary|UK_count|UK_avg_salary|Germany_count|Germany_avg_salary|France_count|France_avg_salary|Canada_count|Canada_avg_salary|\n",
      "+-----------+---------+--------------+--------+-------------+-------------+------------------+------------+-----------------+------------+-----------------+\n",
      "|Engineering|       26|      87948.96|      19|     90710.79|           13|          91025.77|          22|          92605.5|          19|         94731.95|\n",
      "|    Finance|       15|       81691.0|      17|     84926.47|           15|           82742.6|          30|         83490.47|          26|         85838.42|\n",
      "|         HR|       13|      68408.08|      18|     74094.78|           21|          71282.48|          22|         67404.68|          15|         68777.53|\n",
      "|  Marketing|       32|      73564.09|      15|     73648.73|           19|          77102.05|          14|         79742.36|          21|         78678.05|\n",
      "|      Sales|       20|       75284.1|      27|      81081.3|           24|          82575.08|          19|         82376.79|          18|         79396.67|\n",
      "+-----------+---------+--------------+--------+-------------+-------------+------------------+------------+-----------------+------------+-----------------+\n",
      "\n",
      "\n",
      "ðŸ’¡ PIVOT:\n",
      "   - Transform rows to columns\n",
      "   - Syntax: PIVOT (agg_func FOR column IN (values))\n",
      "   - Useful for: Reports, dashboards\n",
      "   - Long format â†’ Wide format\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ”„ 5. PIVOT & UNPIVOT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# A. PIVOT\n",
    "print(\"\\nðŸ“Š A. PIVOT (Transform rows to columns)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"Original data (long format):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT department, country, COUNT(*) as employee_count\n",
    "    FROM employees\n",
    "    GROUP BY department, country\n",
    "    ORDER BY department, country\n",
    "\"\"\").show(10)\n",
    "\n",
    "print(\"\\nPivoted data (wide format):\")\n",
    "result_pivot = spark.sql(\"\"\"\n",
    "    SELECT * FROM (\n",
    "        SELECT department, country, salary\n",
    "        FROM employees\n",
    "    )\n",
    "    PIVOT (\n",
    "        COUNT(*) as count,\n",
    "        ROUND(AVG(salary), 2) as avg_salary\n",
    "        FOR country IN ('USA', 'UK', 'Germany', 'France', 'Canada')\n",
    "    )\n",
    "    ORDER BY department\n",
    "\"\"\")\n",
    "\n",
    "result_pivot.show()\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’¡ PIVOT:\n",
    "   - Transform rows to columns\n",
    "   - Syntax: PIVOT (agg_func FOR column IN (values))\n",
    "   - Useful for: Reports, dashboards\n",
    "   - Long format â†’ Wide format\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š B. PIVOT using CASE WHEN (more flexible)\n",
      "--------------------------------------------------------------------------------\n",
      "+-----------+---------------+---------+--------+-------------+------------+------------+--------------+-------------+\n",
      "| department|total_employees|usa_count|uk_count|germany_count|france_count|canada_count|usa_avg_salary|uk_avg_salary|\n",
      "+-----------+---------------+---------+--------+-------------+------------+------------+--------------+-------------+\n",
      "|Engineering|             99|       26|      19|           13|          22|          19|      87948.96|     90710.79|\n",
      "|    Finance|            103|       15|      17|           15|          30|          26|       81691.0|     84926.47|\n",
      "|         HR|             89|       13|      18|           21|          22|          15|      68408.08|     74094.78|\n",
      "|  Marketing|            101|       32|      15|           19|          14|          21|      73564.09|     73648.73|\n",
      "|      Sales|            108|       20|      27|           24|          19|          18|       75284.1|      81081.3|\n",
      "+-----------+---------------+---------+--------+-------------+------------+------------+--------------+-------------+\n",
      "\n",
      "\n",
      "ðŸ’¡ PIVOT with CASE WHEN:\n",
      "   - More flexible than PIVOT\n",
      "   - Can use any condition\n",
      "   - Can mix different aggregations\n",
      "   - Easier to understand\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# B. PIVOT using CASE WHEN\n",
    "print(\"\\nðŸ“Š B. PIVOT using CASE WHEN (more flexible)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "result_pivot_case = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        COUNT(*) as total_employees,\n",
    "        SUM(CASE WHEN country = 'USA' THEN 1 ELSE 0 END) as usa_count,\n",
    "        SUM(CASE WHEN country = 'UK' THEN 1 ELSE 0 END) as uk_count,\n",
    "        SUM(CASE WHEN country = 'Germany' THEN 1 ELSE 0 END) as germany_count,\n",
    "        SUM(CASE WHEN country = 'France' THEN 1 ELSE 0 END) as france_count,\n",
    "        SUM(CASE WHEN country = 'Canada' THEN 1 ELSE 0 END) as canada_count,\n",
    "        ROUND(AVG(CASE WHEN country = 'USA' THEN salary END), 2) as usa_avg_salary,\n",
    "        ROUND(AVG(CASE WHEN country = 'UK' THEN salary END), 2) as uk_avg_salary\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "    ORDER BY department\n",
    "\"\"\")\n",
    "\n",
    "result_pivot_case.show()\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’¡ PIVOT with CASE WHEN:\n",
    "   - More flexible than PIVOT\n",
    "   - Can use any condition\n",
    "   - Can mix different aggregations\n",
    "   - Easier to understand\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š C. UNPIVOT (Transform columns to rows)\n",
      "--------------------------------------------------------------------------------\n",
      "Pivoted data (wide format):\n",
      "+-----------+---+---+-------+\n",
      "| department|USA| UK|Germany|\n",
      "+-----------+---+---+-------+\n",
      "|      Sales| 20| 27|     24|\n",
      "|Engineering| 26| 19|     13|\n",
      "|         HR| 13| 18|     21|\n",
      "|    Finance| 15| 17|     15|\n",
      "|  Marketing| 32| 15|     19|\n",
      "+-----------+---+---+-------+\n",
      "\n",
      "\n",
      "Unpivoted data (long format):\n",
      "+-----------+-------+--------------+\n",
      "| department|country|employee_count|\n",
      "+-----------+-------+--------------+\n",
      "|Engineering|Germany|            13|\n",
      "|Engineering|     UK|            19|\n",
      "|Engineering|    USA|            26|\n",
      "|    Finance|Germany|            15|\n",
      "|    Finance|     UK|            17|\n",
      "|    Finance|    USA|            15|\n",
      "|         HR|Germany|            21|\n",
      "|         HR|     UK|            18|\n",
      "|         HR|    USA|            13|\n",
      "|  Marketing|Germany|            19|\n",
      "|  Marketing|     UK|            15|\n",
      "|  Marketing|    USA|            32|\n",
      "|      Sales|Germany|            24|\n",
      "|      Sales|     UK|            27|\n",
      "|      Sales|    USA|            20|\n",
      "+-----------+-------+--------------+\n",
      "\n",
      "\n",
      "ðŸ’¡ UNPIVOT:\n",
      "   - Transform columns to rows\n",
      "   - Use LATERAL VIEW STACK\n",
      "   - Wide format â†’ Long format\n",
      "   - Useful for: Normalizing data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# C. UNPIVOT\n",
    "print(\"\\nðŸ“Š C. UNPIVOT (Transform columns to rows)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# First create a pivoted table\n",
    "pivoted = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        SUM(CASE WHEN country = 'USA' THEN 1 ELSE 0 END) as USA,\n",
    "        SUM(CASE WHEN country = 'UK' THEN 1 ELSE 0 END) as UK,\n",
    "        SUM(CASE WHEN country = 'Germany' THEN 1 ELSE 0 END) as Germany\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "\"\"\")\n",
    "\n",
    "pivoted.createOrReplaceTempView(\"pivoted_data\")\n",
    "\n",
    "print(\"Pivoted data (wide format):\")\n",
    "pivoted.show()\n",
    "\n",
    "print(\"\\nUnpivoted data (long format):\")\n",
    "result_unpivot = spark.sql(\"\"\"\n",
    "    SELECT department, country, employee_count\n",
    "    FROM pivoted_data\n",
    "    LATERAL VIEW STACK(3,\n",
    "        'USA', USA,\n",
    "        'UK', UK,\n",
    "        'Germany', Germany\n",
    "    ) AS country, employee_count\n",
    "    ORDER BY department, country\n",
    "\"\"\")\n",
    "\n",
    "result_unpivot.show()\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’¡ UNPIVOT:\n",
    "   - Transform columns to rows\n",
    "   - Use LATERAL VIEW STACK\n",
    "   - Wide format â†’ Long format\n",
    "   - Useful for: Normalizing data\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“ˆ **6. ADVANCED AGGREGATIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ“ˆ 6. ADVANCED AGGREGATIONS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š A. GROUPING SETS (Multiple GROUP BY in one query)\n",
      "--------------------------------------------------------------------------------\n",
      "+-----------+-------+--------------+----------+\n",
      "| department|country|employee_count|avg_salary|\n",
      "+-----------+-------+--------------+----------+\n",
      "|Engineering| Canada|            16|  95807.94|\n",
      "|Engineering| France|            15|  92900.27|\n",
      "|Engineering|Germany|             9|  91312.56|\n",
      "|Engineering|     UK|            13|  95112.15|\n",
      "|Engineering|    USA|            16|  88251.63|\n",
      "|Engineering|   NULL|            69|   92706.2|\n",
      "|    Finance| Canada|            23|   84621.0|\n",
      "|    Finance| France|            25|   83208.4|\n",
      "|    Finance|Germany|            11|  78902.27|\n",
      "|    Finance|     UK|            11|  88459.09|\n",
      "|    Finance|    USA|            14|  81014.43|\n",
      "|    Finance|   NULL|            84|  83353.21|\n",
      "|         HR| Canada|            10|   68230.9|\n",
      "|         HR| France|            13|  63171.69|\n",
      "|         HR|Germany|            18|  73553.28|\n",
      "|         HR|     UK|            15|  72554.53|\n",
      "|         HR|    USA|            12|  67122.83|\n",
      "|         HR|   NULL|            68|  69430.76|\n",
      "|  Marketing| Canada|            16|  78761.44|\n",
      "|  Marketing| France|            10|   81676.5|\n",
      "|  Marketing|Germany|            17|  76746.76|\n",
      "|  Marketing|     UK|            11|  71680.55|\n",
      "|  Marketing|    USA|            22|  73450.77|\n",
      "|  Marketing|   NULL|            76|  76132.18|\n",
      "|      Sales| Canada|            15|   78152.4|\n",
      "|      Sales| France|            12|  80928.58|\n",
      "|      Sales|Germany|            18|  85533.06|\n",
      "|      Sales|     UK|            18|  81241.56|\n",
      "|      Sales|    USA|            16|  75751.13|\n",
      "|      Sales|   NULL|            79|  80473.29|\n",
      "+-----------+-------+--------------+----------+\n",
      "only showing top 30 rows\n",
      "\n",
      "\n",
      "ðŸ’¡ GROUPING SETS:\n",
      "   - Multiple GROUP BY in one query\n",
      "   - More efficient than UNION\n",
      "   - NULL means \"all\" (subtotal/grand total)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ“ˆ 6. ADVANCED AGGREGATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# A. GROUPING SETS\n",
    "print(\"\\nðŸ“Š A. GROUPING SETS (Multiple GROUP BY in one query)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "result_grouping_sets = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        country,\n",
    "        COUNT(*) as employee_count,\n",
    "        ROUND(AVG(salary), 2) as avg_salary\n",
    "    FROM employees\n",
    "    WHERE status = 'Active'\n",
    "    GROUP BY GROUPING SETS (\n",
    "        (department, country),\n",
    "        (department),\n",
    "        (country),\n",
    "        ()\n",
    "    )\n",
    "    ORDER BY department NULLS LAST, country NULLS LAST\n",
    "\"\"\")\n",
    "\n",
    "result_grouping_sets.show(30)\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’¡ GROUPING SETS:\n",
    "   - Multiple GROUP BY in one query\n",
    "   - More efficient than UNION\n",
    "   - NULL means \"all\" (subtotal/grand total)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š B. ROLLUP (Hierarchical subtotals)\n",
      "--------------------------------------------------------------------------------\n",
      "+-----------+-------+--------------+----------+------------+\n",
      "| department|country|employee_count|avg_salary|total_salary|\n",
      "+-----------+-------+--------------+----------+------------+\n",
      "|Engineering| Canada|            16|  95807.94|     1532927|\n",
      "|Engineering| France|            15|  92900.27|     1393504|\n",
      "|Engineering|Germany|             9|  91312.56|      821813|\n",
      "|Engineering|     UK|            13|  95112.15|     1236458|\n",
      "|Engineering|    USA|            16|  88251.63|     1412026|\n",
      "|Engineering|   NULL|            69|   92706.2|     6396728|\n",
      "|    Finance| Canada|            23|   84621.0|     1946283|\n",
      "|    Finance| France|            25|   83208.4|     2080210|\n",
      "|    Finance|Germany|            11|  78902.27|      867925|\n",
      "|    Finance|     UK|            11|  88459.09|      973050|\n",
      "|    Finance|    USA|            14|  81014.43|     1134202|\n",
      "|    Finance|   NULL|            84|  83353.21|     7001670|\n",
      "|         HR| Canada|            10|   68230.9|      682309|\n",
      "|         HR| France|            13|  63171.69|      821232|\n",
      "|         HR|Germany|            18|  73553.28|     1323959|\n",
      "|         HR|     UK|            15|  72554.53|     1088318|\n",
      "|         HR|    USA|            12|  67122.83|      805474|\n",
      "|         HR|   NULL|            68|  69430.76|     4721292|\n",
      "|  Marketing| Canada|            16|  78761.44|     1260183|\n",
      "|  Marketing| France|            10|   81676.5|      816765|\n",
      "|  Marketing|Germany|            17|  76746.76|     1304695|\n",
      "|  Marketing|     UK|            11|  71680.55|      788486|\n",
      "|  Marketing|    USA|            22|  73450.77|     1615917|\n",
      "|  Marketing|   NULL|            76|  76132.18|     5786046|\n",
      "|      Sales| Canada|            15|   78152.4|     1172286|\n",
      "|      Sales| France|            12|  80928.58|      971143|\n",
      "|      Sales|Germany|            18|  85533.06|     1539595|\n",
      "|      Sales|     UK|            18|  81241.56|     1462348|\n",
      "|      Sales|    USA|            16|  75751.13|     1212018|\n",
      "|      Sales|   NULL|            79|  80473.29|     6357390|\n",
      "+-----------+-------+--------------+----------+------------+\n",
      "only showing top 30 rows\n",
      "\n",
      "\n",
      "ðŸ’¡ ROLLUP:\n",
      "   - Hierarchical aggregation\n",
      "   - ROLLUP(A, B) creates:\n",
      "     * (A, B) - Detail level\n",
      "     * (A)    - Subtotal by A\n",
      "     * ()     - Grand total\n",
      "   - Useful for: Reports with subtotals\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# B. ROLLUP\n",
    "print(\"\\nðŸ“Š B. ROLLUP (Hierarchical subtotals)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "result_rollup = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        country,\n",
    "        COUNT(*) as employee_count,\n",
    "        ROUND(AVG(salary), 2) as avg_salary,\n",
    "        ROUND(SUM(salary), 2) as total_salary\n",
    "    FROM employees\n",
    "    WHERE status = 'Active'\n",
    "    GROUP BY ROLLUP(department, country)\n",
    "    ORDER BY department NULLS LAST, country NULLS LAST\n",
    "\"\"\")\n",
    "\n",
    "result_rollup.show(30)\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’¡ ROLLUP:\n",
    "   - Hierarchical aggregation\n",
    "   - ROLLUP(A, B) creates:\n",
    "     * (A, B) - Detail level\n",
    "     * (A)    - Subtotal by A\n",
    "     * ()     - Grand total\n",
    "   - Useful for: Reports with subtotals\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š C. CUBE (All combinations)\n",
      "--------------------------------------------------------------------------------\n",
      "+-----------+-------+--------------+----------+\n",
      "| department|country|employee_count|avg_salary|\n",
      "+-----------+-------+--------------+----------+\n",
      "|Engineering| Canada|            16|  95807.94|\n",
      "|Engineering| France|            15|  92900.27|\n",
      "|Engineering|Germany|             9|  91312.56|\n",
      "|Engineering|     UK|            13|  95112.15|\n",
      "|Engineering|    USA|            16|  88251.63|\n",
      "|Engineering|   NULL|            69|   92706.2|\n",
      "|    Finance| Canada|            23|   84621.0|\n",
      "|    Finance| France|            25|   83208.4|\n",
      "|    Finance|Germany|            11|  78902.27|\n",
      "|    Finance|     UK|            11|  88459.09|\n",
      "|    Finance|    USA|            14|  81014.43|\n",
      "|    Finance|   NULL|            84|  83353.21|\n",
      "|         HR| Canada|            10|   68230.9|\n",
      "|         HR| France|            13|  63171.69|\n",
      "|         HR|Germany|            18|  73553.28|\n",
      "|         HR|     UK|            15|  72554.53|\n",
      "|         HR|    USA|            12|  67122.83|\n",
      "|         HR|   NULL|            68|  69430.76|\n",
      "|  Marketing| Canada|            16|  78761.44|\n",
      "|  Marketing| France|            10|   81676.5|\n",
      "|  Marketing|Germany|            17|  76746.76|\n",
      "|  Marketing|     UK|            11|  71680.55|\n",
      "|  Marketing|    USA|            22|  73450.77|\n",
      "|  Marketing|   NULL|            76|  76132.18|\n",
      "|      Sales| Canada|            15|   78152.4|\n",
      "|      Sales| France|            12|  80928.58|\n",
      "|      Sales|Germany|            18|  85533.06|\n",
      "|      Sales|     UK|            18|  81241.56|\n",
      "|      Sales|    USA|            16|  75751.13|\n",
      "|      Sales|   NULL|            79|  80473.29|\n",
      "|       NULL| Canada|            80|  82424.85|\n",
      "|       NULL| France|            75|  81104.72|\n",
      "|       NULL|Germany|            73|   80246.4|\n",
      "|       NULL|     UK|            68|  81597.94|\n",
      "|       NULL|    USA|            80|  77245.46|\n",
      "|       NULL|   NULL|           376|  80487.04|\n",
      "+-----------+-------+--------------+----------+\n",
      "\n",
      "\n",
      "ðŸ’¡ CUBE:\n",
      "   - All possible combinations\n",
      "   - CUBE(A, B) creates:\n",
      "     * (A, B) - Both\n",
      "     * (A)    - A only\n",
      "     * (B)    - B only\n",
      "     * ()     - Grand total\n",
      "   - More combinations than ROLLUP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# C. CUBE\n",
    "print(\"\\nðŸ“Š C. CUBE (All combinations)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "result_cube = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        country,\n",
    "        COUNT(*) as employee_count,\n",
    "        ROUND(AVG(salary), 2) as avg_salary\n",
    "    FROM employees\n",
    "    WHERE status = 'Active'\n",
    "    GROUP BY CUBE(department, country)\n",
    "    ORDER BY department NULLS LAST, country NULLS LAST\n",
    "\"\"\")\n",
    "\n",
    "result_cube.show(40)\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’¡ CUBE:\n",
    "   - All possible combinations\n",
    "   - CUBE(A, B) creates:\n",
    "     * (A, B) - Both\n",
    "     * (A)    - A only\n",
    "     * (B)    - B only\n",
    "     * ()     - Grand total\n",
    "   - More combinations than ROLLUP\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š D. GROUPING function (Identify subtotal rows)\n",
      "--------------------------------------------------------------------------------\n",
      "+-----------+-------+--------------+----------+-------------+----------------+----------------+\n",
      "| department|country|employee_count|avg_salary|is_dept_total|is_country_total|           level|\n",
      "+-----------+-------+--------------+----------+-------------+----------------+----------------+\n",
      "|Engineering| Canada|            16|  95807.94|            0|               0|          Detail|\n",
      "|Engineering| France|            15|  92900.27|            0|               0|          Detail|\n",
      "|Engineering|Germany|             9|  91312.56|            0|               0|          Detail|\n",
      "|Engineering|     UK|            13|  95112.15|            0|               0|          Detail|\n",
      "|Engineering|    USA|            16|  88251.63|            0|               0|          Detail|\n",
      "|Engineering|   NULL|            69|   92706.2|            0|               1|Department Total|\n",
      "|    Finance| Canada|            23|   84621.0|            0|               0|          Detail|\n",
      "|    Finance| France|            25|   83208.4|            0|               0|          Detail|\n",
      "|    Finance|Germany|            11|  78902.27|            0|               0|          Detail|\n",
      "|    Finance|     UK|            11|  88459.09|            0|               0|          Detail|\n",
      "|    Finance|    USA|            14|  81014.43|            0|               0|          Detail|\n",
      "|    Finance|   NULL|            84|  83353.21|            0|               1|Department Total|\n",
      "|         HR| Canada|            10|   68230.9|            0|               0|          Detail|\n",
      "|         HR| France|            13|  63171.69|            0|               0|          Detail|\n",
      "|         HR|Germany|            18|  73553.28|            0|               0|          Detail|\n",
      "|         HR|     UK|            15|  72554.53|            0|               0|          Detail|\n",
      "|         HR|    USA|            12|  67122.83|            0|               0|          Detail|\n",
      "|         HR|   NULL|            68|  69430.76|            0|               1|Department Total|\n",
      "|  Marketing| Canada|            16|  78761.44|            0|               0|          Detail|\n",
      "|  Marketing| France|            10|   81676.5|            0|               0|          Detail|\n",
      "|  Marketing|Germany|            17|  76746.76|            0|               0|          Detail|\n",
      "|  Marketing|     UK|            11|  71680.55|            0|               0|          Detail|\n",
      "|  Marketing|    USA|            22|  73450.77|            0|               0|          Detail|\n",
      "|  Marketing|   NULL|            76|  76132.18|            0|               1|Department Total|\n",
      "|      Sales| Canada|            15|   78152.4|            0|               0|          Detail|\n",
      "|      Sales| France|            12|  80928.58|            0|               0|          Detail|\n",
      "|      Sales|Germany|            18|  85533.06|            0|               0|          Detail|\n",
      "|      Sales|     UK|            18|  81241.56|            0|               0|          Detail|\n",
      "|      Sales|    USA|            16|  75751.13|            0|               0|          Detail|\n",
      "|      Sales|   NULL|            79|  80473.29|            0|               1|Department Total|\n",
      "+-----------+-------+--------------+----------+-------------+----------------+----------------+\n",
      "only showing top 30 rows\n",
      "\n",
      "\n",
      "ðŸ’¡ GROUPING function:\n",
      "   - Returns 1 if column is aggregated (NULL is subtotal)\n",
      "   - Returns 0 if column is not aggregated (NULL is real NULL)\n",
      "   - Useful for: Identifying subtotal rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# D. GROUPING function\n",
    "print(\"\\nðŸ“Š D. GROUPING function (Identify subtotal rows)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "result_grouping = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        country,\n",
    "        COUNT(*) as employee_count,\n",
    "        ROUND(AVG(salary), 2) as avg_salary,\n",
    "        GROUPING(department) as is_dept_total,\n",
    "        GROUPING(country) as is_country_total,\n",
    "        CASE \n",
    "            WHEN GROUPING(department) = 1 AND GROUPING(country) = 1 THEN 'Grand Total'\n",
    "            WHEN GROUPING(department) = 1 THEN 'Country Total'\n",
    "            WHEN GROUPING(country) = 1 THEN 'Department Total'\n",
    "            ELSE 'Detail'\n",
    "        END as level\n",
    "    FROM employees\n",
    "    WHERE status = 'Active'\n",
    "    GROUP BY ROLLUP(department, country)\n",
    "    ORDER BY department NULLS LAST, country NULLS LAST\n",
    "\"\"\")\n",
    "\n",
    "result_grouping.show(30)\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’¡ GROUPING function:\n",
    "   - Returns 1 if column is aggregated (NULL is subtotal)\n",
    "   - Returns 0 if column is not aggregated (NULL is real NULL)\n",
    "   - Useful for: Identifying subtotal rows\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš¡ **7. PERFORMANCE TIPS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "âš¡ 7. SQL PERFORMANCE TIPS\n",
      "================================================================================\n",
      "\n",
      "ðŸ’¡ PERFORMANCE TIPS:\n",
      "\n",
      "1. FILTER EARLY (Predicate Pushdown)\n",
      "   âœ… Good:\n",
      "      SELECT * FROM large_table\n",
      "      WHERE date = '2024-01-01'\n",
      "      \n",
      "   âŒ Bad:\n",
      "      SELECT * FROM (\n",
      "          SELECT * FROM large_table\n",
      "      ) WHERE date = '2024-01-01'\n",
      "\n",
      "2. SELECT ONLY NEEDED COLUMNS (Column Pruning)\n",
      "   âœ… Good:\n",
      "      SELECT id, name, salary FROM employees\n",
      "      \n",
      "   âŒ Bad:\n",
      "      SELECT * FROM employees\n",
      "\n",
      "3. USE BROADCAST JOIN for small tables\n",
      "   âœ… Good:\n",
      "      SELECT /*+ BROADCAST(small_table) */\n",
      "      FROM large_table JOIN small_table\n",
      "      \n",
      "4. AVOID CROSS JOIN\n",
      "   âŒ Bad:\n",
      "      SELECT * FROM table1 CROSS JOIN table2\n",
      "      \n",
      "   âœ… Good:\n",
      "      SELECT * FROM table1 JOIN table2 ON table1.id = table2.id\n",
      "\n",
      "5. USE WINDOW FUNCTIONS instead of self-joins\n",
      "   âœ… Good:\n",
      "      SELECT *, ROW_NUMBER() OVER (PARTITION BY dept ORDER BY salary DESC)\n",
      "      \n",
      "   âŒ Bad:\n",
      "      SELECT * FROM employees e1\n",
      "      WHERE (SELECT COUNT(*) FROM employees e2 \n",
      "             WHERE e2.dept = e1.dept AND e2.salary > e1.salary) < 3\n",
      "\n",
      "6. CACHE frequently used tables\n",
      "   âœ… Good:\n",
      "      CACHE TABLE employees;\n",
      "      UNCACHE TABLE employees;\n",
      "\n",
      "7. USE EXPLAIN to check execution plan\n",
      "   âœ… Always:\n",
      "      EXPLAIN SELECT * FROM ...\n",
      "\n",
      "\n",
      "ðŸ“Š DEMO: Broadcast Join Hint\n",
      "--------------------------------------------------------------------------------\n",
      "Without broadcast hint:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [name#1, department#3, division#1276]\n",
      "   +- SortMergeJoin [department#3], [department#1275], Inner\n",
      "      :- Sort [department#3 ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(department#3, 200), ENSURE_REQUIREMENTS, [plan_id=2467]\n",
      "      :     +- Project [name#1, department#3]\n",
      "      :        +- Filter isnotnull(department#3)\n",
      "      :           +- Scan ExistingRDD[employee_id#0,name#1,age#2L,department#3,country#4,salary#5L,manager_id#6,status#7,hire_date#8]\n",
      "      +- Sort [department#1275 ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(department#1275, 200), ENSURE_REQUIREMENTS, [plan_id=2468]\n",
      "            +- Filter isnotnull(department#1275)\n",
      "               +- Scan ExistingRDD[department#1275,division#1276]\n",
      "\n",
      "\n",
      "\n",
      "With broadcast hint:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [name#1, department#3, division#1276]\n",
      "   +- BroadcastHashJoin [department#3], [department#1275], Inner, BuildRight, false\n",
      "      :- Project [name#1, department#3]\n",
      "      :  +- Filter isnotnull(department#3)\n",
      "      :     +- Scan ExistingRDD[employee_id#0,name#1,age#2L,department#3,country#4,salary#5L,manager_id#6,status#7,hire_date#8]\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=2501]\n",
      "         +- Filter isnotnull(department#1275)\n",
      "            +- Scan ExistingRDD[department#1275,division#1276]\n",
      "\n",
      "\n",
      "\n",
      "ðŸ’¡ Broadcast Hint:\n",
      "   - Use /*+ BROADCAST(table) */ before SELECT\n",
      "   - Forces small table to be broadcast\n",
      "   - Avoids shuffle\n",
      "   - Much faster for small dimension tables\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"âš¡ 7. SQL PERFORMANCE TIPS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’¡ PERFORMANCE TIPS:\n",
    "\n",
    "1. FILTER EARLY (Predicate Pushdown)\n",
    "   âœ… Good:\n",
    "      SELECT * FROM large_table\n",
    "      WHERE date = '2024-01-01'\n",
    "      \n",
    "   âŒ Bad:\n",
    "      SELECT * FROM (\n",
    "          SELECT * FROM large_table\n",
    "      ) WHERE date = '2024-01-01'\n",
    "\n",
    "2. SELECT ONLY NEEDED COLUMNS (Column Pruning)\n",
    "   âœ… Good:\n",
    "      SELECT id, name, salary FROM employees\n",
    "      \n",
    "   âŒ Bad:\n",
    "      SELECT * FROM employees\n",
    "\n",
    "3. USE BROADCAST JOIN for small tables\n",
    "   âœ… Good:\n",
    "      SELECT /*+ BROADCAST(small_table) */\n",
    "      FROM large_table JOIN small_table\n",
    "      \n",
    "4. AVOID CROSS JOIN\n",
    "   âŒ Bad:\n",
    "      SELECT * FROM table1 CROSS JOIN table2\n",
    "      \n",
    "   âœ… Good:\n",
    "      SELECT * FROM table1 JOIN table2 ON table1.id = table2.id\n",
    "\n",
    "5. USE WINDOW FUNCTIONS instead of self-joins\n",
    "   âœ… Good:\n",
    "      SELECT *, ROW_NUMBER() OVER (PARTITION BY dept ORDER BY salary DESC)\n",
    "      \n",
    "   âŒ Bad:\n",
    "      SELECT * FROM employees e1\n",
    "      WHERE (SELECT COUNT(*) FROM employees e2 \n",
    "             WHERE e2.dept = e1.dept AND e2.salary > e1.salary) < 3\n",
    "\n",
    "6. CACHE frequently used tables\n",
    "   âœ… Good:\n",
    "      CACHE TABLE employees;\n",
    "      UNCACHE TABLE employees;\n",
    "\n",
    "7. USE EXPLAIN to check execution plan\n",
    "   âœ… Always:\n",
    "      EXPLAIN SELECT * FROM ...\n",
    "\"\"\")\n",
    "\n",
    "# Demo: Broadcast hint\n",
    "print(\"\\nðŸ“Š DEMO: Broadcast Join Hint\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Small lookup table\n",
    "lookup = spark.createDataFrame([\n",
    "    (\"Engineering\", \"Tech\"),\n",
    "    (\"Sales\", \"Business\"),\n",
    "    (\"Marketing\", \"Business\"),\n",
    "    (\"HR\", \"Support\"),\n",
    "    (\"Finance\", \"Support\")\n",
    "], [\"department\", \"division\"])\n",
    "\n",
    "lookup.createOrReplaceTempView(\"lookup\")\n",
    "\n",
    "# Without broadcast hint\n",
    "print(\"Without broadcast hint:\")\n",
    "result_no_hint = spark.sql(\"\"\"\n",
    "    SELECT e.name, e.department, l.division\n",
    "    FROM employees e\n",
    "    JOIN lookup l ON e.department = l.department\n",
    "\"\"\")\n",
    "result_no_hint.explain()\n",
    "\n",
    "# With broadcast hint\n",
    "print(\"\\nWith broadcast hint:\")\n",
    "result_with_hint = spark.sql(\"\"\"\n",
    "    SELECT /*+ BROADCAST(l) */ e.name, e.department, l.division\n",
    "    FROM employees e\n",
    "    JOIN lookup l ON e.department = l.department\n",
    "\"\"\")\n",
    "result_with_hint.explain()\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’¡ Broadcast Hint:\n",
    "   - Use /*+ BROADCAST(table) */ before SELECT\n",
    "   - Forces small table to be broadcast\n",
    "   - Avoids shuffle\n",
    "   - Much faster for small dimension tables\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ“ **KEY TAKEAWAYS**\n",
    "\n",
    "### **âœ… What You Learned:**\n",
    "\n",
    "1. **Window Functions**\n",
    "   - ROW_NUMBER(), RANK(), DENSE_RANK()\n",
    "   - LAG(), LEAD()\n",
    "   - Running totals, moving averages\n",
    "   - FIRST_VALUE(), LAST_VALUE()\n",
    "   - NTILE(), PERCENT_RANK()\n",
    "\n",
    "2. **Complex Joins**\n",
    "   - Self Join (hierarchical data)\n",
    "   - Cross Join (cartesian product)\n",
    "   - Multiple Joins\n",
    "\n",
    "3. **Set Operations**\n",
    "   - UNION / UNION ALL\n",
    "   - INTERSECT\n",
    "   - EXCEPT\n",
    "\n",
    "4. **Pivot & Unpivot**\n",
    "   - PIVOT: Rows â†’ Columns\n",
    "   - UNPIVOT: Columns â†’ Rows (LATERAL VIEW STACK)\n",
    "   - CASE WHEN for flexible pivoting\n",
    "\n",
    "5. **Advanced Aggregations**\n",
    "   - GROUPING SETS\n",
    "   - ROLLUP (hierarchical)\n",
    "   - CUBE (all combinations)\n",
    "   - GROUPING() function\n",
    "\n",
    "6. **Performance Tips**\n",
    "   - Filter early\n",
    "   - Select only needed columns\n",
    "   - Use broadcast join for small tables\n",
    "   - Avoid cross join\n",
    "   - Use window functions instead of self-joins\n",
    "   - Cache frequently used tables\n",
    "\n",
    "### **ðŸ“Š Quick Reference:**\n",
    "\n",
    "```sql\n",
    "-- Window Function\n",
    "SELECT \n",
    "    col1,\n",
    "    ROW_NUMBER() OVER (PARTITION BY col2 ORDER BY col3) as rn\n",
    "FROM table\n",
    "\n",
    "-- Self Join\n",
    "SELECT e.*, m.name as manager_name\n",
    "FROM employees e\n",
    "LEFT JOIN employees m ON e.manager_id = m.employee_id\n",
    "\n",
    "-- Pivot\n",
    "SELECT * FROM table\n",
    "PIVOT (\n",
    "    COUNT(*) FOR country IN ('USA', 'UK')\n",
    ")\n",
    "\n",
    "-- Rollup\n",
    "SELECT dept, country, COUNT(*)\n",
    "FROM employees\n",
    "GROUP BY ROLLUP(dept, country)\n",
    "```\n",
    "\n",
    "### **ðŸš€ Next:** Day 5 - Lesson 3: Catalog & Metadata\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark session stopped\n",
      "\n",
      "ðŸŽ‰ DAY 5 - LESSON 2 COMPLETED!\n",
      "\n",
      "ðŸ’¡ Remember:\n",
      "   - Window functions are powerful for analytics\n",
      "   - Use ROLLUP/CUBE for reports with subtotals\n",
      "   - Broadcast small tables for better performance\n",
      "   - EXPLAIN your queries to check execution plan\n",
      "   - SQL and DataFrame API have same performance!\n",
      "\n",
      "ðŸ”¥ Quote: 'Window functions: The Swiss Army knife of SQL!' ðŸªŸ\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "spark.catalog.clearCache()\n",
    "spark.stop()\n",
    "\n",
    "print(\"âœ… Spark session stopped\")\n",
    "print(\"\\nðŸŽ‰ DAY 5 - LESSON 2 COMPLETED!\")\n",
    "print(\"\\nðŸ’¡ Remember:\")\n",
    "print(\"   - Window functions are powerful for analytics\")\n",
    "print(\"   - Use ROLLUP/CUBE for reports with subtotals\")\n",
    "print(\"   - Broadcast small tables for better performance\")\n",
    "print(\"   - EXPLAIN your queries to check execution plan\")\n",
    "print(\"   - SQL and DataFrame API have same performance!\")\n",
    "print(\"\\nðŸ”¥ Quote: 'Window functions: The Swiss Army knife of SQL!' ðŸªŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
