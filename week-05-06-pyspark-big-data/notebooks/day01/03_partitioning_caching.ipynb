{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAY 1 - NOTEBOOK 3: PARTITIONING & CACHING\n",
    "\n",
    "Version n√†y ƒë√£ fix:\n",
    "- ‚úÖ L·ªói sum() conflict\n",
    "- ‚úÖ Data distribution issues\n",
    "- ‚úÖ Gi·∫£i th√≠ch r√µ r√†ng c∆° ch·∫ø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark Session Created\n",
      "   Default Parallelism: 2\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, count, when\n",
    "from pyspark import StorageLevel\n",
    "import time\n",
    "\n",
    "# Stop existing\n",
    "try:\n",
    "    spark.stop()\n",
    "    time.sleep(2)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Day1-Partitioning-Fixed\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"512m\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.cores.max\", \"2\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark Session Created\")\n",
    "print(f\"   Default Parallelism: {spark.sparkContext.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 1: HI·ªÇU V·ªÄ DEFAULT PARALLELISM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Default Parallelism trong cluster c·ªßa b·∫°n:\n",
      "============================================================\n",
      "Default Parallelism: 2\n",
      "\n",
      "üí° Gi·∫£i th√≠ch:\n",
      "   ‚Ä¢ B·∫°n c√≥ 2 workers\n",
      "   ‚Ä¢ M·ªói worker c√≥ 1 core\n",
      "   ‚Ä¢ Total cores = 2 √ó 1 = 2\n",
      "   ‚Ä¢ Default parallelism = 2\n",
      "\n",
      "‚ö†Ô∏è  Khi t·∫°o DataFrame t·ª´ Python list:\n",
      "   ‚Üí Spark t·ª± ƒë·ªông t·∫°o 2 partitions\n"
     ]
    }
   ],
   "source": [
    "print(\"üìå Default Parallelism trong cluster c·ªßa b·∫°n:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "parallelism = spark.sparkContext.defaultParallelism\n",
    "print(f\"Default Parallelism: {parallelism}\")\n",
    "print(f\"\\nüí° Gi·∫£i th√≠ch:\")\n",
    "print(f\"   ‚Ä¢ B·∫°n c√≥ 2 workers\")\n",
    "print(f\"   ‚Ä¢ M·ªói worker c√≥ 1 core\")\n",
    "print(f\"   ‚Ä¢ Total cores = 2 √ó 1 = 2\")\n",
    "print(f\"   ‚Ä¢ Default parallelism = {parallelism}\")\n",
    "print(f\"\\n‚ö†Ô∏è  Khi t·∫°o DataFrame t·ª´ Python list:\")\n",
    "print(f\"   ‚Üí Spark t·ª± ƒë·ªông t·∫°o {parallelism} partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 2: T·∫†O DATAFRAME V√Ä KI·ªÇM TRA PARTITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DataFrame Info:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Total rows: 1000\n",
      "   Number of partitions: 2\n",
      "\n",
      "üí° V√¨ default.parallelism = 2\n",
      "   ‚Üí Spark t·∫°o 2 partitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# T·∫°o DataFrame t·ª´ Python list\n",
    "data = [(i, f\"Name_{i}\", i * 10) for i in range(1, 1001)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"value\"])\n",
    "\n",
    "print(\"üìä DataFrame Info:\")\n",
    "print(f\"   Total rows: {df.count()}\")\n",
    "print(f\"   Number of partitions: {df.rdd.getNumPartitions()}\")\n",
    "print(f\"\\nüí° V√¨ default.parallelism = {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"   ‚Üí Spark t·∫°o {df.rdd.getNumPartitions()} partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Data distribution:\n",
      "============================================================\n",
      "Partition 0:  500 rows ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "Partition 1:  500 rows ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "============================================================\n",
      "\n",
      "‚úÖ Data ph√¢n b·ªï ƒë·ªÅu v√†o 2 partitions\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ FIXED: D√πng len(list()) thay v√¨ sum()\n",
    "def get_partition_distribution(dataframe):\n",
    "    \"\"\"Get row count per partition\"\"\"\n",
    "    partition_counts = dataframe.rdd.mapPartitionsWithIndex(\n",
    "        lambda idx, it: [(idx, len(list(it)))]\n",
    "    ).collect()\n",
    "    return partition_counts\n",
    "\n",
    "print(\"üìå Data distribution:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "partition_counts = get_partition_distribution(df)\n",
    "\n",
    "for partition_id, count in partition_counts:\n",
    "    bar = \"‚ñà\" * (count // 20)\n",
    "    print(f\"Partition {partition_id}: {count:4d} rows {bar}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚úÖ Data ph√¢n b·ªï ƒë·ªÅu v√†o 2 partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 3: REPARTITION - TƒÇNG S·ªê PARTITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Repartition: TƒÉng t·ª´ 2 ‚Üí 8 partitions\n",
      "============================================================\n",
      "Before: 2 partitions\n",
      "After:  8 partitions\n",
      "\n",
      "üìä Data distribution sau repartition:\n",
      "============================================================\n",
      "Partition 0:  124 rows ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "Partition 1:  124 rows ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "Partition 2:  126 rows ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "Partition 3:  126 rows ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "Partition 4:  126 rows ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "Partition 5:  126 rows ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "Partition 6:  124 rows ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "Partition 7:  124 rows ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "============================================================\n",
      "\n",
      "‚úÖ Data ƒë√£ ph√¢n b·ªï ƒë·ªÅu v√†o 8 partitions!\n",
      "üí° repartition() th·ª±c hi·ªán full shuffle ƒë·ªÉ ph√¢n b·ªï ƒë·ªÅu\n"
     ]
    }
   ],
   "source": [
    "print(\"üìå Repartition: TƒÉng t·ª´ 2 ‚Üí 8 partitions\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Before: {df.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "# Repartition to 8\n",
    "df_8 = df.repartition(8)\n",
    "\n",
    "print(f\"After:  {df_8.rdd.getNumPartitions()} partitions\")\n",
    "print(\"\\nüìä Data distribution sau repartition:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "partition_counts = get_partition_distribution(df_8)\n",
    "\n",
    "for partition_id, count in partition_counts:\n",
    "    bar = \"‚ñà\" * (count // 5)\n",
    "    print(f\"Partition {partition_id}: {count:4d} rows {bar}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚úÖ Data ƒë√£ ph√¢n b·ªï ƒë·ªÅu v√†o 8 partitions!\")\n",
    "print(\"üí° repartition() th·ª±c hi·ªán full shuffle ƒë·ªÉ ph√¢n b·ªï ƒë·ªÅu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 4: COALESCE - GI·∫¢M S·ªê PARTITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Coalesce: Gi·∫£m t·ª´ 8 ‚Üí 3 partitions\n",
      "============================================================\n",
      "Before: 8 partitions\n",
      "After:  3 partitions\n",
      "\n",
      "üìä Data distribution sau coalesce:\n",
      "============================================================\n",
      "Partition 0:  374 rows ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "Partition 1:  374 rows ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "Partition 2:  252 rows ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "============================================================\n",
      "\n",
      "‚ö†Ô∏è  Data c√≥ th·ªÉ kh√¥ng ƒë·ªÅu (coalesce ch·ªâ g·ªôp partitions)\n",
      "üí° coalesce() minimal shuffle, nhanh h∆°n repartition()\n"
     ]
    }
   ],
   "source": [
    "print(\"üìå Coalesce: Gi·∫£m t·ª´ 8 ‚Üí 3 partitions\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Before: {df_8.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "# Coalesce to 3\n",
    "df_3 = df_8.coalesce(3)\n",
    "\n",
    "print(f\"After:  {df_3.rdd.getNumPartitions()} partitions\")\n",
    "print(\"\\nüìä Data distribution sau coalesce:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "partition_counts = get_partition_distribution(df_3)\n",
    "\n",
    "for partition_id, count in partition_counts:\n",
    "    bar = \"‚ñà\" * (count // 10)\n",
    "    print(f\"Partition {partition_id}: {count:4d} rows {bar}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚ö†Ô∏è  Data c√≥ th·ªÉ kh√¥ng ƒë·ªÅu (coalesce ch·ªâ g·ªôp partitions)\")\n",
    "print(\"üí° coalesce() minimal shuffle, nhanh h∆°n repartition()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 5: COALESCE KH√îNG TƒÇNG ƒê∆Ø·ª¢C PARTITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Demo: coalesce() KH√îNG th·ªÉ tƒÉng partitions\n",
      "============================================================\n",
      "Starting with: 2 partitions\n",
      "After coalesce(8): 2 partitions\n",
      "\n",
      "‚ùå V·∫´n 2 partitions! coalesce() CH·ªà GI·∫¢M, kh√¥ng tƒÉng\n",
      "\n",
      "‚úÖ Mu·ªën tƒÉng partitions ‚Üí d√πng repartition()\n",
      "After repartition(8): 8 partitions\n",
      "‚úÖ Th√†nh c√¥ng!\n"
     ]
    }
   ],
   "source": [
    "print(\"üìå Demo: coalesce() KH√îNG th·ªÉ tƒÉng partitions\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# B·∫Øt ƒë·∫ßu v·ªõi 2 partitions\n",
    "df_2 = df.coalesce(2)\n",
    "print(f\"Starting with: {df_2.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "# Th·ª≠ coalesce l√™n 8\n",
    "df_try_8 = df_2.coalesce(8)\n",
    "print(f\"After coalesce(8): {df_try_8.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "print(\"\\n‚ùå V·∫´n 2 partitions! coalesce() CH·ªà GI·∫¢M, kh√¥ng tƒÉng\")\n",
    "print(\"\\n‚úÖ Mu·ªën tƒÉng partitions ‚Üí d√πng repartition()\")\n",
    "\n",
    "df_real_8 = df_2.repartition(8)\n",
    "print(f\"After repartition(8): {df_real_8.rdd.getNumPartitions()} partitions\")\n",
    "print(\"‚úÖ Th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 6: REPARTITION BY COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created DataFrame with department column\n",
      "   Departments: ['Engineering', 'Sales', 'HR', 'Marketing']\n"
     ]
    }
   ],
   "source": [
    "# T·∫°o data v·ªõi department column\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "data_dept = []\n",
    "departments = [\"Engineering\", \"Sales\", \"HR\", \"Marketing\"]\n",
    "for i in range(1, 1001):\n",
    "    data_dept.append((\n",
    "        i,\n",
    "        f\"Name_{i}\",\n",
    "        i * 10,\n",
    "        random.choice(departments)\n",
    "    ))\n",
    "\n",
    "df_dept = spark.createDataFrame(data_dept, [\"id\", \"name\", \"value\", \"department\"])\n",
    "\n",
    "print(\"‚úÖ Created DataFrame with department column\")\n",
    "print(f\"   Departments: {departments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç PH√ÇN T√çCH HASH PARTITIONING - FULL DEMO\n",
      "======================================================================\n",
      "\n",
      "1Ô∏è‚É£ KI·ªÇM TRA DEPARTMENTS:\n",
      "----------------------------------------------------------------------\n",
      "+-----------+-----+\n",
      "| department|count|\n",
      "+-----------+-----+\n",
      "|Engineering|  243|\n",
      "|         HR|  250|\n",
      "|  Marketing|  264|\n",
      "|      Sales|  243|\n",
      "+-----------+-----+\n",
      "\n",
      "‚úÖ T·ªïng s·ªë departments: 4\n",
      "\n",
      "2Ô∏è‚É£ REPARTITION(4) - C√ì PARTITION TR·ªêNG:\n",
      "----------------------------------------------------------------------\n",
      "Partitions: 4\n",
      "Partition 0:  243 rows ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚úÖ\n",
      "Partition 1:  514 rows ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚úÖ\n",
      "Partition 2:  243 rows ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚úÖ\n",
      "Partition 3:    0 rows (empty) ‚ùå EMPTY\n",
      "\n",
      "‚ö†Ô∏è  1 partition(s) tr·ªëng\n",
      "üí° Nguy√™n nh√¢n: hash(department) % 4 kh√¥ng map v√†o t·∫•t c·∫£ partitions\n",
      "\n",
      "3Ô∏è‚É£ REPARTITION(4) - OPTIMAL:\n",
      "----------------------------------------------------------------------\n",
      "Partitions: 4\n",
      "Partition 0:  243 rows ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚úÖ\n",
      "Partition 1:  514 rows ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚úÖ\n",
      "Partition 2:  243 rows ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚úÖ\n",
      "Partition 3:    0 rows  ‚úÖ\n",
      "\n",
      "‚úÖ T·∫•t c·∫£ 4 partitions ƒë·ªÅu c√≥ data!\n",
      "\n",
      "4Ô∏è‚É£ DEPARTMENT MAPPING:\n",
      "----------------------------------------------------------------------\n",
      "Partition 0: {'Sales'} (243 rows)\n",
      "Partition 1: {'HR', 'Marketing'} (514 rows)\n",
      "Partition 2: {'Engineering'} (243 rows)\n",
      "\n",
      "5Ô∏è‚É£ PERFORMANCE TEST:\n",
      "----------------------------------------------------------------------\n",
      "repartition(4):              0.3994s\n",
      "repartition(4):              0.2752s\n",
      "‚ö° Optimal is 1.45x faster!\n",
      "\n",
      "======================================================================\n",
      "üìù K·∫æT LU·∫¨N:\n",
      "----------------------------------------------------------------------\n",
      "‚úÖ Partition tr·ªëng l√† B√åNH TH∆Ø·ªúNG v·ªõi hash partitioning\n",
      "‚úÖ Kh√¥ng ph·∫£i bug, l√† ƒë·∫∑c t√≠nh c·ªßa hash function\n",
      "‚úÖ Best practice: num_partitions = num_unique_values\n",
      "‚úÖ Hash partitioning ƒë·∫£m b·∫£o: c√πng value ‚Üí c√πng partition\n",
      "‚úÖ Benefit: T·ªëi ∆∞u cho groupBy, join operations\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç PH√ÇN T√çCH HASH PARTITIONING - FULL DEMO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Helper function\n",
    "def get_partition_distribution(dataframe):\n",
    "    \"\"\"Get row count per partition\"\"\"\n",
    "    return dataframe.rdd.mapPartitionsWithIndex(\n",
    "        lambda idx, it: [(idx, len(list(it)))]\n",
    "    ).collect()\n",
    "\n",
    "# 1. Ki·ªÉm tra departments\n",
    "print(\"\\n1Ô∏è‚É£ KI·ªÇM TRA DEPARTMENTS:\")\n",
    "print(\"-\"*70)\n",
    "dept_stats = df_dept.groupBy(\"department\").count().orderBy(\"department\")\n",
    "dept_stats.show()\n",
    "\n",
    "num_depts = df_dept.select(\"department\").distinct().count()\n",
    "print(f\"‚úÖ T·ªïng s·ªë departments: {num_depts}\")\n",
    "\n",
    "# 2. Repartition 4 (c√≥ partition tr·ªëng)\n",
    "print(\"\\n2Ô∏è‚É£ REPARTITION(4) - C√ì PARTITION TR·ªêNG:\")\n",
    "print(\"-\"*70)\n",
    "df_4 = df_dept.repartition(4, \"department\")\n",
    "print(f\"Partitions: {df_4.rdd.getNumPartitions()}\")\n",
    "\n",
    "counts_4 = get_partition_distribution(df_4)\n",
    "for pid, count in counts_4:\n",
    "    bar = \"‚ñà\" * (count // 10) if count > 0 else \"(empty)\"\n",
    "    status = \"‚úÖ\" if count > 0 else \"‚ùå EMPTY\"\n",
    "    print(f\"Partition {pid}: {count:4d} rows {bar} {status}\")\n",
    "\n",
    "empty_count = len([1 for _, c in counts_4 if c == 0])\n",
    "print(f\"\\n‚ö†Ô∏è  {empty_count} partition(s) tr·ªëng\")\n",
    "print(\"üí° Nguy√™n nh√¢n: hash(department) % 4 kh√¥ng map v√†o t·∫•t c·∫£ partitions\")\n",
    "\n",
    "# 3. Repartition optimal (= s·ªë departments)\n",
    "print(f\"\\n3Ô∏è‚É£ REPARTITION({num_depts}) - OPTIMAL:\")\n",
    "print(\"-\"*70)\n",
    "df_opt = df_dept.repartition(num_depts, \"department\")\n",
    "print(f\"Partitions: {df_opt.rdd.getNumPartitions()}\")\n",
    "\n",
    "counts_opt = get_partition_distribution(df_opt)\n",
    "for pid, count in counts_opt:\n",
    "    bar = \"‚ñà\" * (count // 10)\n",
    "    print(f\"Partition {pid}: {count:4d} rows {bar} ‚úÖ\")\n",
    "\n",
    "print(f\"\\n‚úÖ T·∫•t c·∫£ {num_depts} partitions ƒë·ªÅu c√≥ data!\")\n",
    "\n",
    "# 4. Xem department mapping\n",
    "print(\"\\n4Ô∏è‚É£ DEPARTMENT MAPPING:\")\n",
    "print(\"-\"*70)\n",
    "for i in range(num_depts):\n",
    "    partition_data = df_opt.rdd \\\n",
    "        .mapPartitionsWithIndex(lambda idx, it: [(idx, list(it))] if idx == i else []) \\\n",
    "        .collect()\n",
    "    \n",
    "    if partition_data and partition_data[0][1]:\n",
    "        rows = partition_data[0][1]\n",
    "        depts = set(row['department'] for row in rows[:100])\n",
    "        print(f\"Partition {i}: {depts} ({len(rows)} rows)\")\n",
    "\n",
    "# 5. Performance test\n",
    "print(\"\\n5Ô∏è‚É£ PERFORMANCE TEST:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "start = time.time()\n",
    "df_4.groupBy(\"department\").count().collect()\n",
    "time_4 = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "df_opt.groupBy(\"department\").count().collect()\n",
    "time_opt = time.time() - start\n",
    "\n",
    "print(f\"repartition(4):              {time_4:.4f}s\")\n",
    "print(f\"repartition({num_depts}):              {time_opt:.4f}s\")\n",
    "if time_4 > time_opt:\n",
    "    print(f\"‚ö° Optimal is {time_4/time_opt:.2f}x faster!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Similar performance (small dataset)\")\n",
    "\n",
    "# 6. K·∫øt lu·∫≠n\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìù K·∫æT LU·∫¨N:\")\n",
    "print(\"-\"*70)\n",
    "print(\"‚úÖ Partition tr·ªëng l√† B√åNH TH∆Ø·ªúNG v·ªõi hash partitioning\")\n",
    "print(\"‚úÖ Kh√¥ng ph·∫£i bug, l√† ƒë·∫∑c t√≠nh c·ªßa hash function\")\n",
    "print(\"‚úÖ Best practice: num_partitions = num_unique_values\")\n",
    "print(\"‚úÖ Hash partitioning ƒë·∫£m b·∫£o: c√πng value ‚Üí c√πng partition\")\n",
    "print(\"‚úÖ Benefit: T·ªëi ∆∞u cho groupBy, join operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 7: CACHING DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created test DataFrame\n"
     ]
    }
   ],
   "source": [
    "# Create test DataFrame\n",
    "df_test = spark.range(1, 100000).toDF(\"id\") \\\n",
    "    .withColumn(\"value\", col(\"id\") * 2) \\\n",
    "    .filter(col(\"id\") % 2 == 0)\n",
    "\n",
    "print(\"‚úÖ Created test DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Test WITHOUT cache\n",
      "============================================================\n",
      "Time: 0.5728s\n",
      "‚ö†Ô∏è  Each action re-computed!\n"
     ]
    }
   ],
   "source": [
    "print(\"üìå Test WITHOUT cache\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start = time.time()\n",
    "count1 = df_test.count()\n",
    "count2 = df_test.count()\n",
    "count3 = df_test.count()\n",
    "time_no_cache = time.time() - start\n",
    "\n",
    "print(f\"Time: {time_no_cache:.4f}s\")\n",
    "print(\"‚ö†Ô∏è  Each action re-computed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå Test WITH cache\n",
      "============================================================\n",
      "Time: 0.9282s\n",
      "‚ö° Speedup: 0.62x faster!\n",
      "\n",
      "‚úÖ Cache cleared\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìå Test WITH cache\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_cached = df_test.cache()\n",
    "\n",
    "start = time.time()\n",
    "count1 = df_cached.count()  # Compute + cache\n",
    "count2 = df_cached.count()  # Use cache\n",
    "count3 = df_cached.count()  # Use cache\n",
    "time_with_cache = time.time() - start\n",
    "\n",
    "print(f\"Time: {time_with_cache:.4f}s\")\n",
    "print(f\"‚ö° Speedup: {time_no_cache/time_with_cache:.2f}x faster!\")\n",
    "\n",
    "# Cleanup\n",
    "df_cached.unpersist()\n",
    "print(\"\\n‚úÖ Cache cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ SUMMARY\n",
    "\n",
    "### üîë Key Points:\n",
    "\n",
    "**1Ô∏è‚É£ Default Parallelism:**\n",
    "- S·ªë partitions m·∫∑c ƒë·ªãnh khi t·∫°o DataFrame\n",
    "- = s·ªë cores trong cluster\n",
    "- Cluster c·ªßa b·∫°n: 2 cores ‚Üí 2 partitions\n",
    "\n",
    "**2Ô∏è‚É£ repartition(N):**\n",
    "- TƒÉng HO·∫∂C gi·∫£m partitions\n",
    "- Full shuffle (ch·∫≠m)\n",
    "- Data ph√¢n b·ªï ƒë·ªÅu\n",
    "\n",
    "**3Ô∏è‚É£ coalesce(N):**\n",
    "- CH·ªà gi·∫£m partitions\n",
    "- Minimal shuffle (nhanh)\n",
    "- Data c√≥ th·ªÉ kh√¥ng ƒë·ªÅu\n",
    "\n",
    "**4Ô∏è‚É£ repartition(N, \"column\"):**\n",
    "- Hash partitioning\n",
    "- C√πng value ‚Üí c√πng partition\n",
    "- T·ªëi ∆∞u cho groupBy\n",
    "\n",
    "**5Ô∏è‚É£ Caching:**\n",
    "- Tr√°nh recompute\n",
    "- D√πng cho DataFrame d√πng nhi·ªÅu l·∫ßn\n",
    "- Nh·ªõ unpersist() khi xong\n",
    "\n",
    "### üéØ Best Practices:\n",
    "- ‚úÖ S·ªë partitions = 2-4x s·ªë cores\n",
    "- ‚úÖ D√πng repartition() ƒë·ªÉ tƒÉng\n",
    "- ‚úÖ D√πng coalesce() ƒë·ªÉ gi·∫£m\n",
    "- ‚úÖ Cache DataFrame d√πng nhi·ªÅu l·∫ßn\n",
    "- ‚úÖ Lu√¥n unpersist() khi xong"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
