{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAY 1 - NOTEBOOK 3: PARTITIONING & CACHING\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives:\n",
    "1. Understand Partitioning concept\n",
    "2. Master repartition() vs coalesce()\n",
    "3. Learn Caching strategies\n",
    "4. Understand Storage Levels\n",
    "5. Apply best practices for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SETUP: CREATE SPARK SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import StorageLevel\n",
    "import time\n",
    "\n",
    "# Stop existing session\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"âœ… Stopped existing session\")\n",
    "    time.sleep(2)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create new session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Day1-PartitioningCaching\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"512m\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.cores.max\", \"2\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"âœ… Spark Session Created\")\n",
    "print(f\"   App ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"   Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"   UI: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 1: UNDERSTANDING PARTITIONS\n",
    "\n",
    "### ðŸ“š Theory:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    WHAT IS A PARTITION?                      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                              â”‚\n",
    "â”‚  Partition = Má»™t pháº§n nhá» cá»§a data                           â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚  DataFrame with 1000 rows, 4 partitions:                     â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚\n",
    "â”‚    â”‚ Partition 0 â”‚ â†’ 250 rows â†’ Worker 1, Core 1            â”‚\n",
    "â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                                          â”‚\n",
    "â”‚    â”‚ Partition 1 â”‚ â†’ 250 rows â†’ Worker 1, Core 2            â”‚\n",
    "â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                                          â”‚\n",
    "â”‚    â”‚ Partition 2 â”‚ â†’ 250 rows â†’ Worker 2, Core 1            â”‚\n",
    "â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                                          â”‚\n",
    "â”‚    â”‚ Partition 3 â”‚ â†’ 250 rows â†’ Worker 2, Core 2            â”‚\n",
    "â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚  Benefits:                                                   â”‚\n",
    "â”‚    âœ… Parallel processing (4 tasks run simultaneously)       â”‚\n",
    "â”‚    âœ… Distributed storage                                    â”‚\n",
    "â”‚    âœ… Fault tolerance                                        â”‚\n",
    "â”‚    âœ… Scalability                                            â”‚\n",
    "â”‚                                                              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¬ Demo: Check Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrame\n",
    "data = [(i, f\"Name_{i}\", i * 10) for i in range(1, 1001)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"value\"])\n",
    "\n",
    "print(\"ðŸ“Š DataFrame Info:\")\n",
    "print(f\"   Total rows: {df.count()}\")\n",
    "print(f\"   Number of partitions: {df.rdd.getNumPartitions()}\")\n",
    "print(f\"   Rows per partition (avg): {df.count() / df.rdd.getNumPartitions():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¬ Demo: Data Distribution Across Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“Œ Data distribution across partitions:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get row count per partition\n",
    "partition_counts = df.rdd.mapPartitionsWithIndex(\n",
    "    lambda idx, iterator: [(idx, sum(1 for _ in iterator))]\n",
    ").collect()\n",
    "\n",
    "for partition_id, count in partition_counts:\n",
    "    bar = \"â–ˆ\" * (count // 10)\n",
    "    print(f\"Partition {partition_id}: {count:4d} rows {bar}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 2: REPARTITION\n",
    "\n",
    "### ðŸ“š Theory:\n",
    "\n",
    "```\n",
    "repartition(n):\n",
    "  â€¢ TÄƒng HOáº¶C giáº£m sá»‘ partitions\n",
    "  â€¢ Full shuffle (di chuyá»ƒn data qua network)\n",
    "  â€¢ PhÃ¢n bá»• data Ä‘á»u (balanced)\n",
    "  â€¢ Cháº­m nhÆ°ng cÃ¢n báº±ng\n",
    "  â€¢ Use case: TÄƒng parallelism, balance data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“Œ Repartition: Increase partitions (2 â†’ 8)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Original partitions\n",
    "print(f\"Before: {df.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "# Repartition to 8\n",
    "start = time.time()\n",
    "df_8 = df.repartition(8)\n",
    "df_8.count()  # Trigger action\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"After:  {df_8.rdd.getNumPartitions()} partitions\")\n",
    "print(f\"Time:   {elapsed:.4f}s\")\n",
    "print(\"\\nðŸ“Š Data distribution after repartition:\")\n",
    "\n",
    "partition_counts = df_8.rdd.mapPartitionsWithIndex(\n",
    "    lambda idx, it: [(idx, sum(1 for _ in it))]\n",
    ").collect()\n",
    "\n",
    "for partition_id, count in partition_counts:\n",
    "    bar = \"â–ˆ\" * (count // 5)\n",
    "    print(f\"Partition {partition_id}: {count:4d} rows {bar}\")\n",
    "\n",
    "print(\"\\nâœ… Notice: Data is evenly distributed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¬ Demo: Repartition by Column (Hash Partitioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“Œ Repartition by column (hash partitioning)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Add department column\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "data_with_dept = [\n",
    "    (i, f\"Name_{i}\", i * 10, random.choice([\"Eng\", \"Sales\", \"HR\", \"IT\"]))\n",
    "    for i in range(1, 1001)\n",
    "]\n",
    "df_dept = spark.createDataFrame(data_with_dept, [\"id\", \"name\", \"value\", \"department\"])\n",
    "\n",
    "# Repartition by department\n",
    "df_by_dept = df_dept.repartition(4, \"department\")\n",
    "\n",
    "print(f\"Repartitioned by 'department' into {df_by_dept.rdd.getNumPartitions()} partitions\")\n",
    "print(\"\\nâœ… Benefit: Same department data in same partition\")\n",
    "print(\"   â†’ Faster groupBy operations on department\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 3: COALESCE\n",
    "\n",
    "### ðŸ“š Theory:\n",
    "\n",
    "```\n",
    "coalesce(n):\n",
    "  â€¢ CHá»ˆ giáº£m sá»‘ partitions (khÃ´ng tÄƒng)\n",
    "  â€¢ Minimal shuffle (Ã­t di chuyá»ƒn data)\n",
    "  â€¢ Nhanh hÆ¡n repartition\n",
    "  â€¢ CÃ³ thá»ƒ khÃ´ng cÃ¢n báº±ng (skewed)\n",
    "  â€¢ Use case: Giáº£m partitions sau filter, trÆ°á»›c write\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“Œ Coalesce: Decrease partitions (8 â†’ 2)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Before: {df_8.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "# Coalesce to 2\n",
    "start = time.time()\n",
    "df_2 = df_8.coalesce(2)\n",
    "df_2.count()  # Trigger action\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"After:  {df_2.rdd.getNumPartitions()} partitions\")\n",
    "print(f\"Time:   {elapsed:.4f}s\")\n",
    "print(\"\\nðŸ“Š Data distribution after coalesce:\")\n",
    "\n",
    "partition_counts = df_2.rdd.mapPartitionsWithIndex(\n",
    "    lambda idx, it: [(idx, sum(1 for _ in it))]\n",
    ").collect()\n",
    "\n",
    "for partition_id, count in partition_counts:\n",
    "    bar = \"â–ˆ\" * (count // 20)\n",
    "    print(f\"Partition {partition_id}: {count:4d} rows {bar}\")\n",
    "\n",
    "print(\"\\nâš ï¸  Notice: May not be perfectly balanced (but faster!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 4: REPARTITION vs COALESCE PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create larger dataset for better comparison\n",
    "large_data = [(i, f\"Name_{i}\", i * 10) for i in range(1, 100001)]\n",
    "df_large = spark.createDataFrame(large_data, [\"id\", \"name\", \"value\"]).repartition(20)\n",
    "\n",
    "print(f\"âœ… Created large dataset: {df_large.count()} rows\")\n",
    "print(f\"   Starting with {df_large.rdd.getNumPartitions()} partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ Performance Test: repartition vs coalesce\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 1: repartition (20 â†’ 4)\n",
    "print(\"\\nðŸ“Œ Test 1: repartition(4) - Full shuffle\")\n",
    "start = time.time()\n",
    "df_repart = df_large.repartition(4)\n",
    "df_repart.count()\n",
    "time_repart = time.time() - start\n",
    "print(f\"   Time: {time_repart:.4f}s\")\n",
    "\n",
    "# Test 2: coalesce (20 â†’ 4)\n",
    "print(\"\\nðŸ“Œ Test 2: coalesce(4) - Minimal shuffle\")\n",
    "start = time.time()\n",
    "df_coal = df_large.coalesce(4)\n",
    "df_coal.count()\n",
    "time_coal = time.time() - start\n",
    "print(f\"   Time: {time_coal:.4f}s\")\n",
    "\n",
    "# Comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š COMPARISON:\")\n",
    "print(f\"   repartition: {time_repart:.4f}s\")\n",
    "print(f\"   coalesce:    {time_coal:.4f}s\")\n",
    "print(f\"   âš¡ coalesce is {time_repart/time_coal:.2f}x faster!\")\n",
    "print(\"\\nðŸ’¡ Why? coalesce minimizes data movement (no full shuffle)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 5: WHEN TO USE REPARTITION vs COALESCE\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                   USE REPARTITION WHEN:                      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  âœ… TÄƒng sá»‘ partitions                                        â”‚\n",
    "â”‚  âœ… Cáº§n phÃ¢n bá»• data Ä‘á»u                                      â”‚\n",
    "â”‚  âœ… TrÆ°á»›c khi write nhiá»u files                               â”‚\n",
    "â”‚  âœ… TrÆ°á»›c khi join lá»›n                                        â”‚\n",
    "â”‚  âœ… Partition by column (hash partitioning)                   â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚  Examples:                                                   â”‚\n",
    "â”‚    df.repartition(10).write.parquet(\"output/\")              â”‚\n",
    "â”‚    df.repartition(8, \"department\").groupBy(\"department\")    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    USE COALESCE WHEN:                        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  âœ… Giáº£m sá»‘ partitions                                        â”‚\n",
    "â”‚  âœ… Sau khi filter (data nhá» hÆ¡n)                             â”‚\n",
    "â”‚  âœ… TrÆ°á»›c khi collect                                         â”‚\n",
    "â”‚  âœ… TrÆ°á»›c khi write 1 file                                    â”‚\n",
    "â”‚  âœ… Muá»‘n tá»‘i Æ°u performance                                   â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚  Examples:                                                   â”‚\n",
    "â”‚    df.filter(...).coalesce(1).write.csv(\"output.csv\")       â”‚\n",
    "â”‚    df.filter(...).coalesce(2).collect()                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 6: CACHING & PERSISTENCE\n",
    "\n",
    "### ðŸ“š Theory:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    WHY CACHE?                                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                              â”‚\n",
    "â”‚  Without Cache:                                              â”‚\n",
    "â”‚    action1 â†’ compute from source                             â”‚\n",
    "â”‚    action2 â†’ compute from source (again!)                    â”‚\n",
    "â”‚    action3 â†’ compute from source (again!)                    â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚  With Cache:                                                 â”‚\n",
    "â”‚    action1 â†’ compute + store in memory                       â”‚\n",
    "â”‚    action2 â†’ read from memory (fast!)                        â”‚\n",
    "â”‚    action3 â†’ read from memory (fast!)                        â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚  When to Cache:                                              â”‚\n",
    "â”‚    âœ… DataFrame used multiple times                          â”‚\n",
    "â”‚    âœ… Expensive computation (joins, aggregations)            â”‚\n",
    "â”‚    âœ… Iterative algorithms (ML)                              â”‚\n",
    "â”‚    âœ… Interactive analysis                                   â”‚\n",
    "â”‚                                                              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¬ Demo: Cache Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test DataFrame with expensive transformation\n",
    "df_test = spark.range(1, 1000000).toDF(\"id\") \\\n",
    "    .withColumn(\"value\", col(\"id\") * 2) \\\n",
    "    .filter(col(\"id\") % 2 == 0)\n",
    "\n",
    "print(\"âœ… Created test DataFrame with expensive transformation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“Œ Test 1: WITHOUT cache (3 actions)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start = time.time()\n",
    "count1 = df_test.count()  # Action 1: compute\n",
    "count2 = df_test.count()  # Action 2: compute again!\n",
    "count3 = df_test.count()  # Action 3: compute again!\n",
    "time_no_cache = time.time() - start\n",
    "\n",
    "print(f\"âœ… 3 actions completed\")\n",
    "print(f\"   Time: {time_no_cache:.4f}s\")\n",
    "print(f\"   Avg per action: {time_no_cache/3:.4f}s\")\n",
    "print(\"   âš ï¸  Each action re-computed everything!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“Œ Test 2: WITH cache (3 actions)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cache the DataFrame\n",
    "df_cached = df_test.cache()\n",
    "\n",
    "start = time.time()\n",
    "count1 = df_cached.count()  # Action 1: compute + cache\n",
    "count2 = df_cached.count()  # Action 2: use cache (fast!)\n",
    "count3 = df_cached.count()  # Action 3: use cache (fast!)\n",
    "time_with_cache = time.time() - start\n",
    "\n",
    "print(f\"âœ… 3 actions completed\")\n",
    "print(f\"   Time: {time_with_cache:.4f}s\")\n",
    "print(f\"   Avg per action: {time_with_cache/3:.4f}s\")\n",
    "print(\"   âš¡ Used cache for 2nd and 3rd actions!\")\n",
    "\n",
    "# Comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š COMPARISON:\")\n",
    "print(f\"   Without cache: {time_no_cache:.4f}s\")\n",
    "print(f\"   With cache:    {time_with_cache:.4f}s\")\n",
    "print(f\"   âš¡ Speedup: {time_no_cache/time_with_cache:.2f}x faster!\")\n",
    "\n",
    "# Clean up\n",
    "df_cached.unpersist()\n",
    "print(\"\\nâœ… Cache cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 7: STORAGE LEVELS\n",
    "\n",
    "### ðŸ“š Theory:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                   STORAGE LEVELS                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                              â”‚\n",
    "â”‚  MEMORY_ONLY (default for cache()):                          â”‚\n",
    "â”‚    â€¢ Store in memory only                                    â”‚\n",
    "â”‚    â€¢ Fast but limited by RAM                                 â”‚\n",
    "â”‚    â€¢ If not fit, recompute                                   â”‚\n",
    "â”‚    â€¢ Use: Small-medium datasets                              â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚  MEMORY_AND_DISK:                                            â”‚\n",
    "â”‚    â€¢ Store in memory first                                   â”‚\n",
    "â”‚    â€¢ Spill to disk if not fit                                â”‚\n",
    "â”‚    â€¢ Slower but more reliable                                â”‚\n",
    "â”‚    â€¢ Use: Large datasets                                     â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚  DISK_ONLY:                                                  â”‚\n",
    "â”‚    â€¢ Store on disk only                                      â”‚\n",
    "â”‚    â€¢ Slow but saves memory                                   â”‚\n",
    "â”‚    â€¢ Use: Very large datasets, limited RAM                   â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚  MEMORY_ONLY_SER:                                            â”‚\n",
    "â”‚    â€¢ Serialize before storing                                â”‚\n",
    "â”‚    â€¢ Save memory but slower                                  â”‚\n",
    "â”‚    â€¢ Use: Memory-constrained environments                    â”‚\n",
    "â”‚                                                              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“Œ Storage Levels Demo\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# MEMORY_ONLY (default)\n",
    "print(\"\\n1ï¸âƒ£ MEMORY_ONLY (default)\")\n",
    "df_mem = df_test.cache()  # Same as persist(StorageLevel.MEMORY_ONLY)\n",
    "df_mem.count()\n",
    "print(\"   âœ… Cached in memory only\")\n",
    "df_mem.unpersist()\n",
    "\n",
    "# MEMORY_AND_DISK\n",
    "print(\"\\n2ï¸âƒ£ MEMORY_AND_DISK\")\n",
    "df_mem_disk = df_test.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "df_mem_disk.count()\n",
    "print(\"   âœ… Cached in memory, spills to disk if needed\")\n",
    "df_mem_disk.unpersist()\n",
    "\n",
    "# DISK_ONLY\n",
    "print(\"\\n3ï¸âƒ£ DISK_ONLY\")\n",
    "df_disk = df_test.persist(StorageLevel.DISK_ONLY)\n",
    "df_disk.count()\n",
    "print(\"   âœ… Cached on disk only\")\n",
    "df_disk.unpersist()\n",
    "\n",
    "print(\"\\nâœ… All storage levels demonstrated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 8: BEST PRACTICES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“‹ Partitioning Best Practices:\n",
    "\n",
    "```\n",
    "1. Sá»‘ partitions = 2-4x sá»‘ cores\n",
    "   â€¢ 2 workers Ã— 2 cores = 4 cores\n",
    "   â€¢ Recommended: 8-16 partitions\n",
    "\n",
    "2. Partition size: 128MB - 1GB\n",
    "   â€¢ Too small: overhead\n",
    "   â€¢ Too large: memory issues\n",
    "\n",
    "3. Repartition before:\n",
    "   â€¢ Wide transformations (join, groupBy)\n",
    "   â€¢ Writing to storage\n",
    "\n",
    "4. Coalesce after:\n",
    "   â€¢ Filter (reduced data)\n",
    "   â€¢ Before collect\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“‹ Caching Best Practices:\n",
    "\n",
    "```\n",
    "1. Cache when:\n",
    "   â€¢ DataFrame used multiple times\n",
    "   â€¢ Expensive computation\n",
    "   â€¢ Iterative algorithms\n",
    "\n",
    "2. Don't cache when:\n",
    "   â€¢ Used only once\n",
    "   â€¢ Data too large for memory\n",
    "   â€¢ Simple transformations\n",
    "\n",
    "3. Always unpersist:\n",
    "   â€¢ After done using\n",
    "   â€¢ To free memory\n",
    "\n",
    "4. Monitor:\n",
    "   â€¢ Spark UI â†’ Storage tab\n",
    "   â€¢ Check memory usage\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 9: PRACTICAL EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“Œ Practical Example: Process large dataset with optimization\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create large dataset\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "fake = Faker()\n",
    "Faker.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "large_data = []\n",
    "for i in range(1, 10001):\n",
    "    large_data.append({\n",
    "        \"id\": i,\n",
    "        \"name\": fake.name(),\n",
    "        \"department\": random.choice([\"Engineering\", \"Sales\", \"HR\", \"Marketing\"]),\n",
    "        \"salary\": random.randint(40000, 120000),\n",
    "        \"age\": random.randint(22, 60)\n",
    "    })\n",
    "\n",
    "df_large = spark.createDataFrame(large_data)\n",
    "print(f\"âœ… Created dataset: {df_large.count()} rows\")\n",
    "print(f\"   Initial partitions: {df_large.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Step 1: Repartition for parallel processing\")\n",
    "df_repart = df_large.repartition(8)\n",
    "print(f\"   Partitions: {df_repart.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Step 2: Apply transformations\")\n",
    "df_transformed = df_repart \\\n",
    "    .filter(col(\"salary\") > 50000) \\\n",
    "    .withColumn(\"annual_salary\", col(\"salary\") * 12) \\\n",
    "    .withColumn(\"age_group\", \n",
    "        when(col(\"age\") < 30, \"Young\")\n",
    "        .when((col(\"age\") >= 30) & (col(\"age\") < 50), \"Middle\")\n",
    "        .otherwise(\"Senior\")\n",
    "    )\n",
    "print(\"   âœ… Transformations applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Step 3: Cache for multiple operations\")\n",
    "df_cached = df_transformed.cache()\n",
    "df_cached.count()  # Trigger caching\n",
    "print(\"   âœ… Cached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Step 4: Multiple operations (use cache)\")\n",
    "start = time.time()\n",
    "\n",
    "# Operation 1: Count by department\n",
    "dept_count = df_cached.groupBy(\"department\").count()\n",
    "print(\"\\nðŸ“Š Count by department:\")\n",
    "dept_count.show()\n",
    "\n",
    "# Operation 2: Average salary by age group\n",
    "age_salary = df_cached.groupBy(\"age_group\").agg(avg(\"salary\").alias(\"avg_salary\"))\n",
    "print(\"ðŸ“Š Average salary by age group:\")\n",
    "age_salary.show()\n",
    "\n",
    "# Operation 3: Top 10 highest paid\n",
    "top_10 = df_cached.orderBy(col(\"salary\").desc()).limit(10)\n",
    "print(\"ðŸ“Š Top 10 highest paid:\")\n",
    "top_10.show()\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\nâœ… All operations completed in {elapsed:.4f}s\")\n",
    "print(\"   âš¡ Fast because data was cached!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Step 5: Coalesce before final output\")\n",
    "df_final = df_cached.coalesce(2)\n",
    "print(f\"   Final partitions: {df_final.rdd.getNumPartitions()}\")\n",
    "print(\"   âœ… Ready for write/export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¹ Step 6: Clean up\")\n",
    "df_cached.unpersist()\n",
    "print(\"   âœ… Memory freed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… SUMMARY\n",
    "\n",
    "### ðŸ“š What you learned:\n",
    "\n",
    "**1ï¸âƒ£ Partitioning:**\n",
    "- What is a partition\n",
    "- repartition() - full shuffle, balanced\n",
    "- coalesce() - minimal shuffle, fast\n",
    "- When to use each\n",
    "\n",
    "**2ï¸âƒ£ Caching:**\n",
    "- Why cache (avoid recomputation)\n",
    "- cache() vs persist()\n",
    "- Storage levels (MEMORY_ONLY, MEMORY_AND_DISK, etc.)\n",
    "- When to cache\n",
    "\n",
    "**3ï¸âƒ£ Performance:**\n",
    "- Optimize partitions (2-4x cores)\n",
    "- Cache for multiple actions\n",
    "- Monitor Spark UI\n",
    "\n",
    "**4ï¸âƒ£ Best Practices:**\n",
    "- Partition size: 128MB - 1GB\n",
    "- Repartition before wide transformations\n",
    "- Coalesce after filter\n",
    "- Always unpersist when done\n",
    "\n",
    "### ðŸŽ¯ Key Takeaways:\n",
    "- âœ… Understand partitioning for parallelism\n",
    "- âœ… Use repartition for balance, coalesce for speed\n",
    "- âœ… Cache expensive computations\n",
    "- âœ… Choose right storage level\n",
    "- âœ… Always clean up (unpersist)\n",
    "\n",
    "### ðŸŽ‰ DAY 1 COMPLETED!\n",
    "\n",
    "**You now understand:**\n",
    "- âœ… Spark Architecture\n",
    "- âœ… DataFrame API\n",
    "- âœ… Transformations vs Actions\n",
    "- âœ… Lazy Evaluation\n",
    "- âœ… Partitioning strategies\n",
    "- âœ… Caching techniques\n",
    "\n",
    "### ðŸ“ Next: Day 2 - Data I/O & Cleaning\n",
    "- Reading/Writing different formats\n",
    "- Data cleaning techniques\n",
    "- Handling missing data\n",
    "- Type conversions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}