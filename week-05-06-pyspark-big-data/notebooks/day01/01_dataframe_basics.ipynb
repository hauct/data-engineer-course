{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAY 1 - NOTEBOOK 1: SPARK ARCHITECTURE & DATAFRAME BASICS\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives:\n",
    "1. Understand Spark Architecture\n",
    "2. Create DataFrames (3 methods)\n",
    "3. Basic operations (select, filter, show)\n",
    "4. Schema operations\n",
    "5. Column operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 1: UNDERSTANDING SPARK ARCHITECTURE\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    SPARK CLUSTER ARCHITECTURE                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚\n",
    "â”‚                    â”‚   DRIVER     â”‚  â† Your Jupyter Notebook    â”‚\n",
    "â”‚                    â”‚  (jupyter)   â”‚                             â”‚\n",
    "â”‚                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚\n",
    "â”‚                           â”‚                                     â”‚\n",
    "â”‚                           â”‚ Submit Job                          â”‚\n",
    "â”‚                           â–¼                                     â”‚\n",
    "â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚\n",
    "â”‚                    â”‚CLUSTER MANAGERâ”‚ â† Spark Master             â”‚\n",
    "â”‚                    â”‚(spark-master) â”‚                            â”‚\n",
    "â”‚                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚\n",
    "â”‚                           â”‚                                     â”‚\n",
    "â”‚                           â”‚ Allocate Resources                  â”‚\n",
    "â”‚                           â–¼                                     â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚\n",
    "â”‚         â”‚          EXECUTORS                  â”‚                 â”‚\n",
    "â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚                 â”‚\n",
    "â”‚         â”‚  â”‚ Worker 1 â”‚    â”‚ Worker 2 â”‚      â”‚                 â”‚\n",
    "â”‚         â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â” â”‚      â”‚                 â”‚\n",
    "â”‚         â”‚  â”‚ â”‚Task 1â”‚ â”‚    â”‚ â”‚Task 3â”‚ â”‚      â”‚                 â”‚\n",
    "â”‚         â”‚  â”‚ â”‚Task 2â”‚ â”‚    â”‚ â”‚Task 4â”‚ â”‚      â”‚                 â”‚\n",
    "â”‚         â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”˜ â”‚      â”‚                 â”‚\n",
    "â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚                 â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "| Component | Role | Example |\n",
    "|-----------|------|----------|\n",
    "| **Driver** | Orchestrates job | Jupyter Notebook |\n",
    "| **Cluster Manager** | Manages resources | Spark Master |\n",
    "| **Executors** | Execute tasks | Workers (2 workers) |\n",
    "| **Tasks** | Smallest work unit | Process 1 partition |\n",
    "\n",
    "### Data Flow:\n",
    "1. Driver creates execution plan\n",
    "2. Cluster Manager allocates resources\n",
    "3. Executors receive tasks\n",
    "4. Tasks process data in parallel\n",
    "5. Results return to Driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 2: CREATE SPARK SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark Session Created!\n",
      "   Version: 3.5.1\n",
      "   Master: spark://spark-master:7077\n",
      "   App ID: app-20251222152137-0000\n",
      "   Default Parallelism: 2\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create session with optimized config\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Day1-DataFrameBasics\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"512m\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.cores.max\", \"2\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .config(\"spark.shuffle.service.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"âœ… Spark Session Created!\")\n",
    "print(f\"   Version: {spark.version}\")\n",
    "print(f\"   Master: {spark.sparkContext.master}\")\n",
    "print(f\"   App ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"   Default Parallelism: {spark.sparkContext.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 3: CREATING DATAFRAMES (3 METHODS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: From Python List of Tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created DataFrame from list of tuples\n",
      "\n",
      "ðŸ“Š Display data:\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 25|Engineering| 75000|\n",
      "|  2|    Bob| 30|      Sales| 65000|\n",
      "|  3|Charlie| 35|Engineering| 85000|\n",
      "|  4|  David| 28|         HR| 55000|\n",
      "|  5|    Eve| 32|      Sales| 70000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n",
      "ðŸ“‹ Schema:\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Alice\", 25, \"Engineering\", 75000),\n",
    "    (2, \"Bob\", 30, \"Sales\", 65000),\n",
    "    (3, \"Charlie\", 35, \"Engineering\", 85000),\n",
    "    (4, \"David\", 28, \"HR\", 55000),\n",
    "    (5, \"Eve\", 32, \"Sales\", 70000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"age\", \"department\", \"salary\"])\n",
    "\n",
    "print(\"âœ… Created DataFrame from list of tuples\")\n",
    "print(\"\\nðŸ“Š Display data:\")\n",
    "df.show()\n",
    "\n",
    "print(\"ðŸ“‹ Schema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: From List of Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created DataFrame from list of dictionaries\n",
      "\n",
      "ðŸ“Š Display data:\n",
      "+---+--------+---+-------+\n",
      "|age|    city| id|   name|\n",
      "+---+--------+---+-------+\n",
      "| 25|New York|  1|  Alice|\n",
      "| 30|  London|  2|    Bob|\n",
      "| 35|   Paris|  3|Charlie|\n",
      "+---+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_dict = [\n",
    "    {\"id\": 1, \"name\": \"Alice\", \"age\": 25, \"city\": \"New York\"},\n",
    "    {\"id\": 2, \"name\": \"Bob\", \"age\": 30, \"city\": \"London\"},\n",
    "    {\"id\": 3, \"name\": \"Charlie\", \"age\": 35, \"city\": \"Paris\"}\n",
    "]\n",
    "\n",
    "df_dict = spark.createDataFrame(data_dict)\n",
    "\n",
    "print(\"âœ… Created DataFrame from list of dictionaries\")\n",
    "print(\"\\nðŸ“Š Display data:\")\n",
    "df_dict.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: With Explicit Schema (RECOMMENDED for Production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created DataFrame with explicit schema\n",
      "\n",
      "ðŸ“‹ Schema with constraints:\n",
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- name: string (nullable = false)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "\n",
      "ðŸ’¡ Why use explicit schema?\n",
      "   âœ… Type safety (prevent errors)\n",
      "   âœ… Better performance (no inference)\n",
      "   âœ… Nullable constraints\n",
      "   âœ… Documentation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), nullable=False),\n",
    "    StructField(\"name\", StringType(), nullable=False),\n",
    "    StructField(\"age\", IntegerType(), nullable=True),\n",
    "    StructField(\"department\", StringType(), nullable=True),\n",
    "    StructField(\"salary\", IntegerType(), nullable=True)\n",
    "])\n",
    "\n",
    "df_schema = spark.createDataFrame(data, schema)\n",
    "\n",
    "print(\"âœ… Created DataFrame with explicit schema\")\n",
    "print(\"\\nðŸ“‹ Schema with constraints:\")\n",
    "df_schema.printSchema()\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’¡ Why use explicit schema?\n",
    "   âœ… Type safety (prevent errors)\n",
    "   âœ… Better performance (no inference)\n",
    "   âœ… Nullable constraints\n",
    "   âœ… Documentation\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 4: BASIC DATAFRAME OPERATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1: Inspecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ show() - Display data\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 25|Engineering| 75000|\n",
      "|  2|    Bob| 30|      Sales| 65000|\n",
      "|  3|Charlie| 35|Engineering| 85000|\n",
      "|  4|  David| 28|         HR| 55000|\n",
      "|  5|    Eve| 32|      Sales| 70000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ show() - Display data\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ show(n) - Display first n rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 25|Engineering| 75000|\n",
      "|  2|    Bob| 30|      Sales| 65000|\n",
      "|  3|Charlie| 35|Engineering| 85000|\n",
      "+---+-------+---+-----------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ show(n) - Display first n rows\")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ show(truncate=False) - Don't truncate long strings\n",
      "+---+-------+---+-----------+------+\n",
      "|id |name   |age|department |salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|1  |Alice  |25 |Engineering|75000 |\n",
      "|2  |Bob    |30 |Sales      |65000 |\n",
      "|3  |Charlie|35 |Engineering|85000 |\n",
      "|4  |David  |28 |HR         |55000 |\n",
      "|5  |Eve    |32 |Sales      |70000 |\n",
      "+---+-------+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ show(truncate=False) - Don't truncate long strings\")\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ printSchema() - Display schema\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ printSchema() - Display schema\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ columns - Get column names\n",
      "Columns: ['id', 'name', 'age', 'department', 'salary']\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ columns - Get column names\")\n",
    "print(f\"Columns: {df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ dtypes - Get data types\n",
      "Data types: [('id', 'bigint'), ('name', 'string'), ('age', 'bigint'), ('department', 'string'), ('salary', 'bigint')]\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ dtypes - Get data types\")\n",
    "print(f\"Data types: {df.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ count() - Count rows\n",
      "Total rows: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ count() - Count rows\")\n",
    "print(f\"Total rows: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ describe() - Summary statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/22 15:23:41 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 15:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----+------------------+-----------+------------------+\n",
      "|summary|                id| name|               age| department|            salary|\n",
      "+-------+------------------+-----+------------------+-----------+------------------+\n",
      "|  count|                 5|    5|                 5|          5|                 5|\n",
      "|   mean|               3.0| NULL|              30.0|       NULL|           70000.0|\n",
      "| stddev|1.5811388300841898| NULL|3.8078865529319548|       NULL|11180.339887498949|\n",
      "|    min|                 1|Alice|                25|Engineering|             55000|\n",
      "|    max|                 5|  Eve|                35|      Sales|             85000|\n",
      "+-------+------------------+-----+------------------+-----------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ describe() - Summary statistics\")\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ first() - Get first row\n",
      "First row: Row(id=1, name='Alice', age=25, department='Engineering', salary=75000)\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ first() - Get first row\")\n",
    "print(f\"First row: {df.first()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ take(n) - Get first n rows as list\n",
      "First 2 rows: [Row(id=1, name='Alice', age=25, department='Engineering', salary=75000), Row(id=2, name='Bob', age=30, department='Sales', salary=65000)]\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ take(n) - Get first n rows as list\")\n",
    "print(f\"First 2 rows: {df.take(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ head(n) - Same as take(n)\n",
      "First 2 rows: [Row(id=1, name='Alice', age=25, department='Engineering', salary=75000), Row(id=2, name='Bob', age=30, department='Sales', salary=65000)]\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ head(n) - Same as take(n)\")\n",
    "print(f\"First 2 rows: {df.head(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  collect() - Get ALL rows (USE WITH CAUTION!)\n",
      "    Only use on small datasets!\n",
      "    Collected 5 rows\n",
      "    First row: Row(id=1, name='Alice', age=25, department='Engineering', salary=75000)\n"
     ]
    }
   ],
   "source": [
    "print(\"âš ï¸  collect() - Get ALL rows (USE WITH CAUTION!)\")\n",
    "print(\"    Only use on small datasets!\")\n",
    "rows = df.collect()\n",
    "print(f\"    Collected {len(rows)} rows\")\n",
    "print(f\"    First row: {rows[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2: Selecting Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Select single column\n",
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|  Alice|\n",
      "|    Bob|\n",
      "|Charlie|\n",
      "|  David|\n",
      "|    Eve|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Select single column\")\n",
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Select multiple columns\n",
      "+-------+---+------+\n",
      "|   name|age|salary|\n",
      "+-------+---+------+\n",
      "|  Alice| 25| 75000|\n",
      "|    Bob| 30| 65000|\n",
      "|Charlie| 35| 85000|\n",
      "|  David| 28| 55000|\n",
      "|    Eve| 32| 70000|\n",
      "+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Select multiple columns\")\n",
    "df.select(\"name\", \"age\", \"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Select with col() function\n",
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "|Charlie| 35|\n",
      "|  David| 28|\n",
      "|    Eve| 32|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Select with col() function\")\n",
    "df.select(col(\"name\"), col(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Select all columns\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 25|Engineering| 75000|\n",
      "|  2|    Bob| 30|      Sales| 65000|\n",
      "|  3|Charlie| 35|Engineering| 85000|\n",
      "|  4|  David| 28|         HR| 55000|\n",
      "|  5|    Eve| 32|      Sales| 70000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Select all columns\")\n",
    "df.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Select with expressions\n",
      "+-------+------+-----------------------+\n",
      "|   name|salary|salary_with_10pct_raise|\n",
      "+-------+------+-----------------------+\n",
      "|  Alice| 75000|                82500.0|\n",
      "|    Bob| 65000|                71500.0|\n",
      "|Charlie| 85000|      93500.00000000001|\n",
      "|  David| 55000|      60500.00000000001|\n",
      "|    Eve| 70000|                77000.0|\n",
      "+-------+------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Select with expressions\")\n",
    "df.select(\n",
    "    col(\"name\"),\n",
    "    col(\"salary\"),\n",
    "    (col(\"salary\") * 1.1).alias(\"salary_with_10pct_raise\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Select with column renaming\n",
      "+-------------+-----------+\n",
      "|employee_name|       dept|\n",
      "+-------------+-----------+\n",
      "|        Alice|Engineering|\n",
      "|          Bob|      Sales|\n",
      "|      Charlie|Engineering|\n",
      "|        David|         HR|\n",
      "|          Eve|      Sales|\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Select with column renaming\")\n",
    "df.select(\n",
    "    col(\"name\").alias(\"employee_name\"),\n",
    "    col(\"department\").alias(\"dept\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3: Filtering Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Filter: age > 28\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  2|    Bob| 30|      Sales| 65000|\n",
      "|  3|Charlie| 35|Engineering| 85000|\n",
      "|  5|    Eve| 32|      Sales| 70000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Filter: age > 28\")\n",
    "df.filter(col(\"age\") > 28).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Filter: Multiple conditions (AND)\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  2|    Bob| 30|      Sales| 65000|\n",
      "|  3|Charlie| 35|Engineering| 85000|\n",
      "|  5|    Eve| 32|      Sales| 70000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Filter: Multiple conditions (AND)\")\n",
    "df.filter((col(\"age\") > 28) & (col(\"salary\") > 60000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Filter: Multiple conditions (OR)\n",
      "+---+-----+---+----------+------+\n",
      "| id| name|age|department|salary|\n",
      "+---+-----+---+----------+------+\n",
      "|  2|  Bob| 30|     Sales| 65000|\n",
      "|  4|David| 28|        HR| 55000|\n",
      "|  5|  Eve| 32|     Sales| 70000|\n",
      "+---+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Filter: Multiple conditions (OR)\")\n",
    "df.filter((col(\"department\") == \"Sales\") | (col(\"department\") == \"HR\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Filter: SQL-like syntax\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  2|    Bob| 30|      Sales| 65000|\n",
      "|  3|Charlie| 35|Engineering| 85000|\n",
      "|  5|    Eve| 32|      Sales| 70000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Filter: SQL-like syntax\")\n",
    "df.filter(\"age > 28 AND salary > 60000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Filter: Using isin()\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 25|Engineering| 75000|\n",
      "|  2|    Bob| 30|      Sales| 65000|\n",
      "|  3|Charlie| 35|Engineering| 85000|\n",
      "|  5|    Eve| 32|      Sales| 70000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Filter: Using isin()\")\n",
    "df.filter(col(\"department\").isin(\"Sales\", \"Engineering\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Filter: Pattern matching with like()\n",
      "+---+-----+---+-----------+------+\n",
      "| id| name|age| department|salary|\n",
      "+---+-----+---+-----------+------+\n",
      "|  1|Alice| 25|Engineering| 75000|\n",
      "+---+-----+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Filter: Pattern matching with like()\")\n",
    "df.filter(col(\"name\").like(\"A%\")).show()  # Names starting with 'A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Filter: NOT NULL\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 25|Engineering| 75000|\n",
      "|  2|    Bob| 30|      Sales| 65000|\n",
      "|  3|Charlie| 35|Engineering| 85000|\n",
      "|  4|  David| 28|         HR| 55000|\n",
      "|  5|    Eve| 32|      Sales| 70000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Filter: NOT NULL\")\n",
    "df.filter(col(\"age\").isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Filter: IS NULL\n"
     ]
    },
    {
     "ename": "PySparkValueError",
     "evalue": "[CANNOT_DETERMINE_TYPE] Some of types cannot be determined after inferring.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkValueError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ“Œ Filter: IS NULL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Add a row with NULL\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df_with_null \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39munion(\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFrank\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m80000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      4\u001b[0m df_with_null\u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misNull())\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[0;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/session.py:1093\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1090\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m-> 1093\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1094\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m   1095\u001b[0m     tupled_data: Iterable[Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/session.py:969\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    955\u001b[0m schema \u001b[38;5;241m=\u001b[39m reduce(\n\u001b[1;32m    956\u001b[0m     _merge_type,\n\u001b[1;32m    957\u001b[0m     (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    966\u001b[0m     ),\n\u001b[1;32m    967\u001b[0m )\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[0;32m--> 969\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    970\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_DETERMINE_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    971\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    972\u001b[0m     )\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m schema\n",
      "\u001b[0;31mPySparkValueError\u001b[0m: [CANNOT_DETERMINE_TYPE] Some of types cannot be determined after inferring."
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Filter: IS NULL\")\n",
    "# Add a row with NULL\n",
    "df_with_null = df.union(spark.createDataFrame([(6, \"Frank\", None, \"IT\", 80000)], df.columns))\n",
    "df_with_null.filter(col(\"age\").isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4: Adding & Modifying Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Add new column: annual_salary\n",
      "+---+-------+---+-----------+------+-------------+\n",
      "| id|   name|age| department|salary|annual_salary|\n",
      "+---+-------+---+-----------+------+-------------+\n",
      "|  1|  Alice| 25|Engineering| 75000|       900000|\n",
      "|  2|    Bob| 30|      Sales| 65000|       780000|\n",
      "|  3|Charlie| 35|Engineering| 85000|      1020000|\n",
      "|  4|  David| 28|         HR| 55000|       660000|\n",
      "|  5|    Eve| 32|      Sales| 70000|       840000|\n",
      "+---+-------+---+-----------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Add new column: annual_salary\")\n",
    "df_with_annual = df.withColumn(\"annual_salary\", col(\"salary\") * 12)\n",
    "df_with_annual.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Add multiple columns (chaining)\n",
      "+---+-------+---+-----------+------+-------------+--------------+---------+\n",
      "| id|   name|age| department|salary|annual_salary|age_in_5_years|is_senior|\n",
      "+---+-------+---+-----------+------+-------------+--------------+---------+\n",
      "|  1|  Alice| 25|Engineering| 75000|       900000|            30|    false|\n",
      "|  2|    Bob| 30|      Sales| 65000|       780000|            35|    false|\n",
      "|  3|Charlie| 35|Engineering| 85000|      1020000|            40|     true|\n",
      "|  4|  David| 28|         HR| 55000|       660000|            33|    false|\n",
      "|  5|    Eve| 32|      Sales| 70000|       840000|            37|     true|\n",
      "+---+-------+---+-----------+------+-------------+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Add multiple columns (chaining)\")\n",
    "df_enhanced = df \\\n",
    "    .withColumn(\"annual_salary\", col(\"salary\") * 12) \\\n",
    "    .withColumn(\"age_in_5_years\", col(\"age\") + 5) \\\n",
    "    .withColumn(\"is_senior\", col(\"age\") > 30)\n",
    "df_enhanced.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Modify existing column\n",
      "Salary in thousands:\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 25|Engineering|  75.0|\n",
      "|  2|    Bob| 30|      Sales|  65.0|\n",
      "|  3|Charlie| 35|Engineering|  85.0|\n",
      "|  4|  David| 28|         HR|  55.0|\n",
      "|  5|    Eve| 32|      Sales|  70.0|\n",
      "+---+-------+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Modify existing column\")\n",
    "df_modified = df.withColumn(\"salary\", col(\"salary\") / 1000)\n",
    "print(\"Salary in thousands:\")\n",
    "df_modified.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Rename column\n",
      "+---+-------+---+-----------+--------------+\n",
      "| id|   name|age| department|monthly_salary|\n",
      "+---+-------+---+-----------+--------------+\n",
      "|  1|  Alice| 25|Engineering|         75000|\n",
      "|  2|    Bob| 30|      Sales|         65000|\n",
      "|  3|Charlie| 35|Engineering|         85000|\n",
      "|  4|  David| 28|         HR|         55000|\n",
      "|  5|    Eve| 32|      Sales|         70000|\n",
      "+---+-------+---+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Rename column\")\n",
    "df_renamed = df.withColumnRenamed(\"salary\", \"monthly_salary\")\n",
    "df_renamed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Drop column\n",
      "+---+-------+---+------+\n",
      "| id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|  1|  Alice| 25| 75000|\n",
      "|  2|    Bob| 30| 65000|\n",
      "|  3|Charlie| 35| 85000|\n",
      "|  4|  David| 28| 55000|\n",
      "|  5|    Eve| 32| 70000|\n",
      "+---+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Drop column\")\n",
    "df_dropped = df.drop(\"department\")\n",
    "df_dropped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Drop multiple columns\n",
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 25|\n",
      "|  2|    Bob| 30|\n",
      "|  3|Charlie| 35|\n",
      "|  4|  David| 28|\n",
      "|  5|    Eve| 32|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Drop multiple columns\")\n",
    "df_dropped_multi = df.drop(\"department\", \"salary\")\n",
    "df_dropped_multi.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5: Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Sort by age (ascending)\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 25|Engineering| 75000|\n",
      "|  4|  David| 28|         HR| 55000|\n",
      "|  2|    Bob| 30|      Sales| 65000|\n",
      "|  5|    Eve| 32|      Sales| 70000|\n",
      "|  3|Charlie| 35|Engineering| 85000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Sort by age (ascending)\")\n",
    "df.orderBy(\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Sort by salary (descending)\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  3|Charlie| 35|Engineering| 85000|\n",
      "|  1|  Alice| 25|Engineering| 75000|\n",
      "|  5|    Eve| 32|      Sales| 70000|\n",
      "|  2|    Bob| 30|      Sales| 65000|\n",
      "|  4|  David| 28|         HR| 55000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Sort by salary (descending)\")\n",
    "df.orderBy(col(\"salary\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Sort by multiple columns\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  3|Charlie| 35|Engineering| 85000|\n",
      "|  1|  Alice| 25|Engineering| 75000|\n",
      "|  4|  David| 28|         HR| 55000|\n",
      "|  5|    Eve| 32|      Sales| 70000|\n",
      "|  2|    Bob| 30|      Sales| 65000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Sort by multiple columns\")\n",
    "df.orderBy(\"department\", col(\"salary\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Sort with asc() and desc() explicitly\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 25|Engineering| 75000|\n",
      "|  4|  David| 28|         HR| 55000|\n",
      "|  2|    Bob| 30|      Sales| 65000|\n",
      "|  5|    Eve| 32|      Sales| 70000|\n",
      "|  3|Charlie| 35|Engineering| 85000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Sort with asc() and desc() explicitly\")\n",
    "df.orderBy(col(\"age\").asc(), col(\"salary\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6: Distinct & Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Original data with duplicates\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 25|Engineering| 75000|\n",
      "|  2|    Bob| 30|      Sales| 65000|\n",
      "|  3|Charlie| 35|Engineering| 85000|\n",
      "|  4|  David| 28|         HR| 55000|\n",
      "|  5|    Eve| 32|      Sales| 70000|\n",
      "|  6|  Alice| 25|Engineering| 75000|\n",
      "|  7|    Bob| 30|      Sales| 65000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create data with duplicates\n",
    "data_with_dups = data + [\n",
    "    (6, \"Alice\", 25, \"Engineering\", 75000),  # Duplicate\n",
    "    (7, \"Bob\", 30, \"Sales\", 65000)  # Duplicate\n",
    "]\n",
    "df_dups = spark.createDataFrame(data_with_dups, [\"id\", \"name\", \"age\", \"department\", \"salary\"])\n",
    "\n",
    "print(\"ðŸ“Œ Original data with duplicates\")\n",
    "df_dups.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Get distinct rows\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  4|  David| 28|         HR| 55000|\n",
      "|  5|    Eve| 32|      Sales| 70000|\n",
      "|  7|    Bob| 30|      Sales| 65000|\n",
      "|  6|  Alice| 25|Engineering| 75000|\n",
      "|  3|Charlie| 35|Engineering| 85000|\n",
      "|  1|  Alice| 25|Engineering| 75000|\n",
      "|  2|    Bob| 30|      Sales| 65000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Get distinct rows\")\n",
    "df_dups.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Drop duplicates based on specific columns\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 25|Engineering| 75000|\n",
      "|  2|    Bob| 30|      Sales| 65000|\n",
      "|  3|Charlie| 35|Engineering| 85000|\n",
      "|  4|  David| 28|         HR| 55000|\n",
      "|  5|    Eve| 32|      Sales| 70000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Drop duplicates based on specific columns\")\n",
    "df_dups.dropDuplicates([\"name\", \"age\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Get distinct values of a column\n",
      "+-----------+\n",
      "| department|\n",
      "+-----------+\n",
      "|Engineering|\n",
      "|      Sales|\n",
      "|         HR|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Get distinct values of a column\")\n",
    "df.select(\"department\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PART 5: WORKING WITH LARGER DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Generating 1000 rows of employee data...\n",
      "âœ… Created dataset with 1000 rows\n",
      "   Partitions: 2\n"
     ]
    }
   ],
   "source": [
    "# Generate larger dataset using Faker\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "fake = Faker()\n",
    "Faker.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"ðŸ“Œ Generating 1000 rows of employee data...\")\n",
    "\n",
    "large_data = []\n",
    "departments = [\"Engineering\", \"Sales\", \"HR\", \"Marketing\", \"Finance\", \"IT\", \"Operations\"]\n",
    "\n",
    "for i in range(1, 1001):\n",
    "    large_data.append({\n",
    "        \"id\": i,\n",
    "        \"name\": fake.name(),\n",
    "        \"age\": random.randint(22, 65),\n",
    "        \"department\": random.choice(departments),\n",
    "        \"salary\": random.randint(40000, 150000),\n",
    "        \"city\": fake.city(),\n",
    "        \"country\": fake.country(),\n",
    "        \"email\": fake.email(),\n",
    "        \"join_date\": fake.date_between(start_date=\"-10y\", end_date=\"today\").isoformat()\n",
    "    })\n",
    "\n",
    "df_large = spark.createDataFrame(large_data)\n",
    "\n",
    "print(f\"âœ… Created dataset with {df_large.count()} rows\")\n",
    "print(f\"   Partitions: {df_large.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Sample data:\n",
      "+---+---------------+----------------+-----------+--------------------+---+----------+-----------------+------+\n",
      "|age|           city|         country| department|               email| id| join_date|             name|salary|\n",
      "+---+---------------+----------------+-----------+--------------------+---+----------+-----------------+------+\n",
      "| 62|      East Jill|     Philippines|Engineering|garzaanthony@exam...|  1|2018-02-27|     Allison Hill| 43278|\n",
      "| 39|  Robinsonshire|            Fiji|      Sales|blakeerik@example...|  2|2019-05-17|  Alyssa Gonzalez| 69256|\n",
      "| 30|South Colinstad|     Netherlands|         IT|lindsay78@example...|  3|2025-09-14|     Tyler Rogers| 53434|\n",
      "| 65|    Barbaraland|           Aruba|         IT|kendragalloway@ex...|  4|2024-08-21|    Juan Calderon|111482|\n",
      "| 27|   West Michael|        Paraguay|    Finance|jacqueline19@exam...|  5|2017-09-07|    Jesse Flowers| 95302|\n",
      "| 24|      Lake Mark|      Seychelles|Engineering|perezantonio@exam...|  6|2019-12-26|       Carla Gray| 52280|\n",
      "| 35|      Jasonfort|       Indonesia|      Sales| ithomas@example.org|  7|2023-06-04|   Thomas Bradley|106237|\n",
      "| 60|        Coxberg|          Malawi|Engineering|  ddavis@example.org|  8|2017-07-02| Nicole Patterson|113563|\n",
      "| 34|     Jamesmouth|Marshall Islands|         IT| tasha01@example.net|  9|2024-10-27|  Michael Carlson|125181|\n",
      "| 56|      Ryanmouth|     Afghanistan|  Marketing|teresa28@example.org| 10|2024-09-05|Patricia Peterson| 68893|\n",
      "+---+---------------+----------------+-----------+--------------------+---+----------+-----------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Š Sample data:\")\n",
    "df_large.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Schema:\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- join_date: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Š Schema:\")\n",
    "df_large.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Summary statistics:\n",
      "+-------+------------------+-----------+-----------+-----------+--------------------+-----------------+----------+----------------+------------------+\n",
      "|summary|               age|       city|    country| department|               email|               id| join_date|            name|            salary|\n",
      "+-------+------------------+-----------+-----------+-----------+--------------------+-----------------+----------+----------------+------------------+\n",
      "|  count|              1000|       1000|       1000|       1000|                1000|             1000|      1000|            1000|              1000|\n",
      "|   mean|            43.856|       NULL|       NULL|       NULL|                NULL|            500.5|      NULL|            NULL|         95900.788|\n",
      "| stddev|12.680222870756339|       NULL|       NULL|       NULL|                NULL|288.8194360957494|      NULL|            NULL|30966.077697393597|\n",
      "|    min|                22|  Adamburgh|Afghanistan|Engineering|  abrown@example.net|                1|2015-12-22| Aaron Schroeder|             40350|\n",
      "|    max|                65|Zacharyfurt|   Zimbabwe|      Sales|zthompson@example...|             1000|2025-12-20|Zachary Santiago|            149882|\n",
      "+-------+------------------+-----------+-----------+-----------+--------------------+-----------------+----------+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Š Summary statistics:\")\n",
    "df_large.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1: Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Random sample: 10% (without replacement)\n",
      "Sample size: 95 rows\n",
      "+---+--------------+---------------+-----------+--------------------+---+----------+-----------------+------+\n",
      "|age|          city|        country| department|               email| id| join_date|             name|salary|\n",
      "+---+--------------+---------------+-----------+--------------------+---+----------+-----------------+------+\n",
      "| 60|       Coxberg|         Malawi|Engineering|  ddavis@example.org|  8|2017-07-02| Nicole Patterson|113563|\n",
      "| 28|East Jamesside|       Kiribati|         HR|robertramirez@exa...| 17|2024-09-19|     Amy Chandler| 85082|\n",
      "| 24|     Smithfurt|        Estonia|         IT|smitchell@example...| 19|2025-12-05|  Mr. James Brown|100217|\n",
      "| 29|    Garciastad|North Macedonia|      Sales|bzimmerman@exampl...| 51|2023-04-24|Alexandria Fisher|122240|\n",
      "| 39|    Wilsonland|           Iran| Operations|  ojones@example.net| 59|2016-07-04|     Vincent King|124012|\n",
      "+---+--------------+---------------+-----------+--------------------+---+----------+-----------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Random sample: 10% (without replacement)\")\n",
    "df_sample = df_large.sample(fraction=0.1, seed=42)\n",
    "print(f\"Sample size: {df_sample.count()} rows\")\n",
    "df_sample.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Random sample: 20% (with replacement)\n",
      "Sample size: 204 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Random sample: 20% (with replacement)\")\n",
    "df_sample_replace = df_large.sample(withReplacement=True, fraction=0.2, seed=42)\n",
    "print(f\"Sample size: {df_sample_replace.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2: Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Limit to first 5 rows\n",
      "+---+---------------+-----------+-----------+--------------------+---+----------+---------------+------+\n",
      "|age|           city|    country| department|               email| id| join_date|           name|salary|\n",
      "+---+---------------+-----------+-----------+--------------------+---+----------+---------------+------+\n",
      "| 62|      East Jill|Philippines|Engineering|garzaanthony@exam...|  1|2018-02-27|   Allison Hill| 43278|\n",
      "| 39|  Robinsonshire|       Fiji|      Sales|blakeerik@example...|  2|2019-05-17|Alyssa Gonzalez| 69256|\n",
      "| 30|South Colinstad|Netherlands|         IT|lindsay78@example...|  3|2025-09-14|   Tyler Rogers| 53434|\n",
      "| 65|    Barbaraland|      Aruba|         IT|kendragalloway@ex...|  4|2024-08-21|  Juan Calderon|111482|\n",
      "| 27|   West Michael|   Paraguay|    Finance|jacqueline19@exam...|  5|2017-09-07|  Jesse Flowers| 95302|\n",
      "+---+---------------+-----------+-----------+--------------------+---+----------+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Limit to first 5 rows\")\n",
    "df_large.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3: Complex Queries (Chaining Operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Query: High earners in Engineering\n",
      "Found 57 high earners in Engineering\n",
      "+--------------------+---+------+----------------+\n",
      "|                name|age|salary|            city|\n",
      "+--------------------+---+------+----------------+\n",
      "|Christopher Williams| 33|149602|    Lake William|\n",
      "|        Nicole Lewis| 56|149392|       Sarahview|\n",
      "|      Vanessa Foster| 34|148028|    Stewarthaven|\n",
      "|        Krystal Diaz| 52|147756|      Susanburgh|\n",
      "|      Sarah Phillips| 50|146295|       West John|\n",
      "|       Wayne Russell| 50|146235|South Sherryberg|\n",
      "|       Linda Wilkins| 64|145464|         Troyton|\n",
      "|       Grace Swanson| 34|143692|    New Jennifer|\n",
      "|       Wayne Perkins| 42|141259|      Barkerberg|\n",
      "|         Sarah Jones| 59|140238|     West Jeremy|\n",
      "+--------------------+---+------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Query: High earners in Engineering\")\n",
    "result = df_large \\\n",
    "    .filter(col(\"department\") == \"Engineering\") \\\n",
    "    .filter(col(\"salary\") > 100000) \\\n",
    "    .select(\"name\", \"age\", \"salary\", \"city\") \\\n",
    "    .orderBy(col(\"salary\").desc())\n",
    "\n",
    "print(f\"Found {result.count()} high earners in Engineering\")\n",
    "result.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Query: Average salary by department (top 5)\n",
      "+-----------+--------------+-----------------+\n",
      "| department|employee_count|       avg_salary|\n",
      "+-----------+--------------+-----------------+\n",
      "| Operations|           137|99357.65693430656|\n",
      "|  Marketing|           143|97774.46153846153|\n",
      "|         IT|           151|96760.65562913907|\n",
      "|         HR|           142|96117.23943661971|\n",
      "|Engineering|           139|94520.53237410072|\n",
      "+-----------+--------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Œ Query: Average salary by department (top 5)\")\n",
    "df_large \\\n",
    "    .groupBy(\"department\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"employee_count\"),\n",
    "        avg(\"salary\").alias(\"avg_salary\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"avg_salary\").desc()) \\\n",
    "    .limit(5) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… SUMMARY\n",
    "\n",
    "### ðŸ“š What you learned:\n",
    "\n",
    "**1ï¸âƒ£ Spark Architecture:**\n",
    "- Driver, Cluster Manager, Executors\n",
    "- How jobs are distributed\n",
    "\n",
    "**2ï¸âƒ£ Creating DataFrames:**\n",
    "- From list of tuples\n",
    "- From list of dictionaries\n",
    "- With explicit schema (recommended)\n",
    "\n",
    "**3ï¸âƒ£ Basic Operations:**\n",
    "- Inspecting: show, printSchema, count, describe\n",
    "- Selecting: select, col\n",
    "- Filtering: filter, multiple conditions\n",
    "- Adding columns: withColumn\n",
    "- Modifying: withColumnRenamed\n",
    "- Dropping: drop\n",
    "- Sorting: orderBy, asc, desc\n",
    "- Distinct: distinct, dropDuplicates\n",
    "\n",
    "**4ï¸âƒ£ Working with Data:**\n",
    "- Generated 1000 rows with Faker\n",
    "- Sampling\n",
    "- Limiting\n",
    "- Complex queries with chaining\n",
    "\n",
    "### ðŸŽ¯ Key Takeaways:\n",
    "- âœ… DataFrame is immutable (operations return new DataFrame)\n",
    "- âœ… Use col() for complex expressions\n",
    "- âœ… Chain operations for readability\n",
    "- âœ… Always check schema with printSchema()\n",
    "\n",
    "### ðŸ“ Next: Transformations vs Actions & Lazy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
