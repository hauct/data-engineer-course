{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ CACHING & PERSISTENCE\n",
    "\n",
    "---\n",
    "\n",
    "## üìã **DAY 4 - LESSON 2: CACHING & PERSISTENCE**\n",
    "\n",
    "### **üéØ M·ª§C TI√äU:**\n",
    "\n",
    "1. **Hi·ªÉu Caching** - Khi n√†o d√πng, t·∫°i sao d√πng\n",
    "2. **Storage Levels** - MEMORY_ONLY, MEMORY_AND_DISK, DISK_ONLY...\n",
    "3. **cache() vs persist()** - Kh√°c nhau nh∆∞ th·∫ø n√†o\n",
    "4. **Best Practices** - Khi n√†o cache, khi n√†o kh√¥ng\n",
    "5. **Performance** - So s√°nh th·ª±c t·∫ø\n",
    "\n",
    "---\n",
    "\n",
    "## üí° **KH√ÅI NI·ªÜM C∆† B·∫¢N:**\n",
    "\n",
    "### **Caching l√† g√¨?**\n",
    "- L∆∞u DataFrame v√†o **memory** ho·∫∑c **disk**\n",
    "- T√°i s·ª≠ d·ª•ng nhi·ªÅu l·∫ßn **KH√îNG c·∫ßn ƒë·ªçc l·∫°i t·ª´ source**\n",
    "- TƒÉng t·ªëc ƒë·ªô **10-100x** cho iterative queries\n",
    "\n",
    "### **Khi n√†o d√πng?**\n",
    "‚úÖ DataFrame ƒë∆∞·ª£c d√πng **nhi·ªÅu l·∫ßn** trong c√πng 1 job\n",
    "‚úÖ Iterative algorithms (ML, graph processing)\n",
    "‚úÖ Interactive analysis (Jupyter notebook)\n",
    "‚úÖ Expensive transformations (join, aggregation)\n",
    "\n",
    "### **Khi n√†o KH√îNG d√πng?**\n",
    "‚ùå DataFrame ch·ªâ d√πng **1 l·∫ßn**\n",
    "‚ùå Data qu√° l·ªõn (kh√¥ng fit v√†o memory)\n",
    "‚ùå Simple transformations (filter, select)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß **SETUP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, to_date, year, month, desc, lit, when\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.storagelevel import StorageLevel as SL\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CachingPersistence\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark Session Created\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Executor Memory: {spark.conf.get('spark.executor.memory')}\")\n",
    "print(f\"Driver Memory: {spark.conf.get('spark.driver.memory')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç **CHECK AVAILABLE STORAGE LEVELS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üìã AVAILABLE STORAGE LEVELS IN YOUR PYSPARK VERSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_attrs = [attr for attr in dir(StorageLevel) if not attr.startswith('_')]\n",
    "for attr in all_attrs:\n",
    "    try:\n",
    "        value = getattr(StorageLevel, attr)\n",
    "        print(f\"‚úÖ StorageLevel.{attr}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"\\nüí° These are the storage levels available in your PySpark version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä **1. T·∫†O DATA M·∫™U**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîπ Generating sample data...\")\n",
    "\n",
    "# Generate 50,000 orders (larger dataset for caching demo)\n",
    "countries = [(\"USA\", 0.40), (\"UK\", 0.20), (\"Germany\", 0.15), (\"France\", 0.10), \n",
    "             (\"Canada\", 0.08), (\"Japan\", 0.05), (\"Australia\", 0.02)]\n",
    "categories = [(\"Electronics\", 0.35), (\"Clothing\", 0.25), (\"Books\", 0.15), \n",
    "              (\"Home\", 0.15), (\"Sports\", 0.10)]\n",
    "\n",
    "def weighted_choice(choices):\n",
    "    total = sum(w for c, w in choices)\n",
    "    r = random.uniform(0, total)\n",
    "    upto = 0\n",
    "    for c, w in choices:\n",
    "        if upto + w >= r:\n",
    "            return c\n",
    "        upto += w\n",
    "    return choices[-1][0]\n",
    "\n",
    "start_date = datetime(2024, 1, 1)\n",
    "num_orders = 50000\n",
    "\n",
    "data = []\n",
    "for i in range(num_orders):\n",
    "    days_offset = random.randint(0, 90)\n",
    "    order_date = start_date + timedelta(days=days_offset)\n",
    "    \n",
    "    data.append((\n",
    "        f\"ORD{i+1:06d}\",\n",
    "        f\"CUST{random.randint(1, 5000):05d}\",\n",
    "        order_date.strftime(\"%Y-%m-%d\"),\n",
    "        weighted_choice(countries),\n",
    "        weighted_choice(categories),\n",
    "        random.randint(1, 5),\n",
    "        round(random.uniform(10, 1000), 2),\n",
    "        random.choice([\"completed\", \"pending\", \"cancelled\"])\n",
    "    ))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), False),\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"order_date\", StringType(), False),\n",
    "    StructField(\"country\", StringType(), False),\n",
    "    StructField(\"category\", StringType(), False),\n",
    "    StructField(\"quantity\", IntegerType(), False),\n",
    "    StructField(\"amount\", DoubleType(), False),\n",
    "    StructField(\"status\", StringType(), False)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema) \\\n",
    "    .withColumn(\"order_date\", to_date(col(\"order_date\")))\n",
    "\n",
    "print(f\"‚úÖ Generated {df.count():,} orders\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ **2. CACHE() vs PERSIST() - C∆† B·∫¢N**\n",
    "\n",
    "### **cache() = persist(StorageLevel.MEMORY_AND_DISK)**\n",
    "\n",
    "```python\n",
    "df.cache()  # Shortcut\n",
    "# T∆∞∆°ng ƒë∆∞∆°ng:\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üîπ DEMO 1: cache() vs persist()\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create expensive transformation\n",
    "expensive_df = df \\\n",
    "    .filter(col(\"status\") == \"completed\") \\\n",
    "    .withColumn(\"total\", col(\"quantity\") * col(\"amount\")) \\\n",
    "    .withColumn(\"year\", year(col(\"order_date\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"order_date\")))\n",
    "\n",
    "print(\"\\nüìä Scenario 1: WITHOUT CACHE\")\n",
    "print(\"Running 3 queries on same DataFrame...\")\n",
    "\n",
    "# Query 1\n",
    "start = time.time()\n",
    "row_count1 = expensive_df.count()\n",
    "time1 = time.time() - start\n",
    "print(f\"Query 1 - Count: {row_count1:,} rows in {time1:.2f}s\")\n",
    "\n",
    "# Query 2\n",
    "start = time.time()\n",
    "sum_amount = expensive_df.agg(F.sum(\"total\")).collect()[0][0]\n",
    "time2 = time.time() - start\n",
    "print(f\"Query 2 - Sum: ${sum_amount:,.2f} in {time2:.2f}s\")\n",
    "\n",
    "# Query 3\n",
    "start = time.time()\n",
    "avg_amount = expensive_df.agg(F.avg(\"total\")).collect()[0][0]\n",
    "time3 = time.time() - start\n",
    "print(f\"Query 3 - Avg: ${avg_amount:,.2f} in {time3:.2f}s\")\n",
    "\n",
    "total_time_no_cache = time1 + time2 + time3\n",
    "print(f\"\\n‚è±Ô∏è  Total time WITHOUT cache: {total_time_no_cache:.2f}s\")\n",
    "print(\"üí° Each query reads from source and recomputes transformations!\")\n",
    "\n",
    "# Now with cache\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä Scenario 2: WITH CACHE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "expensive_df_cached = df \\\n",
    "    .filter(col(\"status\") == \"completed\") \\\n",
    "    .withColumn(\"total\", col(\"quantity\") * col(\"amount\")) \\\n",
    "    .withColumn(\"year\", year(col(\"order_date\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"order_date\"))) \\\n",
    "    .cache()  # ‚ö° Cache here!\n",
    "\n",
    "print(\"Running 3 queries on cached DataFrame...\")\n",
    "\n",
    "# Query 1 (triggers caching)\n",
    "start = time.time()\n",
    "row_count1 = expensive_df_cached.count()\n",
    "time1_cached = time.time() - start\n",
    "print(f\"Query 1 - Count: {row_count1:,} rows in {time1_cached:.2f}s (caching...)\")\n",
    "\n",
    "# Query 2 (uses cache)\n",
    "start = time.time()\n",
    "sum_amount = expensive_df_cached.agg(F.sum(\"total\")).collect()[0][0]\n",
    "time2_cached = time.time() - start\n",
    "print(f\"Query 2 - Sum: ${sum_amount:,.2f} in {time2_cached:.2f}s (from cache ‚ö°)\")\n",
    "\n",
    "# Query 3 (uses cache)\n",
    "start = time.time()\n",
    "avg_amount = expensive_df_cached.agg(F.avg(\"total\")).collect()[0][0]\n",
    "time3_cached = time.time() - start\n",
    "print(f\"Query 3 - Avg: ${avg_amount:,.2f} in {time3_cached:.2f}s (from cache ‚ö°)\")\n",
    "\n",
    "total_time_cached = time1_cached + time2_cached + time3_cached\n",
    "print(f\"\\n‚è±Ô∏è  Total time WITH cache: {total_time_cached:.2f}s\")\n",
    "print(f\"üöÄ Speedup: {total_time_no_cache/total_time_cached:.2f}x faster\")\n",
    "print(\"üí° Query 2 & 3 read from memory, no recomputation!\")\n",
    "\n",
    "# Check cache status\n",
    "print(\"\\nüìã Cache Status:\")\n",
    "print(f\"Is Cached: {expensive_df_cached.is_cached}\")\n",
    "print(f\"Storage Level: {expensive_df_cached.storageLevel}\")\n",
    "\n",
    "expensive_df_cached.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì¶ **3. STORAGE LEVELS - CHI TI·∫æT**\n",
    "\n",
    "### **C√°c Storage Levels c√≥ s·∫µn:**\n",
    "\n",
    "| Storage Level | Memory | Disk | Deserialized | Replication |\n",
    "|---------------|--------|------|--------------|-------------|\n",
    "| **MEMORY_ONLY** | ‚úÖ | ‚ùå | ‚úÖ | 1 |\n",
    "| **MEMORY_AND_DISK** | ‚úÖ | ‚úÖ | ‚úÖ | 1 |\n",
    "| **MEMORY_AND_DISK_DESER** | ‚úÖ | ‚úÖ | ‚ùå | 1 |\n",
    "| **DISK_ONLY** | ‚ùå | ‚úÖ | ‚ùå | 1 |\n",
    "| **MEMORY_ONLY_2** | ‚úÖ | ‚ùå | ‚úÖ | 2 |\n",
    "| **MEMORY_AND_DISK_2** | ‚úÖ | ‚úÖ | ‚úÖ | 2 |\n",
    "| **DISK_ONLY_2** | ‚ùå | ‚úÖ | ‚ùå | 2 |\n",
    "| **OFF_HEAP** | ‚úÖ | ‚ùå | ‚ùå | 1 |\n",
    "\n",
    "### **Khi n√†o d√πng g√¨?**\n",
    "\n",
    "- **MEMORY_ONLY**: Data fits in memory, need max speed\n",
    "- **MEMORY_AND_DISK**: Safe default, recommended for most cases\n",
    "- **MEMORY_AND_DISK_DESER**: Save memory (serialized), slower access\n",
    "- **DISK_ONLY**: Memory scarce, data large\n",
    "- **_2 variants**: Fault tolerance (replicate to 2 nodes)\n",
    "- **OFF_HEAP**: Advanced use, reduce GC pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üîπ DEMO 2: Storage Levels Comparison\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare DataFrame\n",
    "test_df = df.filter(col(\"status\") == \"completed\")\n",
    "\n",
    "# Test 1: MEMORY_ONLY\n",
    "print(\"\\nüìä Test 1: MEMORY_ONLY\")\n",
    "df_memory_only = test_df.persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "start = time.time()\n",
    "row_count = df_memory_only.count()\n",
    "time_memory_only = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ Count: {row_count:,} in {time_memory_only:.2f}s\")\n",
    "print(f\"Storage Level: {df_memory_only.storageLevel}\")\n",
    "print(\"üí° Fast but risky if data doesn't fit in memory\")\n",
    "\n",
    "df_memory_only.unpersist()\n",
    "\n",
    "# Test 2: MEMORY_AND_DISK (default for cache())\n",
    "print(\"\\nüìä Test 2: MEMORY_AND_DISK\")\n",
    "df_memory_disk = test_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "start = time.time()\n",
    "row_count = df_memory_disk.count()\n",
    "time_memory_disk = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ Count: {row_count:,} in {time_memory_disk:.2f}s\")\n",
    "print(f\"Storage Level: {df_memory_disk.storageLevel}\")\n",
    "print(\"üí° Safe choice - spills to disk if memory full\")\n",
    "\n",
    "df_memory_disk.unpersist()\n",
    "\n",
    "# Test 3: DISK_ONLY\n",
    "print(\"\\nüìä Test 3: DISK_ONLY\")\n",
    "df_disk_only = test_df.persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "start = time.time()\n",
    "row_count = df_disk_only.count()\n",
    "time_disk_only = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ Count: {row_count:,} in {time_disk_only:.2f}s\")\n",
    "print(f\"Storage Level: {df_disk_only.storageLevel}\")\n",
    "print(\"üí° Slower but no memory pressure\")\n",
    "\n",
    "df_disk_only.unpersist()\n",
    "\n",
    "# Test 4: MEMORY_AND_DISK_DESER (serialized)\n",
    "print(\"\\nüìä Test 4: MEMORY_AND_DISK_DESER (Serialized)\")\n",
    "df_memory_deser = test_df.persist(StorageLevel.MEMORY_AND_DISK_DESER)\n",
    "\n",
    "start = time.time()\n",
    "row_count = df_memory_deser.count()\n",
    "time_memory_deser = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ Count: {row_count:,} in {time_memory_deser:.2f}s\")\n",
    "print(f\"Storage Level: {df_memory_deser.storageLevel}\")\n",
    "print(\"üí° Uses less memory but slower (serialization overhead)\")\n",
    "\n",
    "df_memory_deser.unpersist()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä STORAGE LEVELS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison = [\n",
    "    (\"MEMORY_ONLY\", time_memory_only, \"Fastest\", \"Risky if OOM\"),\n",
    "    (\"MEMORY_AND_DISK\", time_memory_disk, \"Balanced\", \"Recommended\"),\n",
    "    (\"DISK_ONLY\", time_disk_only, \"Slowest\", \"No memory pressure\"),\n",
    "    (\"MEMORY_AND_DISK_DESER\", time_memory_deser, \"Compact\", \"CPU overhead\")\n",
    "]\n",
    "\n",
    "comparison_df = spark.createDataFrame(comparison,\n",
    "    [\"Storage Level\", \"Time (s)\", \"Speed\", \"Note\"])\n",
    "comparison_df.show(truncate=False)\n",
    "\n",
    "print(\"\"\"\n",
    "üí° RECOMMENDATIONS:\n",
    "\n",
    "1. MEMORY_ONLY:\n",
    "   ‚úÖ Use when: Data fits in memory, need max speed\n",
    "   ‚ùå Avoid when: Data is large, memory limited\n",
    "\n",
    "2. MEMORY_AND_DISK (default):\n",
    "   ‚úÖ Use when: General purpose, safe choice\n",
    "   ‚úÖ Best for: Most use cases\n",
    "\n",
    "3. DISK_ONLY:\n",
    "   ‚úÖ Use when: Memory is scarce, data is large\n",
    "   ‚ùå Avoid when: Need fast access\n",
    "\n",
    "4. MEMORY_AND_DISK_DESER:\n",
    "   ‚úÖ Use when: Memory limited, data compressible\n",
    "   ‚ùå Avoid when: CPU is bottleneck\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ **4. ITERATIVE ALGORITHMS - CACHING SHINES**\n",
    "\n",
    "### **Use Case: Machine Learning / Iterative Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üîπ DEMO 3: Iterative Algorithm (Simulated ML Training)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare training data\n",
    "training_data = df \\\n",
    "    .filter(col(\"status\") == \"completed\") \\\n",
    "    .select(\"customer_id\", \"category\", \"amount\", \"quantity\")\n",
    "\n",
    "print(f\"Training data: {training_data.count():,} rows\")\n",
    "\n",
    "# Scenario 1: WITHOUT CACHE\n",
    "print(\"\\nüìä Scenario 1: WITHOUT CACHE (10 iterations)\")\n",
    "start = time.time()\n",
    "\n",
    "for i in range(10):\n",
    "    # Simulate ML training iteration\n",
    "    result = training_data \\\n",
    "        .groupBy(\"category\") \\\n",
    "        .agg(F.avg(\"amount\").alias(\"avg_amount\")) \\\n",
    "        .collect()\n",
    "    \n",
    "    if i == 0:\n",
    "        print(f\"Iteration {i+1}: {len(result)} categories\")\n",
    "\n",
    "time_no_cache = time.time() - start\n",
    "print(f\"\\n‚è±Ô∏è  Total time: {time_no_cache:.2f}s\")\n",
    "print(\"üí° Each iteration reads from source!\")\n",
    "\n",
    "# Scenario 2: WITH CACHE\n",
    "print(\"\\nüìä Scenario 2: WITH CACHE (10 iterations)\")\n",
    "\n",
    "training_data_cached = df \\\n",
    "    .filter(col(\"status\") == \"completed\") \\\n",
    "    .select(\"customer_id\", \"category\", \"amount\", \"quantity\") \\\n",
    "    .cache()  # ‚ö° Cache training data\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(10):\n",
    "    # Simulate ML training iteration\n",
    "    result = training_data_cached \\\n",
    "        .groupBy(\"category\") \\\n",
    "        .agg(F.avg(\"amount\").alias(\"avg_amount\")) \\\n",
    "        .collect()\n",
    "    \n",
    "    if i == 0:\n",
    "        print(f\"Iteration {i+1}: {len(result)} categories (caching...)\")\n",
    "    elif i == 1:\n",
    "        print(f\"Iteration {i+1}: {len(result)} categories (from cache ‚ö°)\")\n",
    "\n",
    "time_cached = time.time() - start\n",
    "print(f\"\\n‚è±Ô∏è  Total time: {time_cached:.2f}s\")\n",
    "print(f\"üöÄ Speedup: {time_no_cache/time_cached:.2f}x faster\")\n",
    "print(\"üí° Only iteration 1 reads from source, rest use cache!\")\n",
    "\n",
    "training_data_cached.unpersist()\n",
    "\n",
    "print(\"\"\"\n",
    "üí° KEY INSIGHT:\n",
    "   - Iterative algorithms benefit MOST from caching\n",
    "   - First iteration: Normal speed (caching)\n",
    "   - Subsequent iterations: 10-100x faster (from cache)\n",
    "   - Essential for: ML training, graph algorithms, iterative analytics\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ **5. REAL-WORLD SCENARIOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üîπ DEMO 4: Real-World Scenarios\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Scenario 1: Interactive Analysis (Jupyter Notebook)\n",
    "print(\"\\nüìä Scenario 1: INTERACTIVE ANALYSIS\")\n",
    "print(\"Use case: Data scientist exploring data in Jupyter\")\n",
    "\n",
    "# Load and cache base data\n",
    "base_data = df.filter(col(\"status\") == \"completed\").cache()\n",
    "print(f\"‚úÖ Cached {base_data.count():,} completed orders\")\n",
    "\n",
    "# Multiple exploratory queries\n",
    "print(\"\\nRunning exploratory queries...\")\n",
    "\n",
    "start = time.time()\n",
    "# Query 1: By country\n",
    "base_data.groupBy(\"country\").count().show(5)\n",
    "time1 = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "# Query 2: By category\n",
    "base_data.groupBy(\"category\").agg(F.sum(\"amount\")).show(5)\n",
    "time2 = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "# Query 3: Top customers\n",
    "base_data.groupBy(\"customer_id\").agg(F.count(\"*\").alias(\"orders\")) \\\n",
    "    .orderBy(desc(\"orders\")).show(5)\n",
    "time3 = time.time() - start\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Query times: {time1:.2f}s, {time2:.2f}s, {time3:.2f}s\")\n",
    "print(\"üí° All queries use cached data - very fast!\")\n",
    "\n",
    "base_data.unpersist()\n",
    "\n",
    "# Scenario 2: ETL Pipeline with Reuse\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä Scenario 2: ETL PIPELINE\")\n",
    "print(\"Use case: Same base data used in multiple transformations\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Base transformation (expensive)\n",
    "base_transform = df \\\n",
    "    .filter(col(\"status\") == \"completed\") \\\n",
    "    .withColumn(\"total\", col(\"quantity\") * col(\"amount\")) \\\n",
    "    .withColumn(\"year\", year(col(\"order_date\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"order_date\"))) \\\n",
    "    .cache()  # ‚ö° Cache base transformation\n",
    "\n",
    "print(\"Base transformation cached\")\n",
    "\n",
    "# Branch 1: Country summary\n",
    "start = time.time()\n",
    "country_summary = base_transform \\\n",
    "    .groupBy(\"country\", \"year\", \"month\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"orders\"),\n",
    "        F.sum(\"total\").alias(\"revenue\")\n",
    "    )\n",
    "country_count = country_summary.count()\n",
    "time_branch1 = time.time() - start\n",
    "\n",
    "print(f\"\\n‚úÖ Branch 1 (Country Summary): {country_count} rows in {time_branch1:.2f}s\")\n",
    "\n",
    "# Branch 2: Category summary\n",
    "start = time.time()\n",
    "category_summary = base_transform \\\n",
    "    .groupBy(\"category\", \"year\", \"month\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"orders\"),\n",
    "        F.sum(\"total\").alias(\"revenue\")\n",
    "    )\n",
    "category_count = category_summary.count()\n",
    "time_branch2 = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ Branch 2 (Category Summary): {category_count} rows in {time_branch2:.2f}s\")\n",
    "\n",
    "# Branch 3: Customer summary\n",
    "start = time.time()\n",
    "customer_summary = base_transform \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"orders\"),\n",
    "        F.sum(\"total\").alias(\"lifetime_value\")\n",
    "    )\n",
    "customer_count = customer_summary.count()\n",
    "time_branch3 = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ Branch 3 (Customer Summary): {customer_count} rows in {time_branch3:.2f}s\")\n",
    "\n",
    "print(f\"\\nüí° All branches reuse cached base transformation!\")\n",
    "print(f\"   Total time: {time_branch1 + time_branch2 + time_branch3:.2f}s\")\n",
    "\n",
    "base_transform.unpersist()\n",
    "\n",
    "# Scenario 3: Join with cached dimension\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä Scenario 3: DIMENSION TABLE CACHING\")\n",
    "print(\"Use case: Small dimension table joined multiple times\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create dimension table (customers)\n",
    "customers = spark.createDataFrame([\n",
    "    (f\"CUST{i:05d}\", f\"Customer {i}\", random.choice([\"Gold\", \"Silver\", \"Bronze\"]))\n",
    "    for i in range(1, 5001)\n",
    "], [\"customer_id\", \"customer_name\", \"tier\"]).cache()  # ‚ö° Cache dimension\n",
    "\n",
    "print(f\"‚úÖ Cached dimension table: {customers.count():,} customers\")\n",
    "\n",
    "# Multiple joins\n",
    "print(\"\\nPerforming multiple joins...\")\n",
    "\n",
    "start = time.time()\n",
    "# Join 1: Orders by tier\n",
    "join1 = df.join(customers, \"customer_id\") \\\n",
    "    .groupBy(\"tier\").count()\n",
    "join1.show()\n",
    "time_join1 = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "# Join 2: Revenue by tier\n",
    "join2 = df.join(customers, \"customer_id\") \\\n",
    "    .groupBy(\"tier\").agg(F.sum(\"amount\"))\n",
    "join2.show()\n",
    "time_join2 = time.time() - start\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Join times: {time_join1:.2f}s, {time_join2:.2f}s\")\n",
    "print(\"üí° Dimension table cached - joins are fast!\")\n",
    "\n",
    "customers.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è **6. COMMON MISTAKES & BEST PRACTICES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"‚ö†Ô∏è COMMON MISTAKES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "‚ùå MISTAKE 1: Caching data used only once\n",
    "-------------------------------------------\n",
    "# BAD:\n",
    "df.cache().filter(col(\"status\") == \"completed\").count()\n",
    "# ‚Üí Wasted memory, no benefit\n",
    "\n",
    "# GOOD:\n",
    "df.filter(col(\"status\") == \"completed\").count()\n",
    "# ‚Üí No cache needed for single use\n",
    "\n",
    "\n",
    "‚ùå MISTAKE 2: Caching before filter\n",
    "-------------------------------------------\n",
    "# BAD:\n",
    "df.cache().filter(col(\"country\") == \"USA\")  # Caches ALL data\n",
    "\n",
    "# GOOD:\n",
    "df.filter(col(\"country\") == \"USA\").cache()  # Caches only USA data\n",
    "\n",
    "\n",
    "‚ùå MISTAKE 3: Forgetting to unpersist\n",
    "-------------------------------------------\n",
    "# BAD:\n",
    "df.cache()\n",
    "# ... use df ...\n",
    "# ‚Üí Memory leak! Cache stays forever\n",
    "\n",
    "# GOOD:\n",
    "df.cache()\n",
    "# ... use df ...\n",
    "df.unpersist()  # Free memory\n",
    "\n",
    "\n",
    "‚ùå MISTAKE 4: Caching too much data\n",
    "-------------------------------------------\n",
    "# BAD:\n",
    "huge_df.cache()  # 100GB data, only 10GB memory\n",
    "# ‚Üí OOM or slow disk spill\n",
    "\n",
    "# GOOD:\n",
    "huge_df.filter(...).cache()  # Cache only needed subset\n",
    "\n",
    "\n",
    "‚ùå MISTAKE 5: Caching after expensive shuffle\n",
    "-------------------------------------------\n",
    "# BAD:\n",
    "df.groupBy(\"key\").agg(...).cache()  # Caches after shuffle\n",
    "\n",
    "# GOOD:\n",
    "df.cache().groupBy(\"key\").agg(...)  # Cache before shuffle\n",
    "# ‚Üí Reuse cached data for multiple aggregations\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ BEST PRACTICES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "1. WHEN TO CACHE:\n",
    "   ‚úÖ DataFrame used 2+ times\n",
    "   ‚úÖ Iterative algorithms (ML, graph)\n",
    "   ‚úÖ Interactive analysis (Jupyter)\n",
    "   ‚úÖ Expensive transformations (join, aggregation)\n",
    "   ‚úÖ Dimension tables (small, frequently joined)\n",
    "\n",
    "2. WHEN NOT TO CACHE:\n",
    "   ‚ùå DataFrame used only once\n",
    "   ‚ùå Data too large for memory\n",
    "   ‚ùå Simple transformations (filter, select)\n",
    "   ‚ùå Streaming data (constantly changing)\n",
    "\n",
    "3. CACHE PLACEMENT:\n",
    "   ‚úÖ Cache AFTER filter (reduce data size)\n",
    "   ‚úÖ Cache BEFORE multiple branches\n",
    "   ‚úÖ Cache BEFORE iterative operations\n",
    "   ‚ùå Don't cache raw data (cache transformed)\n",
    "\n",
    "4. STORAGE LEVEL:\n",
    "   ‚úÖ Default (MEMORY_AND_DISK) for most cases\n",
    "   ‚úÖ MEMORY_ONLY if data fits and speed critical\n",
    "   ‚úÖ DISK_ONLY if memory scarce\n",
    "   ‚úÖ MEMORY_AND_DISK_DESER if memory limited\n",
    "\n",
    "5. MEMORY MANAGEMENT:\n",
    "   ‚úÖ Always unpersist() when done\n",
    "   ‚úÖ Monitor cache usage (Spark UI)\n",
    "   ‚úÖ Cache only necessary columns\n",
    "   ‚úÖ Use appropriate storage level\n",
    "\n",
    "6. MONITORING:\n",
    "   ‚úÖ Check Spark UI ‚Üí Storage tab\n",
    "   ‚úÖ Monitor memory usage\n",
    "   ‚úÖ Check cache hit rate\n",
    "   ‚úÖ Profile query performance\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä **7. CACHE MONITORING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üîπ DEMO 5: Cache Monitoring\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Cache multiple DataFrames\n",
    "df1 = df.filter(col(\"country\") == \"USA\").cache()\n",
    "df2 = df.filter(col(\"category\") == \"Electronics\").cache()\n",
    "df3 = df.filter(col(\"status\") == \"completed\").cache()\n",
    "\n",
    "# Trigger caching\n",
    "print(\"Caching DataFrames...\")\n",
    "print(f\"df1 (USA): {df1.count():,} rows\")\n",
    "print(f\"df2 (Electronics): {df2.count():,} rows\")\n",
    "print(f\"df3 (Completed): {df3.count():,} rows\")\n",
    "\n",
    "# Check cache status\n",
    "print(\"\\nüìã Cache Status:\")\n",
    "print(f\"df1 is cached: {df1.is_cached}\")\n",
    "print(f\"df2 is cached: {df2.is_cached}\")\n",
    "print(f\"df3 is cached: {df3.is_cached}\")\n",
    "\n",
    "print(f\"\\ndf1 storage level: {df1.storageLevel}\")\n",
    "print(f\"df2 storage level: {df2.storageLevel}\")\n",
    "print(f\"df3 storage level: {df3.storageLevel}\")\n",
    "\n",
    "# Unpersist all\n",
    "print(\"\\nüßπ Cleaning up cache...\")\n",
    "df1.unpersist()\n",
    "df2.unpersist()\n",
    "df3.unpersist()\n",
    "\n",
    "print(\"‚úÖ All caches cleared\")\n",
    "\n",
    "print(\"\"\"\n",
    "üí° MONITORING TIPS:\n",
    "\n",
    "1. Spark UI (http://localhost:4040):\n",
    "   - Storage tab: See cached RDDs/DataFrames\n",
    "   - Memory usage: Check if cache fits\n",
    "   - Eviction: See if data is being evicted\n",
    "\n",
    "2. Programmatic checks:\n",
    "   - df.is_cached: Check if cached\n",
    "   - df.storageLevel: Check storage level\n",
    "   - spark.catalog.isCached(\"table_name\"): Check table cache\n",
    "\n",
    "3. Best practices:\n",
    "   - Monitor cache hit rate\n",
    "   - Check for memory pressure\n",
    "   - Unpersist unused caches\n",
    "   - Use appropriate storage levels\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî¨ **8. ADVANCED: CUSTOM STORAGE LEVELS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üîπ DEMO 6: Custom Storage Levels\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create custom storage levels\n",
    "# StorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication)\n",
    "\n",
    "print(\"\\nüìã Creating custom storage levels...\")\n",
    "\n",
    "custom_levels = {\n",
    "    \"Memory Only (Deserialized)\": SL(False, True, False, True, 1),\n",
    "    \"Memory Only (Serialized)\": SL(False, True, False, False, 1),\n",
    "    \"Memory + Disk (Deserialized)\": SL(True, True, False, True, 1),\n",
    "    \"Memory + Disk (Serialized)\": SL(True, True, False, False, 1),\n",
    "    \"Disk Only\": SL(True, False, False, False, 1),\n",
    "    \"Memory Only 2x Replicated\": SL(False, True, False, True, 2),\n",
    "}\n",
    "\n",
    "for name, level in custom_levels.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  useDisk: {level.useDisk}\")\n",
    "    print(f\"  useMemory: {level.useMemory}\")\n",
    "    print(f\"  useOffHeap: {level.useOffHeap}\")\n",
    "    print(f\"  deserialized: {level.deserialized}\")\n",
    "    print(f\"  replication: {level.replication}\")\n",
    "\n",
    "# Test custom storage level\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Testing custom storage level...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_df = df.filter(col(\"status\") == \"completed\")\n",
    "\n",
    "# Use custom serialized memory+disk\n",
    "custom_level = SL(True, True, False, False, 1)  # Disk, Memory, Serialized\n",
    "df_custom = test_df.persist(custom_level)\n",
    "\n",
    "start = time.time()\n",
    "row_count = df_custom.count()\n",
    "time_custom = time.time() - start\n",
    "\n",
    "print(f\"\\n‚úÖ Count: {row_count:,} in {time_custom:.2f}s\")\n",
    "print(f\"Storage Level: {df_custom.storageLevel}\")\n",
    "print(f\"  - Use Disk: {df_custom.storageLevel.useDisk}\")\n",
    "print(f\"  - Use Memory: {df_custom.storageLevel.useMemory}\")\n",
    "print(f\"  - Deserialized: {df_custom.storageLevel.deserialized}\")\n",
    "\n",
    "df_custom.unpersist()\n",
    "\n",
    "print(\"\"\"\n",
    "üí° CUSTOM STORAGE LEVEL PARAMETERS:\n",
    "\n",
    "StorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication)\n",
    "\n",
    "1. useDisk (bool):\n",
    "   - True: Can spill to disk\n",
    "   - False: Memory only\n",
    "\n",
    "2. useMemory (bool):\n",
    "   - True: Use memory\n",
    "   - False: Disk only\n",
    "\n",
    "3. useOffHeap (bool):\n",
    "   - True: Use off-heap memory (advanced)\n",
    "   - False: Use JVM heap\n",
    "\n",
    "4. deserialized (bool):\n",
    "   - True: Store as Java objects (faster access, more memory)\n",
    "   - False: Store as serialized bytes (slower access, less memory)\n",
    "\n",
    "5. replication (int):\n",
    "   - 1: No replication\n",
    "   - 2: Replicate to 2 nodes (fault tolerance)\n",
    "   - 3+: More replicas\n",
    "\n",
    "COMMON COMBINATIONS:\n",
    "- (False, True, False, True, 1)  = MEMORY_ONLY\n",
    "- (True, True, False, True, 1)   = MEMORY_AND_DISK\n",
    "- (True, True, False, False, 1)  = MEMORY_AND_DISK_DESER (Serialized)\n",
    "- (True, False, False, False, 1) = DISK_ONLY\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì **KEY TAKEAWAYS**\n",
    "\n",
    "### **‚úÖ What You Learned:**\n",
    "\n",
    "1. **Caching Basics**\n",
    "   - cache() = persist(MEMORY_AND_DISK)\n",
    "   - Stores DataFrame in memory/disk\n",
    "   - Reuse without recomputation\n",
    "\n",
    "2. **Storage Levels**\n",
    "   - MEMORY_ONLY: Fastest, risky\n",
    "   - MEMORY_AND_DISK: Balanced, recommended\n",
    "   - MEMORY_AND_DISK_DESER: Compact, CPU overhead\n",
    "   - DISK_ONLY: Slowest, safe\n",
    "\n",
    "3. **When to Cache**\n",
    "   - ‚úÖ Used 2+ times\n",
    "   - ‚úÖ Iterative algorithms\n",
    "   - ‚úÖ Interactive analysis\n",
    "   - ‚ùå Used once\n",
    "   - ‚ùå Too large\n",
    "\n",
    "4. **Best Practices**\n",
    "   - Cache after filter\n",
    "   - Cache before branches\n",
    "   - Always unpersist\n",
    "   - Monitor usage\n",
    "\n",
    "### **üìä Quick Reference:**\n",
    "\n",
    "```python\n",
    "# Import\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Cache\n",
    "df.cache()  # Default: MEMORY_AND_DISK\n",
    "df.persist(StorageLevel.MEMORY_ONLY)  # Custom level\n",
    "\n",
    "# Check\n",
    "df.is_cached\n",
    "df.storageLevel\n",
    "\n",
    "# Uncache\n",
    "df.unpersist()\n",
    "\n",
    "# Use F. prefix for functions\n",
    "df.agg(F.count(\"*\"), F.sum(\"amount\"))\n",
    "```\n",
    "\n",
    "### **üöÄ Next:** Day 4 - Lesson 3: Broadcast Variables & Accumulators\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup\n",
    "spark.catalog.clearCache()\n",
    "spark.stop()\n",
    "\n",
    "print(\"‚úÖ Spark session stopped\")\n",
    "print(\"\\nüéâ DAY 4 - LESSON 2 COMPLETED!\")\n",
    "print(\"\\nüí° Remember:\")\n",
    "print(\"   - Cache when used 2+ times\")\n",
    "print(\"   - MEMORY_AND_DISK is safe default\")\n",
    "print(\"   - Always unpersist when done\")\n",
    "print(\"   - Use F. prefix for functions (F.count, F.sum, etc.)\")\n",
    "print(\"   - Monitor cache usage in Spark UI\")\n",
    "print(\"\\nüî• Quote: 'Cache is like coffee - use it wisely, not excessively!' ‚òï\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}