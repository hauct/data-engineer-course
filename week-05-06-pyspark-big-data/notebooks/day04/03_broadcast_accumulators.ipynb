{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ BROADCAST VARIABLES & ACCUMULATORS\n",
    "\n",
    "---\n",
    "\n",
    "## üìã **DAY 4 - LESSON 3: BROADCAST VARIABLES & ACCUMULATORS**\n",
    "\n",
    "### **üéØ M·ª§C TI√äU:**\n",
    "\n",
    "1. **Broadcast Variables** - Share read-only data efficiently\n",
    "2. **Accumulators** - Aggregate information from workers\n",
    "3. **Use Cases** - Khi n√†o d√πng, t·∫°i sao d√πng\n",
    "4. **Best Practices** - Tr√°nh pitfalls\n",
    "5. **Performance** - So s√°nh th·ª±c t·∫ø\n",
    "\n",
    "---\n",
    "\n",
    "## üí° **KH√ÅI NI·ªÜM C∆† B·∫¢N:**\n",
    "\n",
    "### **Broadcast Variables:**\n",
    "- **Read-only** variable cached on each worker\n",
    "- Sent **once** to each executor (not per task)\n",
    "- Efficient for **small lookup tables** (< 2GB)\n",
    "- Use case: Join with small dimension table\n",
    "\n",
    "### **Accumulators:**\n",
    "- **Write-only** variable for aggregating info\n",
    "- Workers can **add** to it\n",
    "- Driver can **read** final value\n",
    "- Use case: Counting errors, metrics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß **SETUP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/11 11:04:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark Session Created\n",
      "Spark Version: 3.5.1\n",
      "Executor Memory: 2g\n",
      "Driver Memory: 1g\n",
      "Broadcast Join Threshold: 10485760 bytes\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, to_date, year, month, desc, lit, when, udf\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BroadcastAccumulators\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark Session Created\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Executor Memory: {spark.conf.get('spark.executor.memory')}\")\n",
    "print(f\"Driver Memory: {spark.conf.get('spark.driver.memory')}\")\n",
    "print(f\"Broadcast Join Threshold: {spark.conf.get('spark.sql.autoBroadcastJoinThreshold')} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä **1. T·∫†O DATA M·∫™U**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Generating sample data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 10:52:31 WARN TaskSetManager: Stage 0 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 100,000 transactions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 10:52:34 WARN TaskSetManager: Stage 3 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------+----------------+-------+-----------+--------+------+---------+\n",
      "|transaction_id|customer_id|product_id|transaction_date|country|   category|quantity|amount|   status|\n",
      "+--------------+-----------+----------+----------------+-------+-----------+--------+------+---------+\n",
      "|    TXN0000001|  CUST09170|  PROD0125|      2024-01-29|    USA|      Books|       9| 765.0|  pending|\n",
      "|    TXN0000002|  CUST07761|  PROD0206|      2024-02-09|     UK|Electronics|       7|813.99|   failed|\n",
      "|    TXN0000003|  CUST02907|  PROD0614|      2024-01-20|    USA|Electronics|       8|549.05|cancelled|\n",
      "|    TXN0000004|  CUST08914|  PROD0134|      2024-02-14| France|Electronics|      10|941.34|cancelled|\n",
      "|    TXN0000005|  CUST02062|  PROD0778|      2024-03-16|    USA|      Books|       2|595.77|   failed|\n",
      "+--------------+-----------+----------+----------------+-------+-----------+--------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "üîπ Creating dimension tables...\n",
      "‚úÖ Products: 1,000 rows\n",
      "‚úÖ Customers: 10,000 rows\n",
      "‚úÖ Country codes: 7 entries\n"
     ]
    }
   ],
   "source": [
    "print(\"üîπ Generating sample data...\")\n",
    "\n",
    "# Generate 100,000 transactions\n",
    "countries = [(\"USA\", 0.40), (\"UK\", 0.20), (\"Germany\", 0.15), (\"France\", 0.10), \n",
    "             (\"Canada\", 0.08), (\"Japan\", 0.05), (\"Australia\", 0.02)]\n",
    "categories = [(\"Electronics\", 0.35), (\"Clothing\", 0.25), (\"Books\", 0.15), \n",
    "              (\"Home\", 0.15), (\"Sports\", 0.10)]\n",
    "\n",
    "def weighted_choice(choices):\n",
    "    total = sum(w for c, w in choices)\n",
    "    r = random.uniform(0, total)\n",
    "    upto = 0\n",
    "    for c, w in choices:\n",
    "        if upto + w >= r:\n",
    "            return c\n",
    "        upto += w\n",
    "    return choices[-1][0]\n",
    "\n",
    "start_date = datetime(2024, 1, 1)\n",
    "num_transactions = 100000\n",
    "\n",
    "data = []\n",
    "for i in range(num_transactions):\n",
    "    days_offset = random.randint(0, 90)\n",
    "    transaction_date = start_date + timedelta(days=days_offset)\n",
    "    \n",
    "    data.append((\n",
    "        f\"TXN{i+1:07d}\",\n",
    "        f\"CUST{random.randint(1, 10000):05d}\",\n",
    "        f\"PROD{random.randint(1, 1000):04d}\",\n",
    "        transaction_date.strftime(\"%Y-%m-%d\"),\n",
    "        weighted_choice(countries),\n",
    "        weighted_choice(categories),\n",
    "        random.randint(1, 10),\n",
    "        round(random.uniform(10, 1000), 2),\n",
    "        random.choice([\"completed\", \"pending\", \"cancelled\", \"failed\"])\n",
    "    ))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), False),\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"product_id\", StringType(), False),\n",
    "    StructField(\"transaction_date\", StringType(), False),\n",
    "    StructField(\"country\", StringType(), False),\n",
    "    StructField(\"category\", StringType(), False),\n",
    "    StructField(\"quantity\", IntegerType(), False),\n",
    "    StructField(\"amount\", DoubleType(), False),\n",
    "    StructField(\"status\", StringType(), False)\n",
    "])\n",
    "\n",
    "transactions = spark.createDataFrame(data, schema) \\\n",
    "    .withColumn(\"transaction_date\", to_date(col(\"transaction_date\")))\n",
    "\n",
    "print(f\"‚úÖ Generated {transactions.count():,} transactions\")\n",
    "transactions.show(5)\n",
    "\n",
    "# Create small dimension tables\n",
    "print(\"\\nüîπ Creating dimension tables...\")\n",
    "\n",
    "# Products dimension (1,000 products)\n",
    "products = spark.createDataFrame([\n",
    "    (f\"PROD{i:04d}\", f\"Product {i}\", random.choice(categories)[0], \n",
    "     round(random.uniform(10, 500), 2))\n",
    "    for i in range(1, 1001)\n",
    "], [\"product_id\", \"product_name\", \"category\", \"unit_price\"])\n",
    "\n",
    "print(f\"‚úÖ Products: {products.count():,} rows\")\n",
    "\n",
    "# Customers dimension (10,000 customers)\n",
    "customers = spark.createDataFrame([\n",
    "    (f\"CUST{i:05d}\", f\"Customer {i}\", \n",
    "     random.choice([\"Gold\", \"Silver\", \"Bronze\"]),\n",
    "     random.choice([c[0] for c in countries]))\n",
    "    for i in range(1, 10001)\n",
    "], [\"customer_id\", \"customer_name\", \"tier\", \"country\"])\n",
    "\n",
    "print(f\"‚úÖ Customers: {customers.count():,} rows\")\n",
    "\n",
    "# Country codes (small lookup table)\n",
    "country_codes = {\n",
    "    \"USA\": \"US\",\n",
    "    \"UK\": \"GB\",\n",
    "    \"Germany\": \"DE\",\n",
    "    \"France\": \"FR\",\n",
    "    \"Canada\": \"CA\",\n",
    "    \"Japan\": \"JP\",\n",
    "    \"Australia\": \"AU\"\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Country codes: {len(country_codes)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì° **2. BROADCAST VARIABLES - C∆† B·∫¢N**\n",
    "\n",
    "### **Khi n√†o d√πng Broadcast?**\n",
    "- Small lookup table (< 2GB)\n",
    "- Join with large table\n",
    "- Avoid shuffle\n",
    "\n",
    "### **Syntax:**\n",
    "```python\n",
    "# Create broadcast variable\n",
    "broadcast_var = spark.sparkContext.broadcast(data)\n",
    "\n",
    "# Access value\n",
    "broadcast_var.value\n",
    "\n",
    "# Destroy\n",
    "broadcast_var.unpersist()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîπ DEMO 1: Broadcast Variables - Basic\n",
      "================================================================================\n",
      "\n",
      "üìä Scenario: WITHOUT BROADCAST\n",
      "Using UDF with regular Python dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 10:52:36 WARN TaskSetManager: Stage 10 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 100,000 rows in 0.58s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 10:52:37 WARN TaskSetManager: Stage 13 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------------+\n",
      "|transaction_id|country|country_code|\n",
      "+--------------+-------+------------+\n",
      "|    TXN0000001|    USA|          US|\n",
      "|    TXN0000002|     UK|          GB|\n",
      "|    TXN0000003|    USA|          US|\n",
      "|    TXN0000004| France|          FR|\n",
      "|    TXN0000005|    USA|          US|\n",
      "+--------------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Scenario: WITH BROADCAST\n",
      "Using broadcast variable...\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Broadcast variable created\n",
      "   Size: 7 entries\n",
      "   Sample: [('USA', 'US'), ('UK', 'GB'), ('Germany', 'DE')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 10:52:38 WARN TaskSetManager: Stage 14 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Processed 100,000 rows in 0.53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 10:52:38 WARN TaskSetManager: Stage 17 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------------+\n",
      "|transaction_id|country|country_code|\n",
      "+--------------+-------+------------+\n",
      "|    TXN0000001|    USA|          US|\n",
      "|    TXN0000002|     UK|          GB|\n",
      "|    TXN0000003|    USA|          US|\n",
      "|    TXN0000004| France|          FR|\n",
      "|    TXN0000005|    USA|          US|\n",
      "+--------------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä COMPARISON\n",
      "================================================================================\n",
      "+-------------+------------------+---------------------------+\n",
      "|Method       |Time (s)          |Note                       |\n",
      "+-------------+------------------+---------------------------+\n",
      "|Regular UDF  |0.5765690803527832|Dict sent with each task   |\n",
      "|Broadcast UDF|0.527691125869751 |Dict sent once per executor|\n",
      "+-------------+------------------+---------------------------+\n",
      "\n",
      "üöÄ Broadcast is 1.09x faster!\n",
      "\n",
      "üí° KEY INSIGHTS:\n",
      "   - Broadcast sends data ONCE per executor\n",
      "   - Regular UDF sends data with EACH task\n",
      "   - Benefit increases with:\n",
      "     * More tasks\n",
      "     * Larger lookup data\n",
      "     * More executors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üîπ DEMO 1: Broadcast Variables - Basic\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Scenario: Map country names to country codes\n",
    "print(\"\\nüìä Scenario: WITHOUT BROADCAST\")\n",
    "print(\"Using UDF with regular Python dict...\")\n",
    "\n",
    "# Regular UDF (inefficient - dict sent with each task)\n",
    "def get_country_code_regular(country):\n",
    "    codes = {\n",
    "        \"USA\": \"US\", \"UK\": \"GB\", \"Germany\": \"DE\",\n",
    "        \"France\": \"FR\", \"Canada\": \"CA\", \"Japan\": \"JP\",\n",
    "        \"Australia\": \"AU\"\n",
    "    }\n",
    "    return codes.get(country, \"UNKNOWN\")\n",
    "\n",
    "get_code_udf = udf(get_country_code_regular, StringType())\n",
    "\n",
    "start = time.time()\n",
    "result_regular = transactions \\\n",
    "    .withColumn(\"country_code\", get_code_udf(col(\"country\"))) \\\n",
    "    .select(\"transaction_id\", \"country\", \"country_code\")\n",
    "row_count_regular = result_regular.count()\n",
    "time_regular = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ Processed {row_count_regular:,} rows in {time_regular:.2f}s\")\n",
    "result_regular.show(5)\n",
    "\n",
    "# With Broadcast\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä Scenario: WITH BROADCAST\")\n",
    "print(\"Using broadcast variable...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Broadcast the dictionary\n",
    "broadcast_codes = spark.sparkContext.broadcast(country_codes)\n",
    "\n",
    "print(f\"\\n‚úÖ Broadcast variable created\")\n",
    "print(f\"   Size: {len(broadcast_codes.value)} entries\")\n",
    "print(f\"   Sample: {list(broadcast_codes.value.items())[:3]}\")\n",
    "\n",
    "# UDF using broadcast variable\n",
    "def get_country_code_broadcast(country):\n",
    "    return broadcast_codes.value.get(country, \"UNKNOWN\")\n",
    "\n",
    "get_code_broadcast_udf = udf(get_country_code_broadcast, StringType())\n",
    "\n",
    "start = time.time()\n",
    "result_broadcast = transactions \\\n",
    "    .withColumn(\"country_code\", get_code_broadcast_udf(col(\"country\"))) \\\n",
    "    .select(\"transaction_id\", \"country\", \"country_code\")\n",
    "row_count_broadcast = result_broadcast.count()\n",
    "time_broadcast = time.time() - start\n",
    "\n",
    "print(f\"\\n‚úÖ Processed {row_count_broadcast:,} rows in {time_broadcast:.2f}s\")\n",
    "result_broadcast.show(5)\n",
    "\n",
    "# Comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison = [\n",
    "    (\"Regular UDF\", time_regular, \"Dict sent with each task\"),\n",
    "    (\"Broadcast UDF\", time_broadcast, \"Dict sent once per executor\")\n",
    "]\n",
    "\n",
    "comparison_df = spark.createDataFrame(comparison,\n",
    "    [\"Method\", \"Time (s)\", \"Note\"])\n",
    "comparison_df.show(truncate=False)\n",
    "\n",
    "if time_regular > time_broadcast:\n",
    "    speedup = time_regular / time_broadcast\n",
    "    print(f\"üöÄ Broadcast is {speedup:.2f}x faster!\")\n",
    "else:\n",
    "    print(\"üí° For small data, difference may be minimal\")\n",
    "\n",
    "print(\"\"\"\n",
    "üí° KEY INSIGHTS:\n",
    "   - Broadcast sends data ONCE per executor\n",
    "   - Regular UDF sends data with EACH task\n",
    "   - Benefit increases with:\n",
    "     * More tasks\n",
    "     * Larger lookup data\n",
    "     * More executors\n",
    "\"\"\")\n",
    "\n",
    "# Cleanup\n",
    "broadcast_codes.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîó **3. BROADCAST JOIN**\n",
    "\n",
    "### **Broadcast Join vs Regular Join:**\n",
    "\n",
    "**Regular Join (Shuffle):**\n",
    "```\n",
    "Large Table (1GB) ‚îÄ‚îÄ‚îê\n",
    "                    ‚îú‚îÄ‚îÄ> Shuffle Both ‚îÄ‚îÄ> Join\n",
    "Small Table (10MB) ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Broadcast Join (No Shuffle):**\n",
    "```\n",
    "Large Table (1GB) ‚îÄ‚îÄ> No Shuffle ‚îÄ‚îÄ‚îê\n",
    "                                   ‚îú‚îÄ‚îÄ> Join\n",
    "Small Table (10MB) ‚îÄ‚îÄ> Broadcast ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîπ DEMO 2: Broadcast Join\n",
      "================================================================================\n",
      "\n",
      "üìä Scenario 1: REGULAR JOIN (with shuffle)\n",
      "Joining transactions (100K) with products (1K)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 10:52:40 WARN TaskSetManager: Stage 20 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Result: 100,000 rows in 2.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 10:52:42 WARN TaskSetManager: Stage 29 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+------------+------+\n",
      "|transaction_id|product_id|product_name|amount|\n",
      "+--------------+----------+------------+------+\n",
      "|    TXN0074140|  PROD0707| Product 707|362.48|\n",
      "|    TXN0073389|  PROD0707| Product 707|621.36|\n",
      "|    TXN0072954|  PROD0707| Product 707|166.78|\n",
      "|    TXN0072689|  PROD0707| Product 707|128.84|\n",
      "|    TXN0072606|  PROD0707| Product 707|364.46|\n",
      "+--------------+----------+------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Scenario 2: BROADCAST JOIN (no shuffle)\n",
      "Using broadcast hint...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 10:52:44 WARN TaskSetManager: Stage 36 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Result: 100,000 rows in 0.85s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 10:52:44 WARN TaskSetManager: Stage 40 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+------------+------+\n",
      "|transaction_id|product_id|product_name|amount|\n",
      "+--------------+----------+------------+------+\n",
      "|    TXN0000001|  PROD0125| Product 125| 765.0|\n",
      "|    TXN0000002|  PROD0206| Product 206|813.99|\n",
      "|    TXN0000003|  PROD0614| Product 614|549.05|\n",
      "|    TXN0000004|  PROD0134| Product 134|941.34|\n",
      "|    TXN0000005|  PROD0778| Product 778|595.77|\n",
      "+--------------+----------+------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä JOIN COMPARISON\n",
      "================================================================================\n",
      "+--------------+------------------+---------------------+-----------+\n",
      "|Method        |Time (s)          |Strategy             |Performance|\n",
      "+--------------+------------------+---------------------+-----------+\n",
      "|Regular Join  |1.9954900741577148|Shuffle both tables  |Slower     |\n",
      "|Broadcast Join|0.8463196754455566|Broadcast small table|Faster     |\n",
      "+--------------+------------------+---------------------+-----------+\n",
      "\n",
      "üöÄ Broadcast join is 2.36x faster!\n",
      "\n",
      "üí° WHEN TO USE BROADCAST JOIN:\n",
      "\n",
      "‚úÖ Use when:\n",
      "   - Small table < 2GB (default threshold: 10MB)\n",
      "   - Join with large table\n",
      "   - Want to avoid shuffle\n",
      "   - Dimension table lookups\n",
      "\n",
      "‚ùå Don't use when:\n",
      "   - Both tables are large\n",
      "   - Small table > executor memory\n",
      "   - Memory constrained\n",
      "\n",
      "üìù Syntax:\n",
      "   df1.join(F.broadcast(df2), \"key\")\n",
      "   # or\n",
      "   df1.join(df2.hint(\"broadcast\"), \"key\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üîπ DEMO 2: Broadcast Join\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Scenario 1: Regular Join (with shuffle)\n",
    "print(\"\\nüìä Scenario 1: REGULAR JOIN (with shuffle)\")\n",
    "print(\"Joining transactions (100K) with products (1K)...\")\n",
    "\n",
    "start = time.time()\n",
    "regular_join = transactions.join(products, \"product_id\")\n",
    "row_count_regular = regular_join.count()\n",
    "time_regular_join = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ Result: {row_count_regular:,} rows in {time_regular_join:.2f}s\")\n",
    "regular_join.select(\"transaction_id\", \"product_id\", \"product_name\", \"amount\").show(5)\n",
    "\n",
    "# Scenario 2: Broadcast Join (no shuffle)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä Scenario 2: BROADCAST JOIN (no shuffle)\")\n",
    "print(\"Using broadcast hint...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start = time.time()\n",
    "broadcast_join = transactions.join(\n",
    "    F.broadcast(products),  # ‚ö° Broadcast hint\n",
    "    \"product_id\"\n",
    ")\n",
    "row_count_broadcast = broadcast_join.count()\n",
    "time_broadcast_join = time.time() - start\n",
    "\n",
    "print(f\"\\n‚úÖ Result: {row_count_broadcast:,} rows in {time_broadcast_join:.2f}s\")\n",
    "broadcast_join.select(\"transaction_id\", \"product_id\", \"product_name\", \"amount\").show(5)\n",
    "\n",
    "# Comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä JOIN COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison = [\n",
    "    (\"Regular Join\", time_regular_join, \"Shuffle both tables\", \"Slower\"),\n",
    "    (\"Broadcast Join\", time_broadcast_join, \"Broadcast small table\", \"Faster\")\n",
    "]\n",
    "\n",
    "comparison_df = spark.createDataFrame(comparison,\n",
    "    [\"Method\", \"Time (s)\", \"Strategy\", \"Performance\"])\n",
    "comparison_df.show(truncate=False)\n",
    "\n",
    "if time_regular_join > time_broadcast_join:\n",
    "    speedup = time_regular_join / time_broadcast_join\n",
    "    print(f\"üöÄ Broadcast join is {speedup:.2f}x faster!\")\n",
    "\n",
    "print(\"\"\"\n",
    "üí° WHEN TO USE BROADCAST JOIN:\n",
    "\n",
    "‚úÖ Use when:\n",
    "   - Small table < 2GB (default threshold: 10MB)\n",
    "   - Join with large table\n",
    "   - Want to avoid shuffle\n",
    "   - Dimension table lookups\n",
    "\n",
    "‚ùå Don't use when:\n",
    "   - Both tables are large\n",
    "   - Small table > executor memory\n",
    "   - Memory constrained\n",
    "\n",
    "üìù Syntax:\n",
    "   df1.join(F.broadcast(df2), \"key\")\n",
    "   # or\n",
    "   df1.join(df2.hint(\"broadcast\"), \"key\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä **4. ACCUMULATORS - C∆† B·∫¢N**\n",
    "\n",
    "### **Accumulators l√† g√¨?**\n",
    "- Variables that workers can **add** to\n",
    "- Driver can **read** final value\n",
    "- Used for **counters** and **sums**\n",
    "\n",
    "### **Syntax:**\n",
    "```python\n",
    "# Create accumulator\n",
    "acc = spark.sparkContext.accumulator(0)\n",
    "\n",
    "# Add to accumulator (in worker)\n",
    "acc.add(1)\n",
    "\n",
    "# Read value (in driver)\n",
    "acc.value\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîπ DEMO 3: Accumulators - Basic\n",
      "================================================================================\n",
      "‚úÖ Created 4 accumulators\n",
      "   - completed_acc: Count completed transactions\n",
      "   - failed_acc: Count failed transactions\n",
      "   - cancelled_acc: Count cancelled transactions\n",
      "   - high_value_acc: Count high-value transactions (> $500)\n",
      "\n",
      "üîπ Processing transactions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 10:52:45 WARN TaskSetManager: Stage 43 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 100,000 transactions\n",
      "\n",
      "================================================================================\n",
      "üìä ACCUMULATOR RESULTS\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Completed transactions: 0\n",
      "‚úÖ Failed transactions: 0\n",
      "‚úÖ Cancelled transactions: 0\n",
      "‚úÖ High-value transactions (> $500): 0\n",
      "\n",
      "üîç Verification with DataFrame operations:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 10:52:48 WARN TaskSetManager: Stage 46 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/01/11 10:52:49 WARN TaskSetManager: Stage 49 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   completed: 24,990\n",
      "   failed: 25,262\n",
      "   cancelled: 24,920\n",
      "   pending: 24,828\n",
      "   High-value (> $500): 50,236\n",
      "\n",
      "üí° KEY INSIGHTS:\n",
      "   - Accumulators aggregate info from workers\n",
      "   - Only driver can read final value\n",
      "   - Workers can only add to accumulator\n",
      "   - Useful for debugging and monitoring\n",
      "   - More efficient than multiple groupBy operations\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üîπ DEMO 3: Accumulators - Basic\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create accumulators for different transaction statuses\n",
    "completed_acc = spark.sparkContext.accumulator(0)\n",
    "failed_acc = spark.sparkContext.accumulator(0)\n",
    "cancelled_acc = spark.sparkContext.accumulator(0)\n",
    "high_value_acc = spark.sparkContext.accumulator(0)\n",
    "\n",
    "print(\"‚úÖ Created 4 accumulators\")\n",
    "print(\"   - completed_acc: Count completed transactions\")\n",
    "print(\"   - failed_acc: Count failed transactions\")\n",
    "print(\"   - cancelled_acc: Count cancelled transactions\")\n",
    "print(\"   - high_value_acc: Count high-value transactions (> $500)\")\n",
    "\n",
    "# Define UDF that uses accumulators\n",
    "def process_transaction(status, amount):\n",
    "    if status == \"completed\":\n",
    "        completed_acc.add(1)\n",
    "    elif status == \"failed\":\n",
    "        failed_acc.add(1)\n",
    "    elif status == \"cancelled\":\n",
    "        cancelled_acc.add(1)\n",
    "    \n",
    "    if amount > 500:\n",
    "        high_value_acc.add(1)\n",
    "    \n",
    "    return status\n",
    "\n",
    "process_udf = udf(process_transaction, StringType())\n",
    "\n",
    "# Process transactions\n",
    "print(\"\\nüîπ Processing transactions...\")\n",
    "\n",
    "result = transactions \\\n",
    "    .withColumn(\"processed_status\", \n",
    "                process_udf(col(\"status\"), col(\"amount\"))) \\\n",
    "    .select(\"transaction_id\", \"status\", \"amount\")\n",
    "\n",
    "# Trigger action to execute UDF\n",
    "row_count = result.count()\n",
    "\n",
    "print(f\"‚úÖ Processed {row_count:,} transactions\")\n",
    "\n",
    "# Read accumulator values\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä ACCUMULATOR RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚úÖ Completed transactions: {completed_acc.value:,}\")\n",
    "print(f\"‚úÖ Failed transactions: {failed_acc.value:,}\")\n",
    "print(f\"‚úÖ Cancelled transactions: {cancelled_acc.value:,}\")\n",
    "print(f\"‚úÖ High-value transactions (> $500): {high_value_acc.value:,}\")\n",
    "\n",
    "# Verify with DataFrame operations\n",
    "print(\"\\nüîç Verification with DataFrame operations:\")\n",
    "status_counts = transactions.groupBy(\"status\").count().collect()\n",
    "for row in status_counts:\n",
    "    print(f\"   {row['status']}: {row['count']:,}\")\n",
    "\n",
    "high_value_count = transactions.filter(col(\"amount\") > 500).count()\n",
    "print(f\"   High-value (> $500): {high_value_count:,}\")\n",
    "\n",
    "print(\"\"\"\n",
    "üí° KEY INSIGHTS:\n",
    "   - Accumulators aggregate info from workers\n",
    "   - Only driver can read final value\n",
    "   - Workers can only add to accumulator\n",
    "   - Useful for debugging and monitoring\n",
    "   - More efficient than multiple groupBy operations\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ **5. REAL-WORLD USE CASES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîπ DEMO 4: Real-World Use Cases\n",
      "================================================================================\n",
      "\n",
      "üìä Use Case 1: DATA QUALITY MONITORING\n",
      "Track data quality issues during ETL...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 10:55:34 WARN TaskSetManager: Stage 52 contains a task of very large size (1668 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Total transactions (including invalid): 100,004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 10:55:35 WARN TaskSetManager: Stage 55 contains a task of very large size (1668 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Data Quality Report:\n",
      "   Null customer IDs: 0\n",
      "   Null product IDs: 0\n",
      "   Invalid amounts: 0\n",
      "   Invalid quantities: 0\n",
      "\n",
      "üîç Sample invalid records:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 10:55:36 WARN TaskSetManager: Stage 58 contains a task of very large size (1668 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/01/11 10:55:36 WARN TaskSetManager: Stage 59 contains a task of very large size (1737 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------+------+--------+-----------------+\n",
      "|transaction_id|customer_id|product_id|amount|quantity|validation_result|\n",
      "+--------------+-----------+----------+------+--------+-----------------+\n",
      "|TXN_INV1      |NULL       |PROD0001  |100.0 |1       |NULL_CUSTOMER    |\n",
      "|TXN_INV2      |CUST00001  |NULL      |100.0 |1       |NULL_PRODUCT     |\n",
      "|TXN_INV3      |CUST00001  |PROD0001  |100.0 |-1      |INVALID_QUANTITY |\n",
      "|TXN_INV4      |CUST00001  |PROD0001  |-50.0 |1       |INVALID_AMOUNT   |\n",
      "+--------------+-----------+----------+------+--------+-----------------+\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Use Case 2: PERFORMANCE MONITORING\n",
      "Track processing time per partition...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 10:55:37 WARN TaskSetManager: Stage 61 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Performance Metrics:\n",
      "   Records processed: 0\n",
      "   Total processing time: 0.00s\n",
      "\n",
      "================================================================================\n",
      "üìä Use Case 3: DATA ENRICHMENT WITH BROADCAST\n",
      "Enrich transactions with customer tier...\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Created customer tier lookup: 10,000 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 10:55:37 WARN TaskSetManager: Stage 65 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+-------------+------+\n",
      "|transaction_id|customer_id|customer_tier|amount|\n",
      "+--------------+-----------+-------------+------+\n",
      "|    TXN0000001|  CUST09170|         Gold| 765.0|\n",
      "|    TXN0000002|  CUST07761|       Silver|813.99|\n",
      "|    TXN0000003|  CUST02907|       Silver|549.05|\n",
      "|    TXN0000004|  CUST08914|         Gold|941.34|\n",
      "|    TXN0000005|  CUST02062|         Gold|595.77|\n",
      "|    TXN0000006|  CUST01734|       Silver|439.96|\n",
      "|    TXN0000007|  CUST00158|       Silver|958.01|\n",
      "|    TXN0000008|  CUST00581|         Gold|317.34|\n",
      "|    TXN0000009|  CUST05294|       Silver|845.92|\n",
      "|    TXN0000010|  CUST02710|       Bronze|614.06|\n",
      "+--------------+-----------+-------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 10:55:37 WARN TaskSetManager: Stage 66 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Enriched 100,000 transactions in 0.31s\n",
      "\n",
      "üìä Transactions by Customer Tier:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 10:55:38 WARN TaskSetManager: Stage 69 contains a task of very large size (1667 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+--------------------+\n",
      "|customer_tier|transactions|       total_revenue|\n",
      "+-------------+------------+--------------------+\n",
      "|       Bronze|       33643|1.6993211430000037E7|\n",
      "|         Gold|       33543| 1.686893751999999E7|\n",
      "|       Silver|       32814| 1.643111931000002E7|\n",
      "+-------------+------------+--------------------+\n",
      "\n",
      "\n",
      "üí° REAL-WORLD BENEFITS:\n",
      "\n",
      "1. Data Quality Monitoring:\n",
      "   - Track issues in real-time\n",
      "   - No need for multiple passes\n",
      "   - Efficient error reporting\n",
      "\n",
      "2. Performance Monitoring:\n",
      "   - Track processing metrics\n",
      "   - Identify bottlenecks\n",
      "   - Monitor SLAs\n",
      "\n",
      "3. Data Enrichment:\n",
      "   - Fast lookup with broadcast\n",
      "   - Avoid expensive joins\n",
      "   - Reduce shuffle\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üîπ DEMO 4: Real-World Use Cases\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use Case 1: Data Quality Monitoring\n",
    "print(\"\\nüìä Use Case 1: DATA QUALITY MONITORING\")\n",
    "print(\"Track data quality issues during ETL...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create accumulators for data quality\n",
    "null_customer_acc = spark.sparkContext.accumulator(0)\n",
    "null_product_acc = spark.sparkContext.accumulator(0)\n",
    "invalid_amount_acc = spark.sparkContext.accumulator(0)\n",
    "invalid_quantity_acc = spark.sparkContext.accumulator(0)\n",
    "\n",
    "def validate_transaction(customer_id, product_id, amount, quantity):\n",
    "    issues = []\n",
    "    \n",
    "    if customer_id is None or customer_id == \"\":\n",
    "        null_customer_acc.add(1)\n",
    "        issues.append(\"NULL_CUSTOMER\")\n",
    "    \n",
    "    if product_id is None or product_id == \"\":\n",
    "        null_product_acc.add(1)\n",
    "        issues.append(\"NULL_PRODUCT\")\n",
    "    \n",
    "    if amount is None or amount <= 0:\n",
    "        invalid_amount_acc.add(1)\n",
    "        issues.append(\"INVALID_AMOUNT\")\n",
    "    \n",
    "    if quantity is None or quantity <= 0:\n",
    "        invalid_quantity_acc.add(1)\n",
    "        issues.append(\"INVALID_QUANTITY\")\n",
    "    \n",
    "    return \",\".join(issues) if issues else \"VALID\"\n",
    "\n",
    "validate_udf = udf(validate_transaction, StringType())\n",
    "\n",
    "# ‚úÖ FIX: Create schema that allows NULL values\n",
    "invalid_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), True),  # Allow NULL\n",
    "    StructField(\"customer_id\", StringType(), True),     # Allow NULL\n",
    "    StructField(\"product_id\", StringType(), True),      # Allow NULL\n",
    "    StructField(\"transaction_date\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),       # Allow NULL\n",
    "    StructField(\"amount\", DoubleType(), True),          # Allow NULL\n",
    "    StructField(\"status\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Add some invalid data\n",
    "invalid_data = [\n",
    "    (\"TXN_INV1\", None, \"PROD0001\", \"2024-01-01\", \"USA\", \"Electronics\", 1, 100.0, \"completed\"),\n",
    "    (\"TXN_INV2\", \"CUST00001\", None, \"2024-01-01\", \"USA\", \"Electronics\", 1, 100.0, \"completed\"),\n",
    "    (\"TXN_INV3\", \"CUST00001\", \"PROD0001\", \"2024-01-01\", \"USA\", \"Electronics\", -1, 100.0, \"completed\"),\n",
    "    (\"TXN_INV4\", \"CUST00001\", \"PROD0001\", \"2024-01-01\", \"USA\", \"Electronics\", 1, -50.0, \"completed\"),\n",
    "]\n",
    "\n",
    "# Create DataFrame with nullable schema\n",
    "invalid_df = spark.createDataFrame(invalid_data, invalid_schema) \\\n",
    "    .withColumn(\"transaction_date\", to_date(col(\"transaction_date\")))\n",
    "\n",
    "# Union with original transactions\n",
    "all_transactions = transactions.unionByName(invalid_df, allowMissingColumns=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Total transactions (including invalid): {all_transactions.count():,}\")\n",
    "\n",
    "# Validate\n",
    "validated = all_transactions \\\n",
    "    .withColumn(\"validation_result\",\n",
    "                validate_udf(col(\"customer_id\"), col(\"product_id\"),\n",
    "                           col(\"amount\"), col(\"quantity\")))\n",
    "\n",
    "validated.count()  # Trigger action\n",
    "\n",
    "print(\"\\nüìä Data Quality Report:\")\n",
    "print(f\"   Null customer IDs: {null_customer_acc.value:,}\")\n",
    "print(f\"   Null product IDs: {null_product_acc.value:,}\")\n",
    "print(f\"   Invalid amounts: {invalid_amount_acc.value:,}\")\n",
    "print(f\"   Invalid quantities: {invalid_quantity_acc.value:,}\")\n",
    "\n",
    "print(\"\\nüîç Sample invalid records:\")\n",
    "validated.filter(col(\"validation_result\") != \"VALID\") \\\n",
    "    .select(\"transaction_id\", \"customer_id\", \"product_id\", \"amount\", \"quantity\", \"validation_result\") \\\n",
    "    .show(10, truncate=False)\n",
    "\n",
    "# Use Case 2: Performance Monitoring\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä Use Case 2: PERFORMANCE MONITORING\")\n",
    "print(\"Track processing time per partition...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create accumulator for processing time\n",
    "processing_time_acc = spark.sparkContext.accumulator(0.0)\n",
    "records_processed_acc = spark.sparkContext.accumulator(0)\n",
    "\n",
    "def process_with_timing(transaction_id):\n",
    "    import time\n",
    "    start = time.time()\n",
    "    \n",
    "    # Simulate processing\n",
    "    time.sleep(0.0001)  # 0.1ms per record\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    processing_time_acc.add(elapsed)\n",
    "    records_processed_acc.add(1)\n",
    "    \n",
    "    return transaction_id\n",
    "\n",
    "timing_udf = udf(process_with_timing, StringType())\n",
    "\n",
    "# Process sample\n",
    "sample_size = 1000\n",
    "sample_df = transactions.limit(sample_size)\n",
    "\n",
    "result = sample_df \\\n",
    "    .withColumn(\"processed_id\", timing_udf(col(\"transaction_id\")))\n",
    "\n",
    "result.count()  # Trigger action\n",
    "\n",
    "print(f\"\\nüìä Performance Metrics:\")\n",
    "print(f\"   Records processed: {records_processed_acc.value:,}\")\n",
    "print(f\"   Total processing time: {processing_time_acc.value:.2f}s\")\n",
    "if records_processed_acc.value > 0:\n",
    "    print(f\"   Avg time per record: {processing_time_acc.value/records_processed_acc.value*1000:.2f}ms\")\n",
    "\n",
    "# Use Case 3: Broadcast for Enrichment\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä Use Case 3: DATA ENRICHMENT WITH BROADCAST\")\n",
    "print(\"Enrich transactions with customer tier...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create customer tier lookup\n",
    "customer_tier_map = {row['customer_id']: row['tier'] \n",
    "                     for row in customers.collect()}\n",
    "\n",
    "print(f\"\\n‚úÖ Created customer tier lookup: {len(customer_tier_map):,} entries\")\n",
    "\n",
    "# Broadcast the lookup\n",
    "broadcast_tiers = spark.sparkContext.broadcast(customer_tier_map)\n",
    "\n",
    "def enrich_with_tier(customer_id):\n",
    "    return broadcast_tiers.value.get(customer_id, \"Unknown\")\n",
    "\n",
    "enrich_udf = udf(enrich_with_tier, StringType())\n",
    "\n",
    "start = time.time()\n",
    "enriched = transactions \\\n",
    "    .withColumn(\"customer_tier\", enrich_udf(col(\"customer_id\"))) \\\n",
    "    .select(\"transaction_id\", \"customer_id\", \"customer_tier\", \"amount\")\n",
    "\n",
    "enriched.show(10)\n",
    "time_enrichment = time.time() - start\n",
    "\n",
    "print(f\"\\n‚úÖ Enriched {enriched.count():,} transactions in {time_enrichment:.2f}s\")\n",
    "\n",
    "# Tier summary\n",
    "print(\"\\nüìä Transactions by Customer Tier:\")\n",
    "enriched.groupBy(\"customer_tier\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"transactions\"),\n",
    "        F.sum(\"amount\").alias(\"total_revenue\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total_revenue\")) \\\n",
    "    .show()\n",
    "\n",
    "# Cleanup\n",
    "broadcast_tiers.unpersist()\n",
    "\n",
    "print(\"\"\"\n",
    "üí° REAL-WORLD BENEFITS:\n",
    "\n",
    "1. Data Quality Monitoring:\n",
    "   - Track issues in real-time\n",
    "   - No need for multiple passes\n",
    "   - Efficient error reporting\n",
    "\n",
    "2. Performance Monitoring:\n",
    "   - Track processing metrics\n",
    "   - Identify bottlenecks\n",
    "   - Monitor SLAs\n",
    "\n",
    "3. Data Enrichment:\n",
    "   - Fast lookup with broadcast\n",
    "   - Avoid expensive joins\n",
    "   - Reduce shuffle\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è **6. COMMON MISTAKES & BEST PRACTICES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "‚ö†Ô∏è COMMON MISTAKES\n",
      "================================================================================\n",
      "\n",
      "‚ùå MISTAKE 1: Broadcasting large data\n",
      "-------------------------------------------\n",
      "# BAD:\n",
      "large_df = spark.read.parquet(\"large_table\")  # 10GB\n",
      "result = df.join(F.broadcast(large_df), \"key\")\n",
      "# ‚Üí OOM! Broadcast data must fit in executor memory\n",
      "\n",
      "# GOOD:\n",
      "small_df = spark.read.parquet(\"small_table\")  # 100MB\n",
      "result = df.join(F.broadcast(small_df), \"key\")\n",
      "# ‚Üí Efficient! Small table fits in memory\n",
      "\n",
      "\n",
      "‚ùå MISTAKE 2: Using accumulators in transformations\n",
      "-------------------------------------------\n",
      "# BAD:\n",
      "acc = spark.sparkContext.accumulator(0)\n",
      "df.filter(lambda x: acc.add(1) or True)  # Transformation\n",
      "# ‚Üí Accumulator may be updated multiple times due to retries!\n",
      "\n",
      "# GOOD:\n",
      "acc = spark.sparkContext.accumulator(0)\n",
      "df.foreach(lambda x: acc.add(1))  # Action\n",
      "# ‚Üí Accumulator updated exactly once per record\n",
      "\n",
      "\n",
      "‚ùå MISTAKE 3: Forgetting to unpersist broadcast\n",
      "-------------------------------------------\n",
      "# BAD:\n",
      "broadcast_var = spark.sparkContext.broadcast(data)\n",
      "# ... use broadcast_var ...\n",
      "# ‚Üí Memory leak! Broadcast stays in memory\n",
      "\n",
      "# GOOD:\n",
      "broadcast_var = spark.sparkContext.broadcast(data)\n",
      "# ... use broadcast_var ...\n",
      "broadcast_var.unpersist()\n",
      "# ‚Üí Memory freed\n",
      "\n",
      "\n",
      "‚ùå MISTAKE 4: Broadcasting mutable data\n",
      "-------------------------------------------\n",
      "# BAD:\n",
      "mutable_list = [1, 2, 3]\n",
      "broadcast_var = spark.sparkContext.broadcast(mutable_list)\n",
      "mutable_list.append(4)  # Modifying after broadcast!\n",
      "# ‚Üí Inconsistent state across executors\n",
      "\n",
      "# GOOD:\n",
      "immutable_tuple = (1, 2, 3)\n",
      "broadcast_var = spark.sparkContext.broadcast(immutable_tuple)\n",
      "# ‚Üí Safe, immutable\n",
      "\n",
      "\n",
      "‚ùå MISTAKE 5: Reading accumulator in workers\n",
      "-------------------------------------------\n",
      "# BAD:\n",
      "acc = spark.sparkContext.accumulator(0)\n",
      "def process(x):\n",
      "    if acc.value > 100:  # Reading in worker!\n",
      "        return x\n",
      "# ‚Üí Error! Workers can only add, not read\n",
      "\n",
      "# GOOD:\n",
      "acc = spark.sparkContext.accumulator(0)\n",
      "def process(x):\n",
      "    acc.add(1)  # Only adding\n",
      "    return x\n",
      "# After action:\n",
      "if acc.value > 100:  # Reading in driver\n",
      "    print(\"Threshold exceeded\")\n",
      "\n",
      "\n",
      "================================================================================\n",
      "‚úÖ BEST PRACTICES\n",
      "================================================================================\n",
      "\n",
      "1. BROADCAST VARIABLES:\n",
      "   ‚úÖ Use for small lookup tables (< 2GB)\n",
      "   ‚úÖ Broadcast read-only data\n",
      "   ‚úÖ Always unpersist when done\n",
      "   ‚úÖ Check broadcast size: spark.conf.get('spark.sql.autoBroadcastJoinThreshold')\n",
      "   ‚úÖ Use immutable data structures\n",
      "   ‚ùå Don't broadcast large data\n",
      "   ‚ùå Don't modify after broadcasting\n",
      "\n",
      "2. ACCUMULATORS:\n",
      "   ‚úÖ Use in actions (foreach, count, etc.)\n",
      "   ‚úÖ Use for counters and metrics\n",
      "   ‚úÖ Read value only in driver\n",
      "   ‚úÖ Use for debugging and monitoring\n",
      "   ‚ùå Don't use in transformations (may double-count)\n",
      "   ‚ùå Don't read value in workers\n",
      "   ‚ùå Don't rely on accumulator for critical logic\n",
      "\n",
      "3. BROADCAST JOIN:\n",
      "   ‚úÖ Use F.broadcast() hint for small tables\n",
      "   ‚úÖ Check query plan (explain()) to verify broadcast\n",
      "   ‚úÖ Monitor memory usage\n",
      "   ‚úÖ Adjust threshold if needed:\n",
      "      spark.conf.set('spark.sql.autoBroadcastJoinThreshold', '100MB')\n",
      "   ‚ùå Don't force broadcast on large tables\n",
      "\n",
      "4. MEMORY MANAGEMENT:\n",
      "   ‚úÖ Monitor executor memory\n",
      "   ‚úÖ Unpersist broadcast variables\n",
      "   ‚úÖ Use appropriate data structures\n",
      "   ‚úÖ Test with production data sizes\n",
      "\n",
      "5. DEBUGGING:\n",
      "   ‚úÖ Check Spark UI for broadcast size\n",
      "   ‚úÖ Verify accumulator values\n",
      "   ‚úÖ Use explain() to check join strategy\n",
      "   ‚úÖ Monitor task metrics\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"‚ö†Ô∏è COMMON MISTAKES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "‚ùå MISTAKE 1: Broadcasting large data\n",
    "-------------------------------------------\n",
    "# BAD:\n",
    "large_df = spark.read.parquet(\"large_table\")  # 10GB\n",
    "result = df.join(F.broadcast(large_df), \"key\")\n",
    "# ‚Üí OOM! Broadcast data must fit in executor memory\n",
    "\n",
    "# GOOD:\n",
    "small_df = spark.read.parquet(\"small_table\")  # 100MB\n",
    "result = df.join(F.broadcast(small_df), \"key\")\n",
    "# ‚Üí Efficient! Small table fits in memory\n",
    "\n",
    "\n",
    "‚ùå MISTAKE 2: Using accumulators in transformations\n",
    "-------------------------------------------\n",
    "# BAD:\n",
    "acc = spark.sparkContext.accumulator(0)\n",
    "df.filter(lambda x: acc.add(1) or True)  # Transformation\n",
    "# ‚Üí Accumulator may be updated multiple times due to retries!\n",
    "\n",
    "# GOOD:\n",
    "acc = spark.sparkContext.accumulator(0)\n",
    "df.foreach(lambda x: acc.add(1))  # Action\n",
    "# ‚Üí Accumulator updated exactly once per record\n",
    "\n",
    "\n",
    "‚ùå MISTAKE 3: Forgetting to unpersist broadcast\n",
    "-------------------------------------------\n",
    "# BAD:\n",
    "broadcast_var = spark.sparkContext.broadcast(data)\n",
    "# ... use broadcast_var ...\n",
    "# ‚Üí Memory leak! Broadcast stays in memory\n",
    "\n",
    "# GOOD:\n",
    "broadcast_var = spark.sparkContext.broadcast(data)\n",
    "# ... use broadcast_var ...\n",
    "broadcast_var.unpersist()\n",
    "# ‚Üí Memory freed\n",
    "\n",
    "\n",
    "‚ùå MISTAKE 4: Broadcasting mutable data\n",
    "-------------------------------------------\n",
    "# BAD:\n",
    "mutable_list = [1, 2, 3]\n",
    "broadcast_var = spark.sparkContext.broadcast(mutable_list)\n",
    "mutable_list.append(4)  # Modifying after broadcast!\n",
    "# ‚Üí Inconsistent state across executors\n",
    "\n",
    "# GOOD:\n",
    "immutable_tuple = (1, 2, 3)\n",
    "broadcast_var = spark.sparkContext.broadcast(immutable_tuple)\n",
    "# ‚Üí Safe, immutable\n",
    "\n",
    "\n",
    "‚ùå MISTAKE 5: Reading accumulator in workers\n",
    "-------------------------------------------\n",
    "# BAD:\n",
    "acc = spark.sparkContext.accumulator(0)\n",
    "def process(x):\n",
    "    if acc.value > 100:  # Reading in worker!\n",
    "        return x\n",
    "# ‚Üí Error! Workers can only add, not read\n",
    "\n",
    "# GOOD:\n",
    "acc = spark.sparkContext.accumulator(0)\n",
    "def process(x):\n",
    "    acc.add(1)  # Only adding\n",
    "    return x\n",
    "# After action:\n",
    "if acc.value > 100:  # Reading in driver\n",
    "    print(\"Threshold exceeded\")\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ BEST PRACTICES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "1. BROADCAST VARIABLES:\n",
    "   ‚úÖ Use for small lookup tables (< 2GB)\n",
    "   ‚úÖ Broadcast read-only data\n",
    "   ‚úÖ Always unpersist when done\n",
    "   ‚úÖ Check broadcast size: spark.conf.get('spark.sql.autoBroadcastJoinThreshold')\n",
    "   ‚úÖ Use immutable data structures\n",
    "   ‚ùå Don't broadcast large data\n",
    "   ‚ùå Don't modify after broadcasting\n",
    "\n",
    "2. ACCUMULATORS:\n",
    "   ‚úÖ Use in actions (foreach, count, etc.)\n",
    "   ‚úÖ Use for counters and metrics\n",
    "   ‚úÖ Read value only in driver\n",
    "   ‚úÖ Use for debugging and monitoring\n",
    "   ‚ùå Don't use in transformations (may double-count)\n",
    "   ‚ùå Don't read value in workers\n",
    "   ‚ùå Don't rely on accumulator for critical logic\n",
    "\n",
    "3. BROADCAST JOIN:\n",
    "   ‚úÖ Use F.broadcast() hint for small tables\n",
    "   ‚úÖ Check query plan (explain()) to verify broadcast\n",
    "   ‚úÖ Monitor memory usage\n",
    "   ‚úÖ Adjust threshold if needed:\n",
    "      spark.conf.set('spark.sql.autoBroadcastJoinThreshold', '100MB')\n",
    "   ‚ùå Don't force broadcast on large tables\n",
    "\n",
    "4. MEMORY MANAGEMENT:\n",
    "   ‚úÖ Monitor executor memory\n",
    "   ‚úÖ Unpersist broadcast variables\n",
    "   ‚úÖ Use appropriate data structures\n",
    "   ‚úÖ Test with production data sizes\n",
    "\n",
    "5. DEBUGGING:\n",
    "   ‚úÖ Check Spark UI for broadcast size\n",
    "   ‚úÖ Verify accumulator values\n",
    "   ‚úÖ Use explain() to check join strategy\n",
    "   ‚úÖ Monitor task metrics\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì **KEY TAKEAWAYS**\n",
    "\n",
    "### **‚úÖ What You Learned:**\n",
    "\n",
    "1. **Broadcast Variables**\n",
    "   - Share read-only data efficiently\n",
    "   - Sent once per executor\n",
    "   - Use for small lookup tables\n",
    "   - Avoid shuffle in joins\n",
    "\n",
    "2. **Accumulators**\n",
    "   - Aggregate info from workers\n",
    "   - Workers add, driver reads\n",
    "   - Use for counters and metrics\n",
    "   - Only in actions, not transformations\n",
    "\n",
    "3. **Broadcast Join**\n",
    "   - No shuffle for small table\n",
    "   - Much faster than regular join\n",
    "   - Use F.broadcast() hint\n",
    "   - Check autoBroadcastJoinThreshold\n",
    "\n",
    "4. **Real-World Use Cases**\n",
    "   - Data quality monitoring\n",
    "   - Performance tracking\n",
    "   - Data enrichment\n",
    "   - Dimension table joins\n",
    "\n",
    "### **üìä Quick Reference:**\n",
    "\n",
    "```python\n",
    "# Broadcast Variable\n",
    "broadcast_var = spark.sparkContext.broadcast(data)\n",
    "value = broadcast_var.value\n",
    "broadcast_var.unpersist()\n",
    "\n",
    "# Accumulator\n",
    "acc = spark.sparkContext.accumulator(0)\n",
    "acc.add(1)  # In worker\n",
    "print(acc.value)  # In driver\n",
    "\n",
    "# Broadcast Join\n",
    "result = large_df.join(F.broadcast(small_df), \"key\")\n",
    "```\n",
    "\n",
    "### **üöÄ Next:** Day 5 - Advanced SQL & Optimization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup\n",
    "spark.catalog.clearCache()\n",
    "spark.stop()\n",
    "\n",
    "print(\"‚úÖ Spark session stopped\")\n",
    "print(\"\\nüéâ DAY 4 - LESSON 3 COMPLETED!\")\n",
    "print(\"\\nüí° Remember:\")\n",
    "print(\"   - Broadcast for small lookup tables (< 2GB)\")\n",
    "print(\"   - Accumulators for counters and metrics\")\n",
    "print(\"   - Use F.broadcast() for explicit broadcast join\")\n",
    "print(\"   - Always unpersist broadcast variables\")\n",
    "print(\"   - Use accumulators only in actions\")\n",
    "print(\"\\nüî• Quote: 'Broadcast once, use everywhere!' üì°\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
