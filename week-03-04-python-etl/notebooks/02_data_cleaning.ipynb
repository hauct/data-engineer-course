{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§¹ BÃ€I 2: DATA CLEANING WITH PYTHON\n",
    "\n",
    "## Má»¥c tiÃªu:\n",
    "- Handle missing values\n",
    "- Remove duplicates\n",
    "- Standardize data formats\n",
    "- Handle outliers\n",
    "- Data type conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sys.path.append('/home/jovyan/week-03-04-python-etl/scripts')\n",
    "\n",
    "from db_connector import DatabaseConnector\n",
    "from data_cleaner import DataCleaner, quick_clean\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“Š PART 1: Load Dirty Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dirty data\n",
    "dirty_data = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10],\n",
    "    'customer_name': [\n",
    "        '  John Doe  ',\n",
    "        'jane    SMITH',\n",
    "        'JOHN DOE',\n",
    "        'Bob   Wilson',\n",
    "        None,\n",
    "        'Alice Brown',\n",
    "        'Charlie  Davis',\n",
    "        'Eve   White',\n",
    "        'Frank   Black',\n",
    "        'Grace   Green',\n",
    "        'Henry   Blue'\n",
    "    ],\n",
    "    'email': [\n",
    "        'john@example.com',\n",
    "        'invalid-email',\n",
    "        'jane@test.com',\n",
    "        'bob@company.com',\n",
    "        None,\n",
    "        'alice@test.com',\n",
    "        'charlie@example',\n",
    "        'eve@test.com',\n",
    "        'frank@company.com',\n",
    "        'grace@test.com',\n",
    "        'henry@example.com'\n",
    "    ],\n",
    "    'age': [25, 30, 25, np.nan, 35, 35, 40, -5, 200, 28, 32],\n",
    "    'salary': [50000, 60000, 50000, 75000, np.nan, 80000, 90000, 55000, 1000000, 65000, 70000],\n",
    "    'join_date': [\n",
    "        '2020-01-15',\n",
    "        '2020-02-20',\n",
    "        '2020-01-15',\n",
    "        '2020-03-10',\n",
    "        '2020-04-05',\n",
    "        '2020-04-05',\n",
    "        'invalid-date',\n",
    "        '2020-05-12',\n",
    "        '2020-06-18',\n",
    "        '2020-07-22',\n",
    "        '2020-08-30'\n",
    "    ],\n",
    "    'country': ['Vietnam', 'vietnam', 'VIETNAM', 'USA', 'usa', 'USA', 'UK', 'uk', 'Vietnam', 'USA', 'UK']\n",
    "})\n",
    "\n",
    "print(f\"Original shape: {dirty_data.shape}\")\n",
    "print(f\"\\nData types:\\n{dirty_data.dtypes}\")\n",
    "print(f\"\\nMissing values:\\n{dirty_data.isna().sum()}\")\n",
    "print(f\"\\nDuplicates: {dirty_data.duplicated().sum()}\")\n",
    "\n",
    "dirty_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ EXERCISE 1: Identify Data Quality Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: List all data quality issues you can find\n",
    "# Write your observations here:\n",
    "\n",
    "\"\"\"\n",
    "Issues found:\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "4. \n",
    "5. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ EXERCISE 2: Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check for duplicate rows\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(f\"Duplicate rows: {dirty_data.duplicated().sum()}\")\n",
    "print(f\"\\nDuplicate customer_ids: {dirty_data['customer_id'].duplicated().sum()}\")\n",
    "\n",
    "# Show duplicates\n",
    "dirty_data[dirty_data.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove duplicate rows based on customer_id\n",
    "# Keep the first occurrence\n",
    "# YOUR CODE HERE\n",
    "\n",
    "cleaned_df = None\n",
    "\n",
    "print(f\"Shape after removing duplicates: {cleaned_df.shape}\")\n",
    "cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ EXERCISE 3: Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze missing values\n",
    "# YOUR CODE HERE\n",
    "\n",
    "missing_summary = None\n",
    "\n",
    "print(\"Missing Values Summary:\")\n",
    "print(missing_summary)\n",
    "\n",
    "# Visualize\n",
    "missing_summary.plot(kind='bar', figsize=(10, 5))\n",
    "plt.title('Missing Values by Column')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill missing values with appropriate strategies:\n",
    "# - customer_name: Fill with 'Unknown'\n",
    "# - email: Fill with 'no-email@unknown.com'\n",
    "# - age: Fill with median\n",
    "# - salary: Fill with median\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(f\"Missing values after filling:\\n{cleaned_df.isna().sum()}\")\n",
    "cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ EXERCISE 4: Standardize Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Standardize customer_name:\n",
    "# - Remove leading/trailing spaces\n",
    "# - Remove extra spaces\n",
    "# - Convert to title case\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(\"Standardized names:\")\n",
    "cleaned_df[['customer_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Standardize country names to uppercase\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(\"Country distribution:\")\n",
    "print(cleaned_df['country'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ EXERCISE 5: Validate and Clean Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create email validation function\n",
    "import re\n",
    "\n",
    "def is_valid_email(email):\n",
    "    \"\"\"Check if email is valid\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "    return bool(re.match(pattern, str(email)))\n",
    "\n",
    "# Test\n",
    "test_emails = ['john@test.com', 'invalid-email', 'test@example', 'valid@test.co.uk']\n",
    "for email in test_emails:\n",
    "    print(f\"{email}: {is_valid_email(email)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add email_valid column\n",
    "# YOUR CODE HERE\n",
    "\n",
    "cleaned_df['email_valid'] = None\n",
    "\n",
    "print(\"Email validation results:\")\n",
    "print(cleaned_df[['email', 'email_valid']])\n",
    "print(f\"\\nValid emails: {cleaned_df['email_valid'].sum()}\")\n",
    "print(f\"Invalid emails: {(~cleaned_df['email_valid']).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ EXERCISE 6: Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize age distribution\n",
    "# YOUR CODE HERE\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Box plot\n",
    "cleaned_df['age'].plot(kind='box', ax=axes[0])\n",
    "axes[0].set_title('Age Distribution (Box Plot)')\n",
    "axes[0].set_ylabel('Age')\n",
    "\n",
    "# Histogram\n",
    "cleaned_df['age'].plot(kind='hist', bins=20, ax=axes[1])\n",
    "axes[1].set_title('Age Distribution (Histogram)')\n",
    "axes[1].set_xlabel('Age')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Age statistics:\\n{cleaned_df['age'].describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove age outliers using IQR method\n",
    "# Outliers: age < 18 or age > 100\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Calculate IQR\n",
    "Q1 = None\n",
    "Q3 = None\n",
    "IQR = None\n",
    "\n",
    "lower_bound = None\n",
    "upper_bound = None\n",
    "\n",
    "print(f\"Age bounds: [{lower_bound:.1f}, {upper_bound:.1f}]\")\n",
    "\n",
    "# Filter outliers\n",
    "outliers = None\n",
    "print(f\"\\nOutliers found: {len(outliers)}\")\n",
    "print(outliers[['customer_name', 'age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Handle outliers - replace with median or remove\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(f\"Shape after handling outliers: {cleaned_df.shape}\")\n",
    "print(f\"\\nAge statistics after cleaning:\\n{cleaned_df['age'].describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ EXERCISE 7: Data Type Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convert join_date to datetime\n",
    "# Handle invalid dates\n",
    "# YOUR CODE HERE\n",
    "\n",
    "cleaned_df['join_date'] = None\n",
    "\n",
    "print(f\"Data types after conversion:\\n{cleaned_df.dtypes}\")\n",
    "print(f\"\\nJoin date range: {cleaned_df['join_date'].min()} to {cleaned_df['join_date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convert country to categorical\n",
    "# YOUR CODE HERE\n",
    "\n",
    "cleaned_df['country'] = None\n",
    "\n",
    "print(f\"Country categories: {cleaned_df['country'].cat.categories.tolist()}\")\n",
    "print(f\"Memory usage before: {dirty_data['country'].memory_usage(deep=True)} bytes\")\n",
    "print(f\"Memory usage after: {cleaned_df['country'].memory_usage(deep=True)} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ EXERCISE 8: Using DataCleaner Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use DataCleaner class to clean the original dirty data\n",
    "# YOUR CODE HERE\n",
    "\n",
    "cleaner = DataCleaner(dirty_data)\n",
    "\n",
    "cleaned_with_class = (\n",
    "    cleaner\n",
    "    # Add your cleaning steps here\n",
    "    .get_cleaned_data()\n",
    ")\n",
    "\n",
    "print(\"Cleaning Report:\")\n",
    "print(cleaner.get_cleaning_report())\n",
    "\n",
    "print(\"\\nCleaned Data:\")\n",
    "cleaned_with_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ CHALLENGE: Complete Data Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a complete cleaning function\n",
    "def clean_customer_data(df):\n",
    "    \"\"\"\n",
    "    Complete data cleaning pipeline\n",
    "    \n",
    "    Steps:\n",
    "    1. Remove duplicates\n",
    "    2. Handle missing values\n",
    "    3. Standardize text\n",
    "    4. Validate emails\n",
    "    5. Handle outliers\n",
    "    6. Convert data types\n",
    "    7. Add derived columns\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Test your function\n",
    "final_cleaned = clean_customer_data(dirty_data.copy())\n",
    "\n",
    "print(f\"Original shape: {dirty_data.shape}\")\n",
    "print(f\"Cleaned shape: {final_cleaned.shape}\")\n",
    "print(f\"\\nCleaned data info:\")\n",
    "print(final_cleaned.info())\n",
    "\n",
    "final_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ REAL-WORLD EXERCISE: Clean Database Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load customers from database and clean\n",
    "db = DatabaseConnector()\n",
    "\n",
    "# Load data\n",
    "customers = db.read_sql(\"SELECT * FROM analytics.customers LIMIT 1000\")\n",
    "\n",
    "print(f\"Loaded {len(customers)} customers\")\n",
    "print(f\"\\nData quality issues:\")\n",
    "print(f\"Missing values:\\n{customers.isna().sum()}\")\n",
    "print(f\"\\nDuplicates: {customers.duplicated().sum()}\")\n",
    "\n",
    "# YOUR CLEANING CODE HERE\n",
    "\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“š SOLUTIONS (Uncomment to view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dirty_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# SOLUTION: Remove duplicates (thÃªm .copy() Ä‘á»ƒ trÃ¡nh warning)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m cleaned_df \u001b[38;5;241m=\u001b[39m \u001b[43mdirty_data\u001b[49m\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m'\u001b[39m], keep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# SOLUTION: Fill missing values\u001b[39;00m\n\u001b[1;32m      5\u001b[0m cleaned_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m cleaned_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dirty_data' is not defined"
     ]
    }
   ],
   "source": [
    "# SOLUTION: Remove duplicates (thÃªm .copy() Ä‘á»ƒ trÃ¡nh warning)\n",
    "cleaned_df = dirty_data.drop_duplicates(subset=['customer_id'], keep='first').copy()\n",
    "\n",
    "# SOLUTION: Fill missing values\n",
    "cleaned_df['customer_name'] = cleaned_df['customer_name'].fillna('Unknown')\n",
    "cleaned_df['email'] = cleaned_df['email'].fillna('no-email@unknown.com')\n",
    "cleaned_df['age'] = cleaned_df['age'].fillna(cleaned_df['age'].median())\n",
    "cleaned_df['salary'] = cleaned_df['salary'].fillna(cleaned_df['salary'].median())\n",
    "\n",
    "# SOLUTION: Standardize text\n",
    "cleaned_df['customer_name'] = cleaned_df['customer_name'].str.strip().str.replace(r'\\s+', ' ', regex=True).str.title()\n",
    "cleaned_df['country'] = cleaned_df['country'].str.upper()\n",
    "\n",
    "# SOLUTION: Validate email (Ä‘á»‹nh nghÄ©a hÃ m trÆ°á»›c)\n",
    "import re\n",
    "\n",
    "def is_valid_email(email):\n",
    "    \"\"\"Check if email is valid\"\"\"\n",
    "    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "    return bool(re.match(pattern, str(email)))\n",
    "\n",
    "cleaned_df['email_valid'] = cleaned_df['email'].apply(is_valid_email)\n",
    "\n",
    "# SOLUTION: Handle outliers\n",
    "Q1 = cleaned_df['age'].quantile(0.25)\n",
    "Q3 = cleaned_df['age'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "cleaned_df = cleaned_df[(cleaned_df['age'] >= lower_bound) & (cleaned_df['age'] <= upper_bound)].copy()\n",
    "\n",
    "# SOLUTION: Convert data types\n",
    "cleaned_df['join_date'] = pd.to_datetime(cleaned_df['join_date'], errors='coerce')\n",
    "cleaned_df['country'] = cleaned_df['country'].astype('category')\n",
    "\n",
    "# Show data\n",
    "cleaned_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
