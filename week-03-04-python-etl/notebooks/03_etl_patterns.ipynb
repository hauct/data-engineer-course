{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÑ B√ÄI 3: ETL PATTERNS & BEST PRACTICES\n",
    "\n",
    "## M·ª•c ti√™u:\n",
    "- Extract patterns (Database, CSV, API)\n",
    "- Transform patterns (Cleaning, Enrichment, Aggregation)\n",
    "- Load patterns (Database, Files)\n",
    "- Error handling & logging\n",
    "- Pipeline orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import sys\n",
    "sys.path.append('/home/jovyan/week-03-04-python-etl/scripts')\n",
    "\n",
    "from db_connector import DatabaseConnector\n",
    "from data_cleaner import DataCleaner\n",
    "from etl_pipeline import ETLPipeline\n",
    "from validators import DataValidator\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä PART 1: Extract Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 1: Extract from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:10:15,814 - db_connector - INFO - Database connector initialized for data_engineer@postgres\n",
      "2025-12-18 08:10:15,815 - __main__ - INFO - Starting customer extraction...\n",
      "/home/jovyan/week-03-04-python-etl/scripts/db_connector.py:105: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn, params=params)\n",
      "2025-12-18 08:10:15,979 - db_connector - INFO - Query executed, DataFrame shape: (1000, 8)\n",
      "2025-12-18 08:10:15,980 - __main__ - INFO - Extracted 1000 customers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1000 customers\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>customer_name</th>\n",
       "      <th>email</th>\n",
       "      <th>country</th>\n",
       "      <th>signup_date</th>\n",
       "      <th>customer_segment</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Megan Mcclain</td>\n",
       "      <td>johnsonjoshua@example.org</td>\n",
       "      <td>Burundi</td>\n",
       "      <td>2025-11-24</td>\n",
       "      <td>Premium</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Lance Hoffman</td>\n",
       "      <td>garzaanthony@example.org</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>2025-01-13</td>\n",
       "      <td>Basic</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Olivia Moore</td>\n",
       "      <td>lrobinson@example.com</td>\n",
       "      <td>Sudan</td>\n",
       "      <td>2025-05-21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Brandon Davis</td>\n",
       "      <td>jpeterson@example.org</td>\n",
       "      <td>Serbia</td>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>Premium</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Jamie Arnold</td>\n",
       "      <td>cassandra07@example.net</td>\n",
       "      <td>British Virgin Islands</td>\n",
       "      <td>2025-02-22</td>\n",
       "      <td>Basic</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  customer_name                      email  \\\n",
       "0            1  Megan Mcclain  johnsonjoshua@example.org   \n",
       "1            2  Lance Hoffman   garzaanthony@example.org   \n",
       "2            3   Olivia Moore      lrobinson@example.com   \n",
       "3            4  Brandon Davis      jpeterson@example.org   \n",
       "4            5   Jamie Arnold    cassandra07@example.net   \n",
       "\n",
       "                  country signup_date customer_segment  \\\n",
       "0                 Burundi  2025-11-24          Premium   \n",
       "1                    Cuba  2025-01-13            Basic   \n",
       "2                   Sudan  2025-05-21          Premium   \n",
       "3                  Serbia  2025-04-25          Premium   \n",
       "4  British Virgin Islands  2025-02-22            Basic   \n",
       "\n",
       "                  created_at                 updated_at  \n",
       "0 2025-12-18 07:56:50.725874 2025-12-18 07:56:50.725874  \n",
       "1 2025-12-18 07:56:50.725874 2025-12-18 07:56:50.725874  \n",
       "2 2025-12-18 07:56:50.725874 2025-12-18 07:56:50.725874  \n",
       "3 2025-12-18 07:56:50.725874 2025-12-18 07:56:50.725874  \n",
       "4 2025-12-18 07:56:50.725874 2025-12-18 07:56:50.725874  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Create extract function with error handling\n",
    "def extract_customers(db, date_from=None):\n",
    "    \"\"\"\n",
    "    Extract customers from database\n",
    "    \n",
    "    Args:\n",
    "        db: DatabaseConnector instance\n",
    "        date_from: Optional date filter\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with customers\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Starting customer extraction...\")\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        query = \"\"\"\n",
    "            SELECT * FROM analytics.customers\n",
    "            WHERE 1=1\n",
    "        \"\"\"\n",
    "        \n",
    "        if date_from:\n",
    "            query += f\" AND created_at >= '{date_from}'\"\n",
    "        \n",
    "        df = db.read_sql(query)\n",
    "        \n",
    "        logger.info(f\"Extracted {len(df)} customers\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Extraction failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Test\n",
    "db = DatabaseConnector()\n",
    "customers = extract_customers(db)\n",
    "print(f\"Extracted {len(customers)} customers\")\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Incremental Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/week-03-04-python-etl/scripts/db_connector.py:105: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn, params=params)\n",
      "2025-12-18 08:10:33,343 - db_connector - INFO - Query executed, DataFrame shape: (10000, 7)\n",
      "2025-12-18 08:10:33,345 - __main__ - INFO - Extracted 10000 new/updated orders since 2025-12-11 08:10:33.297253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New orders: 10000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement incremental extraction\n",
    "def extract_orders_incremental(db, last_extracted_date):\n",
    "    \"\"\"\n",
    "    Extract only new/updated orders since last extraction\n",
    "    \n",
    "    Args:\n",
    "        db: DatabaseConnector\n",
    "        last_extracted_date: Last extraction timestamp\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with new orders\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    query = \"\"\"\n",
    "        SELECT * FROM analytics.orders\n",
    "        WHERE updated_at > %s\n",
    "        ORDER BY updated_at\n",
    "    \"\"\"\n",
    "    \n",
    "    df = db.read_sql(query, (last_extracted_date,))\n",
    "    logger.info(f\"Extracted {len(df)} new/updated orders since {last_extracted_date}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Test\n",
    "last_date = datetime.now() - timedelta(days=7)\n",
    "new_orders = extract_orders_incremental(db, last_date)\n",
    "print(f\"New orders: {len(new_orders)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 3: Extract from CSV with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create CSV extraction with validation\n",
    "def extract_from_csv(file_path, expected_columns):\n",
    "    \"\"\"\n",
    "    Extract data from CSV with validation\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to CSV file\n",
    "        expected_columns: List of expected column names\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Reading CSV: {file_path}\")\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Validate columns\n",
    "        missing_cols = set(expected_columns) - set(df.columns)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing columns: {missing_cols}\")\n",
    "        \n",
    "        logger.info(f\"Successfully loaded {len(df)} rows\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"CSV extraction failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Create sample CSV for testing\n",
    "sample_data = pd.DataFrame({\n",
    "    'product_id': [1, 2, 3],\n",
    "    'product_name': ['Product A', 'Product B', 'Product C'],\n",
    "    'price': [100, 200, 300]\n",
    "})\n",
    "sample_data.to_csv('/home/jovyan/work/week-03-04-python-etl/data/raw/sample_products.csv', index=False)\n",
    "\n",
    "# Test\n",
    "products = extract_from_csv(\n",
    "    '/home/jovyan/work/week-03-04-python-etl/data/raw/sample_products.csv',\n",
    "    ['product_id', 'product_name', 'price']\n",
    ")\n",
    "products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß PART 2: Transform Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 1: Data Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:12:10,046 - db_connector - INFO - Query executed, DataFrame shape: (1000, 7)\n",
      "2025-12-18 08:12:10,046 - __main__ - INFO - Enriching customer data...\n",
      "2025-12-18 08:12:10,107 - __main__ - INFO - Enriched 1000 customers\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>customer_name</th>\n",
       "      <th>email</th>\n",
       "      <th>country</th>\n",
       "      <th>signup_date</th>\n",
       "      <th>customer_segment</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>total_orders</th>\n",
       "      <th>total_revenue</th>\n",
       "      <th>avg_order_value</th>\n",
       "      <th>max_order_value</th>\n",
       "      <th>first_order_date</th>\n",
       "      <th>last_order_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Megan Mcclain</td>\n",
       "      <td>johnsonjoshua@example.org</td>\n",
       "      <td>Burundi</td>\n",
       "      <td>2025-11-24</td>\n",
       "      <td>Premium</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Lance Hoffman</td>\n",
       "      <td>garzaanthony@example.org</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>2025-01-13</td>\n",
       "      <td>Basic</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10242.68</td>\n",
       "      <td>10242.68</td>\n",
       "      <td>10242.68</td>\n",
       "      <td>2025-02-19</td>\n",
       "      <td>2025-02-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Olivia Moore</td>\n",
       "      <td>lrobinson@example.com</td>\n",
       "      <td>Sudan</td>\n",
       "      <td>2025-05-21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Brandon Davis</td>\n",
       "      <td>jpeterson@example.org</td>\n",
       "      <td>Serbia</td>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>Premium</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Jamie Arnold</td>\n",
       "      <td>cassandra07@example.net</td>\n",
       "      <td>British Virgin Islands</td>\n",
       "      <td>2025-02-22</td>\n",
       "      <td>Basic</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3028.50</td>\n",
       "      <td>3028.50</td>\n",
       "      <td>3028.50</td>\n",
       "      <td>2025-06-10</td>\n",
       "      <td>2025-06-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  customer_name                      email  \\\n",
       "0            1  Megan Mcclain  johnsonjoshua@example.org   \n",
       "1            2  Lance Hoffman   garzaanthony@example.org   \n",
       "2            3   Olivia Moore      lrobinson@example.com   \n",
       "3            4  Brandon Davis      jpeterson@example.org   \n",
       "4            5   Jamie Arnold    cassandra07@example.net   \n",
       "\n",
       "                  country signup_date customer_segment  \\\n",
       "0                 Burundi  2025-11-24          Premium   \n",
       "1                    Cuba  2025-01-13            Basic   \n",
       "2                   Sudan  2025-05-21          Premium   \n",
       "3                  Serbia  2025-04-25          Premium   \n",
       "4  British Virgin Islands  2025-02-22            Basic   \n",
       "\n",
       "                  created_at                 updated_at  total_orders  \\\n",
       "0 2025-12-18 07:56:50.725874 2025-12-18 07:56:50.725874           0.0   \n",
       "1 2025-12-18 07:56:50.725874 2025-12-18 07:56:50.725874           1.0   \n",
       "2 2025-12-18 07:56:50.725874 2025-12-18 07:56:50.725874           0.0   \n",
       "3 2025-12-18 07:56:50.725874 2025-12-18 07:56:50.725874           0.0   \n",
       "4 2025-12-18 07:56:50.725874 2025-12-18 07:56:50.725874           1.0   \n",
       "\n",
       "   total_revenue  avg_order_value  max_order_value first_order_date  \\\n",
       "0           0.00              NaN              NaN              NaN   \n",
       "1       10242.68         10242.68         10242.68       2025-02-19   \n",
       "2           0.00              NaN              NaN              NaN   \n",
       "3           0.00              NaN              NaN              NaN   \n",
       "4        3028.50          3028.50          3028.50       2025-06-10   \n",
       "\n",
       "  last_order_date  \n",
       "0             NaN  \n",
       "1      2025-02-19  \n",
       "2             NaN  \n",
       "3             NaN  \n",
       "4      2025-06-10  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Create enrichment transformation\n",
    "def enrich_customer_data(customers_df, orders_df):\n",
    "    \"\"\"\n",
    "    Enrich customers with order statistics\n",
    "    \n",
    "    Args:\n",
    "        customers_df: Customer DataFrame\n",
    "        orders_df: Orders DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        Enriched DataFrame\n",
    "    \"\"\"\n",
    "    logger.info(\"Enriching customer data...\")\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # Calculate order statistics per customer\n",
    "    order_stats = orders_df.groupby('customer_id').agg({\n",
    "        'order_id': 'count',\n",
    "        'total_amount': ['sum', 'mean', 'max'],\n",
    "        'order_date': ['min', 'max']\n",
    "    }).reset_index()\n",
    "    \n",
    "    order_stats.columns = [\n",
    "        'customer_id', 'total_orders', 'total_revenue',\n",
    "        'avg_order_value', 'max_order_value',\n",
    "        'first_order_date', 'last_order_date'\n",
    "    ]\n",
    "    \n",
    "    # Merge with customers\n",
    "    enriched = customers_df.merge(order_stats, on='customer_id', how='left')\n",
    "    \n",
    "    # Fill nulls for customers without orders\n",
    "    enriched['total_orders'] = enriched['total_orders'].fillna(0)\n",
    "    enriched['total_revenue'] = enriched['total_revenue'].fillna(0)\n",
    "    \n",
    "    logger.info(f\"Enriched {len(enriched)} customers\")\n",
    "    return enriched\n",
    "\n",
    "# Test\n",
    "orders = db.read_sql(\"SELECT * FROM analytics.orders LIMIT 1000\")\n",
    "enriched_customers = enrich_customer_data(customers, orders)\n",
    "enriched_customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:12:13,644 - __main__ - INFO - Creating daily summary...\n",
      "2025-12-18 08:12:13,680 - __main__ - INFO - Created summary for 337 days\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>total_orders</th>\n",
       "      <th>total_revenue</th>\n",
       "      <th>avg_order_value</th>\n",
       "      <th>unique_customers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-01-02</td>\n",
       "      <td>2</td>\n",
       "      <td>4008.54</td>\n",
       "      <td>2004.270000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-01-03</td>\n",
       "      <td>4</td>\n",
       "      <td>12639.22</td>\n",
       "      <td>3159.805000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-01-04</td>\n",
       "      <td>4</td>\n",
       "      <td>9823.39</td>\n",
       "      <td>2455.847500</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-01-05</td>\n",
       "      <td>5</td>\n",
       "      <td>13386.33</td>\n",
       "      <td>2677.266000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-01-06</td>\n",
       "      <td>6</td>\n",
       "      <td>24054.94</td>\n",
       "      <td>4009.156667</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-01-07</td>\n",
       "      <td>1</td>\n",
       "      <td>3666.91</td>\n",
       "      <td>3666.910000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-01-08</td>\n",
       "      <td>1</td>\n",
       "      <td>11796.91</td>\n",
       "      <td>11796.910000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-01-09</td>\n",
       "      <td>3</td>\n",
       "      <td>10414.97</td>\n",
       "      <td>3471.656667</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-01-10</td>\n",
       "      <td>3</td>\n",
       "      <td>8693.67</td>\n",
       "      <td>2897.890000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-01-11</td>\n",
       "      <td>4</td>\n",
       "      <td>12844.17</td>\n",
       "      <td>3211.042500</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  total_orders  total_revenue  avg_order_value  unique_customers\n",
       "0  2025-01-02             2        4008.54      2004.270000                 2\n",
       "1  2025-01-03             4       12639.22      3159.805000                 4\n",
       "2  2025-01-04             4        9823.39      2455.847500                 4\n",
       "3  2025-01-05             5       13386.33      2677.266000                 5\n",
       "4  2025-01-06             6       24054.94      4009.156667                 6\n",
       "5  2025-01-07             1        3666.91      3666.910000                 1\n",
       "6  2025-01-08             1       11796.91     11796.910000                 1\n",
       "7  2025-01-09             3       10414.97      3471.656667                 3\n",
       "8  2025-01-10             3        8693.67      2897.890000                 3\n",
       "9  2025-01-11             4       12844.17      3211.042500                 4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Create aggregation transformation\n",
    "def create_daily_summary(orders_df):\n",
    "    \"\"\"\n",
    "    Create daily order summary\n",
    "    \n",
    "    Args:\n",
    "        orders_df: Orders DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        Daily summary DataFrame\n",
    "    \"\"\"\n",
    "    logger.info(\"Creating daily summary...\")\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    orders_df['order_date'] = pd.to_datetime(orders_df['order_date'])\n",
    "    \n",
    "    daily_summary = orders_df.groupby(orders_df['order_date'].dt.date).agg({\n",
    "        'order_id': 'count',\n",
    "        'total_amount': ['sum', 'mean'],\n",
    "        'customer_id': 'nunique'\n",
    "    }).reset_index()\n",
    "    \n",
    "    daily_summary.columns = [\n",
    "        'date', 'total_orders', 'total_revenue',\n",
    "        'avg_order_value', 'unique_customers'\n",
    "    ]\n",
    "    \n",
    "    logger.info(f\"Created summary for {len(daily_summary)} days\")\n",
    "    return daily_summary\n",
    "\n",
    "# Test\n",
    "daily_summary = create_daily_summary(orders)\n",
    "daily_summary.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 3: Data Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:12:31,805 - __main__ - INFO - Cleaning customer data...\n",
      "2025-12-18 08:12:31,811 - data_cleaner - INFO - DataCleaner initialized with shape (1000, 8)\n",
      "2025-12-18 08:12:31,815 - data_cleaner - INFO - remove_duplicates: Removed 0 duplicates (0.00%)\n",
      "2025-12-18 08:12:31,818 - data_cleaner - INFO - handle_missing: customer_name: 0 ‚Üí 0 missing values\n",
      "2025-12-18 08:12:31,822 - data_cleaner - INFO - handle_missing: email: 0 ‚Üí 0 missing values\n",
      "2025-12-18 08:12:31,824 - data_cleaner - INFO - handle_missing: country: 0 ‚Üí 0 missing values\n",
      "2025-12-18 08:12:31,832 - data_cleaner - INFO - standardize_text: Standardized customer_name\n",
      "2025-12-18 08:12:31,837 - data_cleaner - INFO - standardize_text: Standardized country\n",
      "2025-12-18 08:12:31,840 - data_cleaner - INFO - Cleaning complete: (1000, 8) ‚Üí (1000, 8)\n",
      "2025-12-18 08:12:31,841 - __main__ - INFO - Cleaning complete: 1000 ‚Üí 1000 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>customer_name</th>\n",
       "      <th>email</th>\n",
       "      <th>country</th>\n",
       "      <th>signup_date</th>\n",
       "      <th>customer_segment</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>megan mcclain</td>\n",
       "      <td>johnsonjoshua@example.org</td>\n",
       "      <td>burundi</td>\n",
       "      <td>2025-11-24</td>\n",
       "      <td>Premium</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>lance hoffman</td>\n",
       "      <td>garzaanthony@example.org</td>\n",
       "      <td>cuba</td>\n",
       "      <td>2025-01-13</td>\n",
       "      <td>Basic</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>olivia moore</td>\n",
       "      <td>lrobinson@example.com</td>\n",
       "      <td>sudan</td>\n",
       "      <td>2025-05-21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>brandon davis</td>\n",
       "      <td>jpeterson@example.org</td>\n",
       "      <td>serbia</td>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>Premium</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>jamie arnold</td>\n",
       "      <td>cassandra07@example.net</td>\n",
       "      <td>british virgin islands</td>\n",
       "      <td>2025-02-22</td>\n",
       "      <td>Basic</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "      <td>2025-12-18 07:56:50.725874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  customer_name                      email  \\\n",
       "0            1  megan mcclain  johnsonjoshua@example.org   \n",
       "1            2  lance hoffman   garzaanthony@example.org   \n",
       "2            3   olivia moore      lrobinson@example.com   \n",
       "3            4  brandon davis      jpeterson@example.org   \n",
       "4            5   jamie arnold    cassandra07@example.net   \n",
       "\n",
       "                  country signup_date customer_segment  \\\n",
       "0                 burundi  2025-11-24          Premium   \n",
       "1                    cuba  2025-01-13            Basic   \n",
       "2                   sudan  2025-05-21          Premium   \n",
       "3                  serbia  2025-04-25          Premium   \n",
       "4  british virgin islands  2025-02-22            Basic   \n",
       "\n",
       "                  created_at                 updated_at  \n",
       "0 2025-12-18 07:56:50.725874 2025-12-18 07:56:50.725874  \n",
       "1 2025-12-18 07:56:50.725874 2025-12-18 07:56:50.725874  \n",
       "2 2025-12-18 07:56:50.725874 2025-12-18 07:56:50.725874  \n",
       "3 2025-12-18 07:56:50.725874 2025-12-18 07:56:50.725874  \n",
       "4 2025-12-18 07:56:50.725874 2025-12-18 07:56:50.725874  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Create reusable cleaning pipeline\n",
    "def clean_customer_data(df):\n",
    "    \"\"\"\n",
    "    Standard customer data cleaning\n",
    "    \"\"\"\n",
    "    logger.info(\"Cleaning customer data...\")\n",
    "    \n",
    "    cleaner = DataCleaner(df)\n",
    "    \n",
    "    cleaned = (\n",
    "        cleaner\n",
    "        .remove_duplicates(subset=['customer_id'])\n",
    "        .handle_missing_values({\n",
    "            'customer_name': 'Unknown',\n",
    "            'email': 'no-email@unknown.com',\n",
    "            'country': 'Unknown'\n",
    "        })\n",
    "        .standardize_text(['customer_name', 'country'])\n",
    "        .get_cleaned_data()\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Cleaning complete: {len(df)} ‚Üí {len(cleaned)} rows\")\n",
    "    return cleaned\n",
    "\n",
    "# Test\n",
    "cleaned_customers = clean_customer_data(customers)\n",
    "cleaned_customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üíæ PART 3: Load Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 1: Full Load (Replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement full load pattern\n",
    "def load_full_replace(df, table_name, db):\n",
    "    \"\"\"\n",
    "    Full load - replace entire table\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading {len(df)} rows to {table_name} (REPLACE)\")\n",
    "    \n",
    "    try:\n",
    "        # YOUR CODE HERE\n",
    "        rows = db.write_dataframe(\n",
    "            df,\n",
    "            table_name,\n",
    "            schema='analytics',\n",
    "            if_exists='replace'\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Successfully loaded {rows} rows\")\n",
    "        return rows\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Load failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Test (don't actually run to preserve data)\n",
    "# load_full_replace(daily_summary, 'daily_order_summary', db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Incremental Load (Append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement incremental load\n",
    "def load_incremental(df, table_name, db):\n",
    "    \"\"\"\n",
    "    Incremental load - append new records\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading {len(df)} rows to {table_name} (APPEND)\")\n",
    "    \n",
    "    try:\n",
    "        # YOUR CODE HERE\n",
    "        rows = db.write_dataframe(\n",
    "            df,\n",
    "            table_name,\n",
    "            schema='analytics',\n",
    "            if_exists='append'\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Successfully appended {rows} rows\")\n",
    "        return rows\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Load failed: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 3: Upsert (Update or Insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement upsert pattern\n",
    "def load_upsert(df, table_name, key_columns, db):\n",
    "    \"\"\"\n",
    "    Upsert - update existing records or insert new ones\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to load\n",
    "        table_name: Target table\n",
    "        key_columns: Columns to match for updates\n",
    "        db: DatabaseConnector\n",
    "    \"\"\"\n",
    "    logger.info(f\"Upserting {len(df)} rows to {table_name}\")\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # Strategy: Load to temp table, then merge\n",
    "    temp_table = f\"{table_name}_temp\"\n",
    "    \n",
    "    # Load to temp table\n",
    "    db.write_dataframe(df, temp_table, if_exists='replace')\n",
    "    \n",
    "    # Build upsert query\n",
    "    key_condition = \" AND \".join([f\"t.{col} = s.{col}\" for col in key_columns])\n",
    "    \n",
    "    upsert_query = f\"\"\"\n",
    "        -- Delete existing records\n",
    "        DELETE FROM analytics.{table_name} t\n",
    "        USING analytics.{temp_table} s\n",
    "        WHERE {key_condition};\n",
    "        \n",
    "        -- Insert all records from temp\n",
    "        INSERT INTO analytics.{table_name}\n",
    "        SELECT * FROM analytics.{temp_table};\n",
    "        \n",
    "        -- Drop temp table\n",
    "        DROP TABLE analytics.{temp_table};\n",
    "    \"\"\"\n",
    "    \n",
    "    db.execute_query(upsert_query, fetch=False)\n",
    "    logger.info(f\"Upsert complete\")\n",
    "\n",
    "# Test (commented out)\n",
    "# load_upsert(enriched_customers, 'customers_enriched', ['customer_id'], db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ EXERCISE: Build Complete ETL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customer_enrichment_pipeline():\n",
    "    \"\"\"Using ETLPipeline framework\"\"\"\n",
    "    \n",
    "    pipeline = ETLPipeline('customer_enrichment')\n",
    "    \n",
    "    try:\n",
    "        # EXTRACT - D√πng pipeline.extract()\n",
    "        customers = pipeline.extract(\n",
    "            source='database',\n",
    "            query=\"SELECT * FROM analytics.customers\"\n",
    "        )\n",
    "        \n",
    "        orders = pipeline.extract(\n",
    "            source='database',\n",
    "            query=\"SELECT * FROM analytics.orders\"\n",
    "        )\n",
    "        \n",
    "        # TRANSFORM - D√πng pipeline.transform()\n",
    "        def calc_stats(df):\n",
    "            return df.groupby('customer_id').agg({\n",
    "                'order_id': 'count',\n",
    "                'total_amount': ['sum', 'mean']\n",
    "            }).reset_index()\n",
    "        \n",
    "        def merge_data(df):\n",
    "            order_stats = calc_stats(orders)\n",
    "            return df.merge(order_stats, on='customer_id', how='left')\n",
    "        \n",
    "        enriched = pipeline.transform(\n",
    "            customers,\n",
    "            transformations=[merge_data]\n",
    "        )\n",
    "        \n",
    "        # LOAD - D√πng pipeline.load()\n",
    "        pipeline.load(\n",
    "            enriched,\n",
    "            destination='database',\n",
    "            table='customers_enriched'\n",
    "        )\n",
    "        \n",
    "        # Get summary\n",
    "        summary = pipeline.get_summary()\n",
    "        print(summary)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        pipeline.log_step('ERROR', 'FAILED', str(e))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ CHALLENGE: Build Your Own Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:13:45,415 - db_connector - INFO - Database connector initialized for data_engineer@postgres\n",
      "2025-12-18 08:13:45,416 - __main__ - INFO - ============================================================\n",
      "2025-12-18 08:13:45,417 - __main__ - INFO - EXTRACT PHASE\n",
      "2025-12-18 08:13:45,417 - __main__ - INFO - ============================================================\n",
      "2025-12-18 08:13:45,418 - __main__ - INFO - Extracting orders from 2025-11-18 to 2025-12-18\n",
      "/home/jovyan/week-03-04-python-etl/scripts/db_connector.py:105: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn, params=params)\n",
      "2025-12-18 08:13:45,442 - db_connector - INFO - Query executed, DataFrame shape: (2621, 10)\n",
      "2025-12-18 08:13:45,442 - __main__ - INFO - ‚úÖ Extracted 2621 order items\n",
      "2025-12-18 08:13:45,443 - __main__ - INFO - ============================================================\n",
      "2025-12-18 08:13:45,443 - __main__ - INFO - TRANSFORM PHASE\n",
      "2025-12-18 08:13:45,444 - __main__ - INFO - ============================================================\n",
      "2025-12-18 08:13:45,444 - __main__ - INFO - Calculating product metrics...\n",
      "2025-12-18 08:13:45,464 - __main__ - INFO - ‚úÖ Calculated metrics for 100 products\n",
      "2025-12-18 08:13:45,471 - __main__ - INFO - ‚úÖ Transformation complete\n",
      "2025-12-18 08:13:45,471 - __main__ - INFO - \n",
      "üìä PERFORMANCE SUMMARY:\n",
      "2025-12-18 08:13:45,472 - __main__ - INFO -   Total Products: 100\n",
      "2025-12-18 08:13:45,474 - __main__ - INFO -   Total Revenue: $3,934,241.26\n",
      "2025-12-18 08:13:45,475 - __main__ - INFO -   Total Quantity Sold: 7,842\n",
      "2025-12-18 08:13:45,475 - __main__ - INFO -   Avg Order Value: $1,509.84\n",
      "2025-12-18 08:13:45,476 - __main__ - INFO - ============================================================\n",
      "2025-12-18 08:13:45,477 - __main__ - INFO - VALIDATION PHASE\n",
      "2025-12-18 08:13:45,478 - __main__ - INFO - ============================================================\n",
      "2025-12-18 08:13:45,478 - validators - INFO - DataValidator initialized for 'product_performance' with shape (100, 17)\n",
      "2025-12-18 08:13:45,479 - __main__ - ERROR - ‚ùå Pipeline failed: 'DataValidator' object has no attribute 'check_not_null'\n",
      "2025-12-18 08:13:45,481 - __main__ - ERROR - Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1386/420721564.py\", line 132, in product_performance_pipeline\n",
      "    .check_not_null(['product_id', 'product_name', 'category'])\n",
      "     ^^^^^^^^^^^^^^\n",
      "AttributeError: 'DataValidator' object has no attribute 'check_not_null'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting Product Performance Pipeline...\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataValidator' object has no attribute 'check_not_null'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 210\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müöÄ Starting Product Performance Pipeline...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 210\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mproduct_performance_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 132\u001b[0m, in \u001b[0;36mproduct_performance_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVALIDATION PHASE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    128\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m    130\u001b[0m validator \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    131\u001b[0m     \u001b[43mDataValidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproduct_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproduct_performance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 132\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_not_null\u001b[49m([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;241m.\u001b[39mcheck_unique([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;241m.\u001b[39mcheck_range(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_revenue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;241m.\u001b[39mcheck_range(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_quantity_sold\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m.\u001b[39mcheck_range(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber_of_orders\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    137\u001b[0m )\n\u001b[1;32m    139\u001b[0m validator\u001b[38;5;241m.\u001b[39mprint_report()\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Check if validation passed\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataValidator' object has no attribute 'check_not_null'"
     ]
    }
   ],
   "source": [
    "def product_performance_pipeline():\n",
    "    \"\"\"\n",
    "    Product Performance ETL Pipeline - FIXED VERSION\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    try:\n",
    "        # Initialize database connection\n",
    "        db = DatabaseConnector()\n",
    "        \n",
    "        # ==========================================\n",
    "        # EXTRACT PHASE\n",
    "        # ==========================================\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"EXTRACT PHASE\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # Calculate date range\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=30)\n",
    "        \n",
    "        logger.info(f\"Extracting orders from {start_date.date()} to {end_date.date()}\")\n",
    "        \n",
    "        # ‚úÖ FIX: Calculate subtotal instead of selecting it\n",
    "        orders_query = \"\"\"\n",
    "            SELECT \n",
    "                o.order_id,\n",
    "                o.customer_id,\n",
    "                o.order_date,\n",
    "                o.total_amount,\n",
    "                oi.product_id,\n",
    "                oi.quantity,\n",
    "                oi.unit_price,\n",
    "                (oi.quantity * oi.unit_price) as subtotal,  -- ‚úÖ T√çNH SUBTOTAL\n",
    "                p.product_name,\n",
    "                p.category\n",
    "            FROM analytics.orders o\n",
    "            JOIN analytics.order_items oi ON o.order_id = oi.order_id\n",
    "            JOIN analytics.products p ON oi.product_id = p.product_id\n",
    "            WHERE o.order_date >= %s\n",
    "              AND o.order_date <= %s\n",
    "        \"\"\"\n",
    "        \n",
    "        orders_df = db.read_sql(orders_query, (start_date, end_date))\n",
    "        logger.info(f\"‚úÖ Extracted {len(orders_df)} order items\")\n",
    "        \n",
    "        if len(orders_df) == 0:\n",
    "            logger.warning(\"‚ö†Ô∏è No orders found in the last 30 days\")\n",
    "            return None\n",
    "        \n",
    "        # ==========================================\n",
    "        # TRANSFORM PHASE\n",
    "        # ==========================================\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"TRANSFORM PHASE\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # 1. Calculate product performance metrics\n",
    "        logger.info(\"Calculating product metrics...\")\n",
    "        \n",
    "        product_metrics = orders_df.groupby(['product_id', 'product_name', 'category']).agg({\n",
    "            'order_id': 'nunique',           # Number of unique orders\n",
    "            'quantity': 'sum',                # Total quantity sold\n",
    "            'subtotal': 'sum',                # Total revenue\n",
    "            'unit_price': 'mean',             # Average price\n",
    "            'order_date': ['min', 'max']      # First and last sale date\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Flatten column names\n",
    "        product_metrics.columns = [\n",
    "            'product_id', 'product_name', 'category',\n",
    "            'number_of_orders', 'total_quantity_sold', 'total_revenue',\n",
    "            'avg_unit_price', 'first_sale_date', 'last_sale_date'\n",
    "        ]\n",
    "        \n",
    "        logger.info(f\"‚úÖ Calculated metrics for {len(product_metrics)} products\")\n",
    "        \n",
    "        # 2. Calculate average order value per product\n",
    "        product_metrics['avg_order_value'] = (\n",
    "            product_metrics['total_revenue'] / product_metrics['number_of_orders']\n",
    "        ).round(2)\n",
    "        \n",
    "        # 3. Calculate revenue trend (daily average)\n",
    "        product_metrics['days_in_period'] = (\n",
    "            pd.to_datetime(product_metrics['last_sale_date']) - \n",
    "            pd.to_datetime(product_metrics['first_sale_date'])\n",
    "        ).dt.days + 1\n",
    "        \n",
    "        product_metrics['daily_avg_revenue'] = (\n",
    "            product_metrics['total_revenue'] / product_metrics['days_in_period']\n",
    "        ).round(2)\n",
    "        \n",
    "        # 4. Add performance ranking\n",
    "        product_metrics['revenue_rank'] = product_metrics['total_revenue'].rank(\n",
    "            ascending=False, method='dense'\n",
    "        ).astype(int)\n",
    "        \n",
    "        product_metrics['quantity_rank'] = product_metrics['total_quantity_sold'].rank(\n",
    "            ascending=False, method='dense'\n",
    "        ).astype(int)\n",
    "        \n",
    "        # 5. Add analysis period\n",
    "        product_metrics['analysis_start_date'] = start_date.date()\n",
    "        product_metrics['analysis_end_date'] = end_date.date()\n",
    "        product_metrics['created_at'] = datetime.now()\n",
    "        \n",
    "        # 6. Round numeric columns\n",
    "        numeric_columns = ['total_revenue', 'avg_unit_price', 'avg_order_value', 'daily_avg_revenue']\n",
    "        product_metrics[numeric_columns] = product_metrics[numeric_columns].round(2)\n",
    "        \n",
    "        logger.info(\"‚úÖ Transformation complete\")\n",
    "        \n",
    "        # Display summary\n",
    "        logger.info(\"\\nüìä PERFORMANCE SUMMARY:\")\n",
    "        logger.info(f\"  Total Products: {len(product_metrics)}\")\n",
    "        logger.info(f\"  Total Revenue: ${product_metrics['total_revenue'].sum():,.2f}\")\n",
    "        logger.info(f\"  Total Quantity Sold: {product_metrics['total_quantity_sold'].sum():,}\")\n",
    "        logger.info(f\"  Avg Order Value: ${product_metrics['avg_order_value'].mean():,.2f}\")\n",
    "        \n",
    "        # ==========================================\n",
    "        # VALIDATE PHASE\n",
    "        # ==========================================\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"VALIDATION PHASE\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        validator = (\n",
    "            DataValidator(product_metrics, \"product_performance\")\n",
    "            .check_no_nulls(['product_id', 'product_name', 'category'])\n",
    "            .check_unique(['product_id'])\n",
    "            .check_range('total_revenue', 0, float('inf'))\n",
    "            .check_range('total_quantity_sold', 0, float('inf'))\n",
    "            .check_range('number_of_orders', 1, float('inf'))\n",
    "        )\n",
    "        \n",
    "        validator.print_report()\n",
    "        \n",
    "        # Check if validation passed\n",
    "        summary = validator.get_summary()\n",
    "        if summary['failed'] > 0:\n",
    "            logger.error(\"‚ùå Validation failed! Aborting load.\")\n",
    "            return None\n",
    "        \n",
    "        # ==========================================\n",
    "        # LOAD PHASE\n",
    "        # ==========================================\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"LOAD PHASE\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # Create table if not exists\n",
    "        create_table_sql = \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS analytics.product_performance (\n",
    "                product_id INTEGER PRIMARY KEY,\n",
    "                product_name VARCHAR(255),\n",
    "                category VARCHAR(100),\n",
    "                number_of_orders INTEGER,\n",
    "                total_quantity_sold INTEGER,\n",
    "                total_revenue DECIMAL(15,2),\n",
    "                avg_unit_price DECIMAL(10,2),\n",
    "                avg_order_value DECIMAL(10,2),\n",
    "                daily_avg_revenue DECIMAL(10,2),\n",
    "                revenue_rank INTEGER,\n",
    "                quantity_rank INTEGER,\n",
    "                first_sale_date DATE,\n",
    "                last_sale_date DATE,\n",
    "                days_in_period INTEGER,\n",
    "                analysis_start_date DATE,\n",
    "                analysis_end_date DATE,\n",
    "                created_at TIMESTAMP\n",
    "            );\n",
    "        \"\"\"\n",
    "        \n",
    "        db.execute_query(create_table_sql, fetch=False)\n",
    "        logger.info(\"‚úÖ Table created/verified\")\n",
    "        \n",
    "        # Load data\n",
    "        rows_loaded = db.write_dataframe(\n",
    "            product_metrics,\n",
    "            table_name='product_performance',\n",
    "            schema='analytics',\n",
    "            if_exists='replace'\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"‚úÖ Loaded {rows_loaded} rows to analytics.product_performance\")\n",
    "        \n",
    "        # ==========================================\n",
    "        # COMPLETION\n",
    "        # ==========================================\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        return product_metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Pipeline failed: {e}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "\n",
    "# Run the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nüöÄ Starting Product Performance Pipeline...\\n\")\n",
    "    \n",
    "    result = product_performance_pipeline()\n",
    "    \n",
    "    if result is not None:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üìä TOP 10 PRODUCTS BY REVENUE\")\n",
    "        print(\"=\" * 60)\n",
    "        top_products = result.nlargest(10, 'total_revenue')[\n",
    "            ['product_name', 'category', 'total_revenue', 'total_quantity_sold', 'number_of_orders']\n",
    "        ]\n",
    "        print(top_products.to_string(index=False))\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üìà CATEGORY PERFORMANCE\")\n",
    "        print(\"=\" * 60)\n",
    "        category_summary = result.groupby('category').agg({\n",
    "            'total_revenue': 'sum',\n",
    "            'total_quantity_sold': 'sum',\n",
    "            'product_id': 'count'\n",
    "        }).round(2)\n",
    "        category_summary.columns = ['Total Revenue', 'Total Quantity', 'Number of Products']\n",
    "        print(category_summary.to_string())\n",
    "        \n",
    "        print(\"\\n‚úÖ Pipeline completed successfully!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Pipeline failed or no data found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìö KEY TAKEAWAYS\n",
    "\n",
    "### ETL Best Practices:\n",
    "1. **Always log** - Track what's happening\n",
    "2. **Handle errors** - Use try/except blocks\n",
    "3. **Validate data** - Check before and after transformations\n",
    "4. **Use transactions** - Ensure data consistency\n",
    "5. **Make it idempotent** - Safe to run multiple times\n",
    "6. **Document** - Clear docstrings and comments\n",
    "7. **Test** - Unit tests for each function\n",
    "8. **Monitor** - Track pipeline performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 08:12:41,047 - db_connector - INFO - Database connector initialized for data_engineer@postgres\n",
      "/home/jovyan/week-03-04-python-etl/scripts/db_connector.py:105: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn, params=params)\n",
      "2025-12-18 08:12:41,082 - db_connector - INFO - Query executed, DataFrame shape: (8, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Columns in analytics.order_items:\n",
      "        column_name                    data_type\n",
      "0     order_item_id                      integer\n",
      "1          order_id                      integer\n",
      "2        product_id                      integer\n",
      "3          quantity                      integer\n",
      "4        unit_price                      numeric\n",
      "5  discount_percent                      numeric\n",
      "6        line_total                      numeric\n",
      "7        created_at  timestamp without time zone\n"
     ]
    }
   ],
   "source": [
    "# Ch·∫°y cell n√†y ƒë·ªÉ xem c·∫•u tr√∫c b·∫£ng\n",
    "db = DatabaseConnector()\n",
    "\n",
    "# Ki·ªÉm tra columns c·ªßa order_items\n",
    "check_query = \"\"\"\n",
    "    SELECT column_name, data_type \n",
    "    FROM information_schema.columns \n",
    "    WHERE table_schema = 'analytics' \n",
    "      AND table_name = 'order_items'\n",
    "    ORDER BY ordinal_position\n",
    "\"\"\"\n",
    "\n",
    "columns = db.read_sql(check_query)\n",
    "print(\"üìã Columns in analytics.order_items:\")\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
