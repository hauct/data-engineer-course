{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”„ BÃ€I 3: ETL PATTERNS & BEST PRACTICES\n",
    "\n",
    "## Má»¥c tiÃªu:\n",
    "- Extract patterns (Database, CSV, API)\n",
    "- Transform patterns (Cleaning, Enrichment, Aggregation)\n",
    "- Load patterns (Database, Files)\n",
    "- Error handling & logging\n",
    "- Pipeline orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import sys\n",
    "sys.path.append('/home/jovyan/week-03-04-python-etl/scripts')\n",
    "\n",
    "from db_connector import DatabaseConnector\n",
    "from data_cleaner import DataCleaner\n",
    "from etl_pipeline import ETLPipeline\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“Š PART 1: Extract Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 1: Extract from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 17:40:41,237 - db_connector - INFO - Database connector initialized for data_engineer@postgres\n",
      "2025-12-17 17:40:41,238 - __main__ - INFO - Starting customer extraction...\n",
      "/home/jovyan/week-03-04-python-etl/scripts/db_connector.py:105: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn, params=params)\n",
      "2025-12-17 17:40:41,418 - db_connector - INFO - Query executed, DataFrame shape: (1000, 8)\n",
      "2025-12-17 17:40:41,420 - __main__ - INFO - Extracted 1000 customers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1000 customers\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>customer_name</th>\n",
       "      <th>email</th>\n",
       "      <th>country</th>\n",
       "      <th>signup_date</th>\n",
       "      <th>customer_segment</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Timothy Vincent</td>\n",
       "      <td>maria99@example.org</td>\n",
       "      <td>Azerbaijan</td>\n",
       "      <td>2023-06-18</td>\n",
       "      <td>Standard</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Edward Williamson</td>\n",
       "      <td>mcdonaldlisa@example.com</td>\n",
       "      <td>Anguilla</td>\n",
       "      <td>2025-05-09</td>\n",
       "      <td>Basic</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Jessica Reed</td>\n",
       "      <td>sdavis@example.net</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>2025-03-11</td>\n",
       "      <td>Basic</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Carrie Davis</td>\n",
       "      <td>taylorleslie@example.org</td>\n",
       "      <td>Cote d'Ivoire</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>Standard</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Rita Fuller</td>\n",
       "      <td>gonzalezsamantha@example.org</td>\n",
       "      <td>Greece</td>\n",
       "      <td>2023-07-26</td>\n",
       "      <td>Premium</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id      customer_name                         email  \\\n",
       "0            1    Timothy Vincent           maria99@example.org   \n",
       "1            2  Edward Williamson      mcdonaldlisa@example.com   \n",
       "2            3       Jessica Reed            sdavis@example.net   \n",
       "3            4       Carrie Davis      taylorleslie@example.org   \n",
       "4            5        Rita Fuller  gonzalezsamantha@example.org   \n",
       "\n",
       "         country signup_date customer_segment                 created_at  \\\n",
       "0     Azerbaijan  2023-06-18         Standard 2025-12-14 10:05:15.115901   \n",
       "1       Anguilla  2025-05-09            Basic 2025-12-14 10:05:15.115901   \n",
       "2       Thailand  2025-03-11            Basic 2025-12-14 10:05:15.115901   \n",
       "3  Cote d'Ivoire  2023-11-01         Standard 2025-12-14 10:05:15.115901   \n",
       "4         Greece  2023-07-26          Premium 2025-12-14 10:05:15.115901   \n",
       "\n",
       "                  updated_at  \n",
       "0 2025-12-14 10:05:15.115901  \n",
       "1 2025-12-14 10:05:15.115901  \n",
       "2 2025-12-14 10:05:15.115901  \n",
       "3 2025-12-14 10:05:15.115901  \n",
       "4 2025-12-14 10:05:15.115901  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Create extract function with error handling\n",
    "def extract_customers(db, date_from=None):\n",
    "    \"\"\"\n",
    "    Extract customers from database\n",
    "    \n",
    "    Args:\n",
    "        db: DatabaseConnector instance\n",
    "        date_from: Optional date filter\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with customers\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Starting customer extraction...\")\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        query = \"\"\"\n",
    "            SELECT * FROM analytics.customers\n",
    "            WHERE 1=1\n",
    "        \"\"\"\n",
    "        \n",
    "        if date_from:\n",
    "            query += f\" AND created_at >= '{date_from}'\"\n",
    "        \n",
    "        df = db.read_sql(query)\n",
    "        \n",
    "        logger.info(f\"Extracted {len(df)} customers\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Extraction failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Test\n",
    "db = DatabaseConnector()\n",
    "customers = extract_customers(db)\n",
    "print(f\"Extracted {len(customers)} customers\")\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Incremental Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/week-03-04-python-etl/scripts/db_connector.py:105: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn, params=params)\n",
      "2025-12-17 17:40:51,889 - db_connector - INFO - Query executed, DataFrame shape: (10000, 7)\n",
      "2025-12-17 17:40:51,890 - __main__ - INFO - Extracted 10000 new/updated orders since 2025-12-10 17:40:51.826839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New orders: 10000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement incremental extraction\n",
    "def extract_orders_incremental(db, last_extracted_date):\n",
    "    \"\"\"\n",
    "    Extract only new/updated orders since last extraction\n",
    "    \n",
    "    Args:\n",
    "        db: DatabaseConnector\n",
    "        last_extracted_date: Last extraction timestamp\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with new orders\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    query = \"\"\"\n",
    "        SELECT * FROM analytics.orders\n",
    "        WHERE updated_at > %s\n",
    "        ORDER BY updated_at\n",
    "    \"\"\"\n",
    "    \n",
    "    df = db.read_sql(query, (last_extracted_date,))\n",
    "    logger.info(f\"Extracted {len(df)} new/updated orders since {last_extracted_date}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Test\n",
    "last_date = datetime.now() - timedelta(days=7)\n",
    "new_orders = extract_orders_incremental(db, last_date)\n",
    "print(f\"New orders: {len(new_orders)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 3: Extract from CSV with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create CSV extraction with validation\n",
    "def extract_from_csv(file_path, expected_columns):\n",
    "    \"\"\"\n",
    "    Extract data from CSV with validation\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to CSV file\n",
    "        expected_columns: List of expected column names\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Reading CSV: {file_path}\")\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Validate columns\n",
    "        missing_cols = set(expected_columns) - set(df.columns)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing columns: {missing_cols}\")\n",
    "        \n",
    "        logger.info(f\"Successfully loaded {len(df)} rows\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"CSV extraction failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Create sample CSV for testing\n",
    "sample_data = pd.DataFrame({\n",
    "    'product_id': [1, 2, 3],\n",
    "    'product_name': ['Product A', 'Product B', 'Product C'],\n",
    "    'price': [100, 200, 300]\n",
    "})\n",
    "sample_data.to_csv('/home/jovyan/work/week-03-04-python-etl/data/raw/sample_products.csv', index=False)\n",
    "\n",
    "# Test\n",
    "products = extract_from_csv(\n",
    "    '/home/jovyan/work/week-03-04-python-etl/data/raw/sample_products.csv',\n",
    "    ['product_id', 'product_name', 'price']\n",
    ")\n",
    "products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ”§ PART 2: Transform Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 1: Data Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 17:41:32,818 - db_connector - INFO - Query executed, DataFrame shape: (1000, 7)\n",
      "2025-12-17 17:41:32,820 - __main__ - INFO - Enriching customer data...\n",
      "2025-12-17 17:41:32,906 - __main__ - INFO - Enriched 1000 customers\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>customer_name</th>\n",
       "      <th>email</th>\n",
       "      <th>country</th>\n",
       "      <th>signup_date</th>\n",
       "      <th>customer_segment</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>total_orders</th>\n",
       "      <th>total_revenue</th>\n",
       "      <th>avg_order_value</th>\n",
       "      <th>max_order_value</th>\n",
       "      <th>first_order_date</th>\n",
       "      <th>last_order_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Timothy Vincent</td>\n",
       "      <td>maria99@example.org</td>\n",
       "      <td>Azerbaijan</td>\n",
       "      <td>2023-06-18</td>\n",
       "      <td>Standard</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Edward Williamson</td>\n",
       "      <td>mcdonaldlisa@example.com</td>\n",
       "      <td>Anguilla</td>\n",
       "      <td>2025-05-09</td>\n",
       "      <td>Basic</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4694.03</td>\n",
       "      <td>4694.030000</td>\n",
       "      <td>4694.03</td>\n",
       "      <td>2024-02-02</td>\n",
       "      <td>2024-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Jessica Reed</td>\n",
       "      <td>sdavis@example.net</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>2025-03-11</td>\n",
       "      <td>Basic</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21812.05</td>\n",
       "      <td>7270.683333</td>\n",
       "      <td>10240.26</td>\n",
       "      <td>2024-07-08</td>\n",
       "      <td>2025-11-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Carrie Davis</td>\n",
       "      <td>taylorleslie@example.org</td>\n",
       "      <td>Cote d'Ivoire</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>Standard</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3342.87</td>\n",
       "      <td>3342.870000</td>\n",
       "      <td>3342.87</td>\n",
       "      <td>2024-04-14</td>\n",
       "      <td>2024-04-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Rita Fuller</td>\n",
       "      <td>gonzalezsamantha@example.org</td>\n",
       "      <td>Greece</td>\n",
       "      <td>2023-07-26</td>\n",
       "      <td>Premium</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id      customer_name                         email  \\\n",
       "0            1    Timothy Vincent           maria99@example.org   \n",
       "1            2  Edward Williamson      mcdonaldlisa@example.com   \n",
       "2            3       Jessica Reed            sdavis@example.net   \n",
       "3            4       Carrie Davis      taylorleslie@example.org   \n",
       "4            5        Rita Fuller  gonzalezsamantha@example.org   \n",
       "\n",
       "         country signup_date customer_segment                 created_at  \\\n",
       "0     Azerbaijan  2023-06-18         Standard 2025-12-14 10:05:15.115901   \n",
       "1       Anguilla  2025-05-09            Basic 2025-12-14 10:05:15.115901   \n",
       "2       Thailand  2025-03-11            Basic 2025-12-14 10:05:15.115901   \n",
       "3  Cote d'Ivoire  2023-11-01         Standard 2025-12-14 10:05:15.115901   \n",
       "4         Greece  2023-07-26          Premium 2025-12-14 10:05:15.115901   \n",
       "\n",
       "                  updated_at  total_orders  total_revenue  avg_order_value  \\\n",
       "0 2025-12-14 10:05:15.115901           0.0           0.00              NaN   \n",
       "1 2025-12-14 10:05:15.115901           1.0        4694.03      4694.030000   \n",
       "2 2025-12-14 10:05:15.115901           3.0       21812.05      7270.683333   \n",
       "3 2025-12-14 10:05:15.115901           1.0        3342.87      3342.870000   \n",
       "4 2025-12-14 10:05:15.115901           0.0           0.00              NaN   \n",
       "\n",
       "   max_order_value first_order_date last_order_date  \n",
       "0              NaN              NaN             NaN  \n",
       "1          4694.03       2024-02-02      2024-02-02  \n",
       "2         10240.26       2024-07-08      2025-11-12  \n",
       "3          3342.87       2024-04-14      2024-04-14  \n",
       "4              NaN              NaN             NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Create enrichment transformation\n",
    "def enrich_customer_data(customers_df, orders_df):\n",
    "    \"\"\"\n",
    "    Enrich customers with order statistics\n",
    "    \n",
    "    Args:\n",
    "        customers_df: Customer DataFrame\n",
    "        orders_df: Orders DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        Enriched DataFrame\n",
    "    \"\"\"\n",
    "    logger.info(\"Enriching customer data...\")\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # Calculate order statistics per customer\n",
    "    order_stats = orders_df.groupby('customer_id').agg({\n",
    "        'order_id': 'count',\n",
    "        'total_amount': ['sum', 'mean', 'max'],\n",
    "        'order_date': ['min', 'max']\n",
    "    }).reset_index()\n",
    "    \n",
    "    order_stats.columns = [\n",
    "        'customer_id', 'total_orders', 'total_revenue',\n",
    "        'avg_order_value', 'max_order_value',\n",
    "        'first_order_date', 'last_order_date'\n",
    "    ]\n",
    "    \n",
    "    # Merge with customers\n",
    "    enriched = customers_df.merge(order_stats, on='customer_id', how='left')\n",
    "    \n",
    "    # Fill nulls for customers without orders\n",
    "    enriched['total_orders'] = enriched['total_orders'].fillna(0)\n",
    "    enriched['total_revenue'] = enriched['total_revenue'].fillna(0)\n",
    "    \n",
    "    logger.info(f\"Enriched {len(enriched)} customers\")\n",
    "    return enriched\n",
    "\n",
    "# Test\n",
    "orders = db.read_sql(\"SELECT * FROM analytics.orders LIMIT 1000\")\n",
    "enriched_customers = enrich_customer_data(customers, orders)\n",
    "enriched_customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 17:46:00,092 - __main__ - INFO - Creating daily summary...\n",
      "2025-12-17 17:46:00,116 - __main__ - INFO - Created summary for 546 days\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>total_orders</th>\n",
       "      <th>total_revenue</th>\n",
       "      <th>avg_order_value</th>\n",
       "      <th>unique_customers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-12-15</td>\n",
       "      <td>1</td>\n",
       "      <td>8405.16</td>\n",
       "      <td>8405.160</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-12-16</td>\n",
       "      <td>1</td>\n",
       "      <td>2687.41</td>\n",
       "      <td>2687.410</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-12-17</td>\n",
       "      <td>5</td>\n",
       "      <td>25711.65</td>\n",
       "      <td>5142.330</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-12-18</td>\n",
       "      <td>1</td>\n",
       "      <td>9261.83</td>\n",
       "      <td>9261.830</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-12-19</td>\n",
       "      <td>2</td>\n",
       "      <td>7797.27</td>\n",
       "      <td>3898.635</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>2</td>\n",
       "      <td>6213.13</td>\n",
       "      <td>3106.565</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-12-21</td>\n",
       "      <td>1</td>\n",
       "      <td>3577.18</td>\n",
       "      <td>3577.180</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-12-22</td>\n",
       "      <td>1</td>\n",
       "      <td>517.66</td>\n",
       "      <td>517.660</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-12-24</td>\n",
       "      <td>2</td>\n",
       "      <td>8047.95</td>\n",
       "      <td>4023.975</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-12-25</td>\n",
       "      <td>1</td>\n",
       "      <td>6594.96</td>\n",
       "      <td>6594.960</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  total_orders  total_revenue  avg_order_value  unique_customers\n",
       "0  2023-12-15             1        8405.16         8405.160                 1\n",
       "1  2023-12-16             1        2687.41         2687.410                 1\n",
       "2  2023-12-17             5       25711.65         5142.330                 5\n",
       "3  2023-12-18             1        9261.83         9261.830                 1\n",
       "4  2023-12-19             2        7797.27         3898.635                 2\n",
       "5  2023-12-20             2        6213.13         3106.565                 2\n",
       "6  2023-12-21             1        3577.18         3577.180                 1\n",
       "7  2023-12-22             1         517.66          517.660                 1\n",
       "8  2023-12-24             2        8047.95         4023.975                 2\n",
       "9  2023-12-25             1        6594.96         6594.960                 1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Create aggregation transformation\n",
    "def create_daily_summary(orders_df):\n",
    "    \"\"\"\n",
    "    Create daily order summary\n",
    "    \n",
    "    Args:\n",
    "        orders_df: Orders DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        Daily summary DataFrame\n",
    "    \"\"\"\n",
    "    logger.info(\"Creating daily summary...\")\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    orders_df['order_date'] = pd.to_datetime(orders_df['order_date'])\n",
    "    \n",
    "    daily_summary = orders_df.groupby(orders_df['order_date'].dt.date).agg({\n",
    "        'order_id': 'count',\n",
    "        'total_amount': ['sum', 'mean'],\n",
    "        'customer_id': 'nunique'\n",
    "    }).reset_index()\n",
    "    \n",
    "    daily_summary.columns = [\n",
    "        'date', 'total_orders', 'total_revenue',\n",
    "        'avg_order_value', 'unique_customers'\n",
    "    ]\n",
    "    \n",
    "    logger.info(f\"Created summary for {len(daily_summary)} days\")\n",
    "    return daily_summary\n",
    "\n",
    "# Test\n",
    "daily_summary = create_daily_summary(orders)\n",
    "daily_summary.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 3: Data Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 17:46:07,154 - __main__ - INFO - Cleaning customer data...\n",
      "2025-12-17 17:46:07,157 - data_cleaner - INFO - DataCleaner initialized with shape (1000, 8)\n",
      "2025-12-17 17:46:07,159 - data_cleaner - INFO - remove_duplicates: Removed 0 duplicates (0.00%)\n",
      "2025-12-17 17:46:07,162 - data_cleaner - INFO - handle_missing: customer_name: 0 â†’ 0 missing values\n",
      "2025-12-17 17:46:07,164 - data_cleaner - INFO - handle_missing: email: 0 â†’ 0 missing values\n",
      "2025-12-17 17:46:07,166 - data_cleaner - INFO - handle_missing: country: 0 â†’ 0 missing values\n",
      "2025-12-17 17:46:07,171 - data_cleaner - INFO - standardize_text: Standardized customer_name\n",
      "2025-12-17 17:46:07,178 - data_cleaner - INFO - standardize_text: Standardized country\n",
      "2025-12-17 17:46:07,179 - data_cleaner - INFO - Cleaning complete: (1000, 8) â†’ (1000, 8)\n",
      "2025-12-17 17:46:07,180 - __main__ - INFO - Cleaning complete: 1000 â†’ 1000 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>customer_name</th>\n",
       "      <th>email</th>\n",
       "      <th>country</th>\n",
       "      <th>signup_date</th>\n",
       "      <th>customer_segment</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>timothy vincent</td>\n",
       "      <td>maria99@example.org</td>\n",
       "      <td>azerbaijan</td>\n",
       "      <td>2023-06-18</td>\n",
       "      <td>Standard</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>edward williamson</td>\n",
       "      <td>mcdonaldlisa@example.com</td>\n",
       "      <td>anguilla</td>\n",
       "      <td>2025-05-09</td>\n",
       "      <td>Basic</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>jessica reed</td>\n",
       "      <td>sdavis@example.net</td>\n",
       "      <td>thailand</td>\n",
       "      <td>2025-03-11</td>\n",
       "      <td>Basic</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>carrie davis</td>\n",
       "      <td>taylorleslie@example.org</td>\n",
       "      <td>cote d'ivoire</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>Standard</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>rita fuller</td>\n",
       "      <td>gonzalezsamantha@example.org</td>\n",
       "      <td>greece</td>\n",
       "      <td>2023-07-26</td>\n",
       "      <td>Premium</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "      <td>2025-12-14 10:05:15.115901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id      customer_name                         email  \\\n",
       "0            1    timothy vincent           maria99@example.org   \n",
       "1            2  edward williamson      mcdonaldlisa@example.com   \n",
       "2            3       jessica reed            sdavis@example.net   \n",
       "3            4       carrie davis      taylorleslie@example.org   \n",
       "4            5        rita fuller  gonzalezsamantha@example.org   \n",
       "\n",
       "         country signup_date customer_segment                 created_at  \\\n",
       "0     azerbaijan  2023-06-18         Standard 2025-12-14 10:05:15.115901   \n",
       "1       anguilla  2025-05-09            Basic 2025-12-14 10:05:15.115901   \n",
       "2       thailand  2025-03-11            Basic 2025-12-14 10:05:15.115901   \n",
       "3  cote d'ivoire  2023-11-01         Standard 2025-12-14 10:05:15.115901   \n",
       "4         greece  2023-07-26          Premium 2025-12-14 10:05:15.115901   \n",
       "\n",
       "                  updated_at  \n",
       "0 2025-12-14 10:05:15.115901  \n",
       "1 2025-12-14 10:05:15.115901  \n",
       "2 2025-12-14 10:05:15.115901  \n",
       "3 2025-12-14 10:05:15.115901  \n",
       "4 2025-12-14 10:05:15.115901  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Create reusable cleaning pipeline\n",
    "def clean_customer_data(df):\n",
    "    \"\"\"\n",
    "    Standard customer data cleaning\n",
    "    \"\"\"\n",
    "    logger.info(\"Cleaning customer data...\")\n",
    "    \n",
    "    cleaner = DataCleaner(df)\n",
    "    \n",
    "    cleaned = (\n",
    "        cleaner\n",
    "        .remove_duplicates(subset=['customer_id'])\n",
    "        .handle_missing_values({\n",
    "            'customer_name': 'Unknown',\n",
    "            'email': 'no-email@unknown.com',\n",
    "            'country': 'Unknown'\n",
    "        })\n",
    "        .standardize_text(['customer_name', 'country'])\n",
    "        .get_cleaned_data()\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Cleaning complete: {len(df)} â†’ {len(cleaned)} rows\")\n",
    "    return cleaned\n",
    "\n",
    "# Test\n",
    "cleaned_customers = clean_customer_data(customers)\n",
    "cleaned_customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ’¾ PART 3: Load Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 1: Full Load (Replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement full load pattern\n",
    "def load_full_replace(df, table_name, db):\n",
    "    \"\"\"\n",
    "    Full load - replace entire table\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading {len(df)} rows to {table_name} (REPLACE)\")\n",
    "    \n",
    "    try:\n",
    "        # YOUR CODE HERE\n",
    "        rows = db.write_dataframe(\n",
    "            df,\n",
    "            table_name,\n",
    "            schema='analytics',\n",
    "            if_exists='replace'\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Successfully loaded {rows} rows\")\n",
    "        return rows\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Load failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Test (don't actually run to preserve data)\n",
    "# load_full_replace(daily_summary, 'daily_order_summary', db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Incremental Load (Append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement incremental load\n",
    "def load_incremental(df, table_name, db):\n",
    "    \"\"\"\n",
    "    Incremental load - append new records\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading {len(df)} rows to {table_name} (APPEND)\")\n",
    "    \n",
    "    try:\n",
    "        # YOUR CODE HERE\n",
    "        rows = db.write_dataframe(\n",
    "            df,\n",
    "            table_name,\n",
    "            schema='analytics',\n",
    "            if_exists='append'\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Successfully appended {rows} rows\")\n",
    "        return rows\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Load failed: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 3: Upsert (Update or Insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement upsert pattern\n",
    "def load_upsert(df, table_name, key_columns, db):\n",
    "    \"\"\"\n",
    "    Upsert - update existing records or insert new ones\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to load\n",
    "        table_name: Target table\n",
    "        key_columns: Columns to match for updates\n",
    "        db: DatabaseConnector\n",
    "    \"\"\"\n",
    "    logger.info(f\"Upserting {len(df)} rows to {table_name}\")\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # Strategy: Load to temp table, then merge\n",
    "    temp_table = f\"{table_name}_temp\"\n",
    "    \n",
    "    # Load to temp table\n",
    "    db.write_dataframe(df, temp_table, if_exists='replace')\n",
    "    \n",
    "    # Build upsert query\n",
    "    key_condition = \" AND \".join([f\"t.{col} = s.{col}\" for col in key_columns])\n",
    "    \n",
    "    upsert_query = f\"\"\"\n",
    "        -- Delete existing records\n",
    "        DELETE FROM analytics.{table_name} t\n",
    "        USING analytics.{temp_table} s\n",
    "        WHERE {key_condition};\n",
    "        \n",
    "        -- Insert all records from temp\n",
    "        INSERT INTO analytics.{table_name}\n",
    "        SELECT * FROM analytics.{temp_table};\n",
    "        \n",
    "        -- Drop temp table\n",
    "        DROP TABLE analytics.{temp_table};\n",
    "    \"\"\"\n",
    "    \n",
    "    db.execute_query(upsert_query, fetch=False)\n",
    "    logger.info(f\"Upsert complete\")\n",
    "\n",
    "# Test (commented out)\n",
    "# load_upsert(enriched_customers, 'customers_enriched', ['customer_id'], db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ EXERCISE: Build Complete ETL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customer_enrichment_pipeline():\n",
    "    \"\"\"Using ETLPipeline framework\"\"\"\n",
    "    \n",
    "    pipeline = ETLPipeline('customer_enrichment')\n",
    "    \n",
    "    try:\n",
    "        # EXTRACT - DÃ¹ng pipeline.extract()\n",
    "        customers = pipeline.extract(\n",
    "            source='database',\n",
    "            query=\"SELECT * FROM analytics.customers\"\n",
    "        )\n",
    "        \n",
    "        orders = pipeline.extract(\n",
    "            source='database',\n",
    "            query=\"SELECT * FROM analytics.orders\"\n",
    "        )\n",
    "        \n",
    "        # TRANSFORM - DÃ¹ng pipeline.transform()\n",
    "        def calc_stats(df):\n",
    "            return df.groupby('customer_id').agg({\n",
    "                'order_id': 'count',\n",
    "                'total_amount': ['sum', 'mean']\n",
    "            }).reset_index()\n",
    "        \n",
    "        def merge_data(df):\n",
    "            order_stats = calc_stats(orders)\n",
    "            return df.merge(order_stats, on='customer_id', how='left')\n",
    "        \n",
    "        enriched = pipeline.transform(\n",
    "            customers,\n",
    "            transformations=[merge_data]\n",
    "        )\n",
    "        \n",
    "        # LOAD - DÃ¹ng pipeline.load()\n",
    "        pipeline.load(\n",
    "            enriched,\n",
    "            destination='database',\n",
    "            table='customers_enriched'\n",
    "        )\n",
    "        \n",
    "        # Get summary\n",
    "        summary = pipeline.get_summary()\n",
    "        print(summary)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        pipeline.log_step('ERROR', 'FAILED', str(e))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ CHALLENGE: Build Your Own Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 18:20:08,438 - db_connector - INFO - Database connector initialized for data_engineer@postgres\n",
      "2025-12-17 18:20:08,440 - __main__ - INFO - ============================================================\n",
      "2025-12-17 18:20:08,442 - __main__ - INFO - EXTRACT PHASE\n",
      "2025-12-17 18:20:08,443 - __main__ - INFO - ============================================================\n",
      "2025-12-17 18:20:08,445 - __main__ - INFO - Extracting orders from 2025-11-17 to 2025-12-17\n",
      "/home/jovyan/week-03-04-python-etl/scripts/db_connector.py:105: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn, params=params)\n",
      "2025-12-17 18:20:08,508 - db_connector - INFO - Query executed, DataFrame shape: (1030, 10)\n",
      "2025-12-17 18:20:08,510 - __main__ - INFO - âœ… Extracted 1030 order items\n",
      "2025-12-17 18:20:08,512 - __main__ - INFO - ============================================================\n",
      "2025-12-17 18:20:08,512 - __main__ - INFO - TRANSFORM PHASE\n",
      "2025-12-17 18:20:08,513 - __main__ - INFO - ============================================================\n",
      "2025-12-17 18:20:08,514 - __main__ - INFO - Calculating product metrics...\n",
      "2025-12-17 18:20:08,558 - __main__ - INFO - âœ… Calculated metrics for 100 products\n",
      "2025-12-17 18:20:08,582 - __main__ - INFO - âœ… Transformation complete\n",
      "2025-12-17 18:20:08,583 - __main__ - INFO - \n",
      "ðŸ“Š PERFORMANCE SUMMARY:\n",
      "2025-12-17 18:20:08,584 - __main__ - INFO -   Total Products: 100\n",
      "2025-12-17 18:20:08,585 - __main__ - INFO -   Total Revenue: $1,690,651.81\n",
      "2025-12-17 18:20:08,586 - __main__ - INFO -   Total Quantity Sold: 3,041\n",
      "2025-12-17 18:20:08,588 - __main__ - INFO -   Avg Order Value: $1,643.03\n",
      "2025-12-17 18:20:08,588 - __main__ - INFO - ============================================================\n",
      "2025-12-17 18:20:08,589 - __main__ - INFO - VALIDATION PHASE\n",
      "2025-12-17 18:20:08,590 - __main__ - INFO - ============================================================\n",
      "2025-12-17 18:20:08,590 - __main__ - ERROR - âŒ Pipeline failed: name 'DataValidator' is not defined\n",
      "2025-12-17 18:20:08,591 - __main__ - ERROR - Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1905/420721564.py\", line 131, in product_performance_pipeline\n",
      "    DataValidator(product_metrics, \"product_performance\")\n",
      "    ^^^^^^^^^^^^^\n",
      "NameError: name 'DataValidator' is not defined\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starting Product Performance Pipeline...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DataValidator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 210\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸš€ Starting Product Performance Pipeline...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 210\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mproduct_performance_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 131\u001b[0m, in \u001b[0;36mproduct_performance_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVALIDATION PHASE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    128\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m    130\u001b[0m validator \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 131\u001b[0m     \u001b[43mDataValidator\u001b[49m(product_metrics, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_performance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;241m.\u001b[39mcheck_not_null([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;241m.\u001b[39mcheck_unique([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;241m.\u001b[39mcheck_range(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_revenue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;241m.\u001b[39mcheck_range(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_quantity_sold\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m.\u001b[39mcheck_range(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber_of_orders\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    137\u001b[0m )\n\u001b[1;32m    139\u001b[0m validator\u001b[38;5;241m.\u001b[39mprint_report()\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Check if validation passed\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataValidator' is not defined"
     ]
    }
   ],
   "source": [
    "def product_performance_pipeline():\n",
    "    \"\"\"\n",
    "    Product Performance ETL Pipeline - FIXED VERSION\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    try:\n",
    "        # Initialize database connection\n",
    "        db = DatabaseConnector()\n",
    "        \n",
    "        # ==========================================\n",
    "        # EXTRACT PHASE\n",
    "        # ==========================================\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"EXTRACT PHASE\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # Calculate date range\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=30)\n",
    "        \n",
    "        logger.info(f\"Extracting orders from {start_date.date()} to {end_date.date()}\")\n",
    "        \n",
    "        # âœ… FIX: Calculate subtotal instead of selecting it\n",
    "        orders_query = \"\"\"\n",
    "            SELECT \n",
    "                o.order_id,\n",
    "                o.customer_id,\n",
    "                o.order_date,\n",
    "                o.total_amount,\n",
    "                oi.product_id,\n",
    "                oi.quantity,\n",
    "                oi.unit_price,\n",
    "                (oi.quantity * oi.unit_price) as subtotal,  -- âœ… TÃNH SUBTOTAL\n",
    "                p.product_name,\n",
    "                p.category\n",
    "            FROM analytics.orders o\n",
    "            JOIN analytics.order_items oi ON o.order_id = oi.order_id\n",
    "            JOIN analytics.products p ON oi.product_id = p.product_id\n",
    "            WHERE o.order_date >= %s\n",
    "              AND o.order_date <= %s\n",
    "        \"\"\"\n",
    "        \n",
    "        orders_df = db.read_sql(orders_query, (start_date, end_date))\n",
    "        logger.info(f\"âœ… Extracted {len(orders_df)} order items\")\n",
    "        \n",
    "        if len(orders_df) == 0:\n",
    "            logger.warning(\"âš ï¸ No orders found in the last 30 days\")\n",
    "            return None\n",
    "        \n",
    "        # ==========================================\n",
    "        # TRANSFORM PHASE\n",
    "        # ==========================================\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"TRANSFORM PHASE\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # 1. Calculate product performance metrics\n",
    "        logger.info(\"Calculating product metrics...\")\n",
    "        \n",
    "        product_metrics = orders_df.groupby(['product_id', 'product_name', 'category']).agg({\n",
    "            'order_id': 'nunique',           # Number of unique orders\n",
    "            'quantity': 'sum',                # Total quantity sold\n",
    "            'subtotal': 'sum',                # Total revenue\n",
    "            'unit_price': 'mean',             # Average price\n",
    "            'order_date': ['min', 'max']      # First and last sale date\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Flatten column names\n",
    "        product_metrics.columns = [\n",
    "            'product_id', 'product_name', 'category',\n",
    "            'number_of_orders', 'total_quantity_sold', 'total_revenue',\n",
    "            'avg_unit_price', 'first_sale_date', 'last_sale_date'\n",
    "        ]\n",
    "        \n",
    "        logger.info(f\"âœ… Calculated metrics for {len(product_metrics)} products\")\n",
    "        \n",
    "        # 2. Calculate average order value per product\n",
    "        product_metrics['avg_order_value'] = (\n",
    "            product_metrics['total_revenue'] / product_metrics['number_of_orders']\n",
    "        ).round(2)\n",
    "        \n",
    "        # 3. Calculate revenue trend (daily average)\n",
    "        product_metrics['days_in_period'] = (\n",
    "            pd.to_datetime(product_metrics['last_sale_date']) - \n",
    "            pd.to_datetime(product_metrics['first_sale_date'])\n",
    "        ).dt.days + 1\n",
    "        \n",
    "        product_metrics['daily_avg_revenue'] = (\n",
    "            product_metrics['total_revenue'] / product_metrics['days_in_period']\n",
    "        ).round(2)\n",
    "        \n",
    "        # 4. Add performance ranking\n",
    "        product_metrics['revenue_rank'] = product_metrics['total_revenue'].rank(\n",
    "            ascending=False, method='dense'\n",
    "        ).astype(int)\n",
    "        \n",
    "        product_metrics['quantity_rank'] = product_metrics['total_quantity_sold'].rank(\n",
    "            ascending=False, method='dense'\n",
    "        ).astype(int)\n",
    "        \n",
    "        # 5. Add analysis period\n",
    "        product_metrics['analysis_start_date'] = start_date.date()\n",
    "        product_metrics['analysis_end_date'] = end_date.date()\n",
    "        product_metrics['created_at'] = datetime.now()\n",
    "        \n",
    "        # 6. Round numeric columns\n",
    "        numeric_columns = ['total_revenue', 'avg_unit_price', 'avg_order_value', 'daily_avg_revenue']\n",
    "        product_metrics[numeric_columns] = product_metrics[numeric_columns].round(2)\n",
    "        \n",
    "        logger.info(\"âœ… Transformation complete\")\n",
    "        \n",
    "        # Display summary\n",
    "        logger.info(\"\\nðŸ“Š PERFORMANCE SUMMARY:\")\n",
    "        logger.info(f\"  Total Products: {len(product_metrics)}\")\n",
    "        logger.info(f\"  Total Revenue: ${product_metrics['total_revenue'].sum():,.2f}\")\n",
    "        logger.info(f\"  Total Quantity Sold: {product_metrics['total_quantity_sold'].sum():,}\")\n",
    "        logger.info(f\"  Avg Order Value: ${product_metrics['avg_order_value'].mean():,.2f}\")\n",
    "        \n",
    "        # ==========================================\n",
    "        # VALIDATE PHASE\n",
    "        # ==========================================\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"VALIDATION PHASE\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        validator = (\n",
    "            DataValidator(product_metrics, \"product_performance\")\n",
    "            .check_not_null(['product_id', 'product_name', 'category'])\n",
    "            .check_unique(['product_id'])\n",
    "            .check_range('total_revenue', 0, float('inf'))\n",
    "            .check_range('total_quantity_sold', 0, float('inf'))\n",
    "            .check_range('number_of_orders', 1, float('inf'))\n",
    "        )\n",
    "        \n",
    "        validator.print_report()\n",
    "        \n",
    "        # Check if validation passed\n",
    "        summary = validator.get_summary()\n",
    "        if summary['failed'] > 0:\n",
    "            logger.error(\"âŒ Validation failed! Aborting load.\")\n",
    "            return None\n",
    "        \n",
    "        # ==========================================\n",
    "        # LOAD PHASE\n",
    "        # ==========================================\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"LOAD PHASE\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # Create table if not exists\n",
    "        create_table_sql = \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS analytics.product_performance (\n",
    "                product_id INTEGER PRIMARY KEY,\n",
    "                product_name VARCHAR(255),\n",
    "                category VARCHAR(100),\n",
    "                number_of_orders INTEGER,\n",
    "                total_quantity_sold INTEGER,\n",
    "                total_revenue DECIMAL(15,2),\n",
    "                avg_unit_price DECIMAL(10,2),\n",
    "                avg_order_value DECIMAL(10,2),\n",
    "                daily_avg_revenue DECIMAL(10,2),\n",
    "                revenue_rank INTEGER,\n",
    "                quantity_rank INTEGER,\n",
    "                first_sale_date DATE,\n",
    "                last_sale_date DATE,\n",
    "                days_in_period INTEGER,\n",
    "                analysis_start_date DATE,\n",
    "                analysis_end_date DATE,\n",
    "                created_at TIMESTAMP\n",
    "            );\n",
    "        \"\"\"\n",
    "        \n",
    "        db.execute_query(create_table_sql, fetch=False)\n",
    "        logger.info(\"âœ… Table created/verified\")\n",
    "        \n",
    "        # Load data\n",
    "        rows_loaded = db.write_dataframe(\n",
    "            product_metrics,\n",
    "            table_name='product_performance',\n",
    "            schema='analytics',\n",
    "            if_exists='replace'\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"âœ… Loaded {rows_loaded} rows to analytics.product_performance\")\n",
    "        \n",
    "        # ==========================================\n",
    "        # COMPLETION\n",
    "        # ==========================================\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        return product_metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Pipeline failed: {e}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "\n",
    "# Run the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nðŸš€ Starting Product Performance Pipeline...\\n\")\n",
    "    \n",
    "    result = product_performance_pipeline()\n",
    "    \n",
    "    if result is not None:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ðŸ“Š TOP 10 PRODUCTS BY REVENUE\")\n",
    "        print(\"=\" * 60)\n",
    "        top_products = result.nlargest(10, 'total_revenue')[\n",
    "            ['product_name', 'category', 'total_revenue', 'total_quantity_sold', 'number_of_orders']\n",
    "        ]\n",
    "        print(top_products.to_string(index=False))\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ðŸ“ˆ CATEGORY PERFORMANCE\")\n",
    "        print(\"=\" * 60)\n",
    "        category_summary = result.groupby('category').agg({\n",
    "            'total_revenue': 'sum',\n",
    "            'total_quantity_sold': 'sum',\n",
    "            'product_id': 'count'\n",
    "        }).round(2)\n",
    "        category_summary.columns = ['Total Revenue', 'Total Quantity', 'Number of Products']\n",
    "        print(category_summary.to_string())\n",
    "        \n",
    "        print(\"\\nâœ… Pipeline completed successfully!\")\n",
    "    else:\n",
    "        print(\"\\nâŒ Pipeline failed or no data found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“š KEY TAKEAWAYS\n",
    "\n",
    "### ETL Best Practices:\n",
    "1. **Always log** - Track what's happening\n",
    "2. **Handle errors** - Use try/except blocks\n",
    "3. **Validate data** - Check before and after transformations\n",
    "4. **Use transactions** - Ensure data consistency\n",
    "5. **Make it idempotent** - Safe to run multiple times\n",
    "6. **Document** - Clear docstrings and comments\n",
    "7. **Test** - Unit tests for each function\n",
    "8. **Monitor** - Track pipeline performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 18:19:55,641 - db_connector - INFO - Database connector initialized for data_engineer@postgres\n",
      "2025-12-17 18:19:55,726 - db_connector - INFO - Query executed, DataFrame shape: (8, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Columns in analytics.order_items:\n",
      "        column_name                    data_type\n",
      "0     order_item_id                      integer\n",
      "1          order_id                      integer\n",
      "2        product_id                      integer\n",
      "3          quantity                      integer\n",
      "4        unit_price                      numeric\n",
      "5  discount_percent                      numeric\n",
      "6        line_total                      numeric\n",
      "7        created_at  timestamp without time zone\n"
     ]
    }
   ],
   "source": [
    "# Cháº¡y cell nÃ y Ä‘á»ƒ xem cáº¥u trÃºc báº£ng\n",
    "db = DatabaseConnector()\n",
    "\n",
    "# Kiá»ƒm tra columns cá»§a order_items\n",
    "check_query = \"\"\"\n",
    "    SELECT column_name, data_type \n",
    "    FROM information_schema.columns \n",
    "    WHERE table_schema = 'analytics' \n",
    "      AND table_name = 'order_items'\n",
    "    ORDER BY ordinal_position\n",
    "\"\"\"\n",
    "\n",
    "columns = db.read_sql(check_query)\n",
    "print(\"ðŸ“‹ Columns in analytics.order_items:\")\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
