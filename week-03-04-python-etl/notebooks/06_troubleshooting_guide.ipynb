{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîß 06. TROUBLESHOOTING GUIDE\n",
    "\n",
    "## üéØ M·ª§C TI√äU:\n",
    "- Ch·∫©n ƒëo√°n v√† fix c√°c l·ªói th∆∞·ªùng g·∫∑p\n",
    "- Debug ETL pipeline\n",
    "- Recovery strategies\n",
    "\n",
    "## üìö N·ªòI DUNG:\n",
    "1. Common Errors & Solutions\n",
    "2. Database Connection Issues\n",
    "3. Data Quality Issues\n",
    "4. ETL Pipeline Failures\n",
    "5. Performance Issues\n",
    "6. Recovery Procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from db_connector import DatabaseConnector\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. COMMON ERRORS & SOLUTIONS\n",
    "\n",
    "### ‚ùå Error 1: Connection Refused\n",
    "```\n",
    "psycopg2.OperationalError: could not connect to server: Connection refused\n",
    "```\n",
    "\n",
    "**Causes:**\n",
    "- PostgreSQL service not running\n",
    "- Wrong host/port in .env\n",
    "- Firewall blocking connection\n",
    "\n",
    "**Solutions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç DIAGNOSE: Connection Issues\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if PostgreSQL is running\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    # For macOS/Linux\n",
    "    result = subprocess.run(['pg_isready'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ PostgreSQL is running\")\n",
    "    else:\n",
    "        print(\"‚ùå PostgreSQL is not running\")\n",
    "        print(\"\\nTo start PostgreSQL:\")\n",
    "        print(\"  macOS: brew services start postgresql\")\n",
    "        print(\"  Linux: sudo systemctl start postgresql\")\n",
    "        print(\"  Windows: net start postgresql-x64-14\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è pg_isready not found. Check PostgreSQL installation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check .env configuration\n",
    "print(\"\\nüîç Check .env Configuration:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "required_vars = ['DB_HOST', 'DB_PORT', 'DB_NAME', 'DB_USER', 'DB_PASSWORD']\n",
    "\n",
    "for var in required_vars:\n",
    "    value = os.getenv(var)\n",
    "    if value:\n",
    "        # Mask password\n",
    "        display_value = '***' if var == 'DB_PASSWORD' else value\n",
    "        print(f\"  {var:.<20} {display_value}\")\n",
    "    else:\n",
    "        print(f\"  {var:.<20} ‚ùå NOT SET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test connection\n",
    "print(\"\\nüîç Test Database Connection:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "try:\n",
    "    db = DatabaseConnector()\n",
    "    result = db.read_sql(\"SELECT version()\")\n",
    "    print(\"‚úÖ Connection successful!\")\n",
    "    print(f\"\\nPostgreSQL version: {result['version'][0][:50]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    print(\"\\nüí° Solutions:\")\n",
    "    print(\"  1. Check if PostgreSQL is running\")\n",
    "    print(\"  2. Verify .env configuration\")\n",
    "    print(\"  3. Check firewall settings\")\n",
    "    print(\"  4. Verify database exists: psql -l\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùå Error 2: Table Does Not Exist\n",
    "```\n",
    "psycopg2.errors.UndefinedTable: relation \"raw.customers\" does not exist\n",
    "```\n",
    "\n",
    "**Solutions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç DIAGNOSE: Missing Tables\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "db = DatabaseConnector()\n",
    "\n",
    "# Check schemas\n",
    "schemas_query = \"\"\"\n",
    "SELECT schema_name \n",
    "FROM information_schema.schemata \n",
    "WHERE schema_name IN ('raw', 'staging', 'prod')\n",
    "\"\"\"\n",
    "\n",
    "schemas = db.read_sql(schemas_query)['schema_name'].tolist()\n",
    "\n",
    "print(\"\\nSchemas:\")\n",
    "for schema in ['raw', 'staging', 'prod']:\n",
    "    status = \"‚úÖ\" if schema in schemas else \"‚ùå\"\n",
    "    print(f\"  {status} {schema}\")\n",
    "\n",
    "if len(schemas) < 3:\n",
    "    print(\"\\nüí° Solution: Run schema initialization\")\n",
    "    print(\"  make db-init\")\n",
    "    print(\"  or: psql -d your_db -f sql/init_schema.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check tables in each schema\n",
    "print(\"\\nüîç Check Tables:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "expected_tables = {\n",
    "    'raw': ['customers', 'products', 'orders', 'order_items'],\n",
    "    'staging': ['customers', 'products', 'orders', 'order_items'],\n",
    "    'prod': ['daily_sales', 'monthly_sales', 'daily_category_metrics', \n",
    "             'daily_product_metrics', 'customer_metrics']\n",
    "}\n",
    "\n",
    "missing_tables = []\n",
    "\n",
    "for schema, tables in expected_tables.items():\n",
    "    if schema not in schemas:\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n{schema.upper()} Schema:\")\n",
    "    for table in tables:\n",
    "        check_query = f\"\"\"\n",
    "        SELECT COUNT(*) \n",
    "        FROM information_schema.tables \n",
    "        WHERE table_schema = '{schema}' AND table_name = '{table}'\n",
    "        \"\"\"\n",
    "        exists = db.read_sql(check_query).iloc[0, 0] > 0\n",
    "        status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "        print(f\"  {status} {table}\")\n",
    "        \n",
    "        if not exists:\n",
    "            missing_tables.append(f\"{schema}.{table}\")\n",
    "\n",
    "if missing_tables:\n",
    "    print(\"\\nüí° Solution: Create missing tables\")\n",
    "    print(\"  make db-init\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùå Error 3: Duplicate Key Violation\n",
    "```\n",
    "psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint\n",
    "```\n",
    "\n",
    "**Solutions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç DIAGNOSE: Duplicate Keys\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check for duplicates in staging\n",
    "duplicate_checks = [\n",
    "    (\"staging.customers\", \"customer_id\"),\n",
    "    (\"staging.customers\", \"email\"),\n",
    "    (\"staging.products\", \"product_id\"),\n",
    "    (\"staging.orders\", \"order_id\")\n",
    "]\n",
    "\n",
    "print(\"\\nDuplicate Check:\")\n",
    "for table, column in duplicate_checks:\n",
    "    query = f\"\"\"\n",
    "    SELECT COUNT(*) - COUNT(DISTINCT {column}) as duplicates\n",
    "    FROM {table}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = db.read_sql(query)\n",
    "        dups = result['duplicates'][0]\n",
    "        status = \"‚úÖ\" if dups == 0 else f\"‚ùå {dups} duplicates\"\n",
    "        print(f\"  {table}.{column:.<20} {status}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {table}.{column:.<20} ‚ö†Ô∏è Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicate records\n",
    "print(\"\\nüîç Find Duplicate Records:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Example: Find duplicate emails\n",
    "dup_query = \"\"\"\n",
    "SELECT email, COUNT(*) as count\n",
    "FROM staging.customers\n",
    "GROUP BY email\n",
    "HAVING COUNT(*) > 1\n",
    "ORDER BY count DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    duplicates = db.read_sql(dup_query)\n",
    "    if len(duplicates) > 0:\n",
    "        print(\"\\n‚ùå Found duplicate emails:\")\n",
    "        display(duplicates)\n",
    "        \n",
    "        print(\"\\nüí° Solution: Remove duplicates\")\n",
    "        print(\"  Option 1: Truncate and reload staging\")\n",
    "        print(\"    TRUNCATE staging.customers CASCADE;\")\n",
    "        print(\"    make etl-run-stg\")\n",
    "        print(\"\\n  Option 2: Delete duplicates keeping first\")\n",
    "        print(\"    DELETE FROM staging.customers WHERE customer_id IN (\")\n",
    "        print(\"      SELECT customer_id FROM (\")\n",
    "        print(\"        SELECT customer_id, ROW_NUMBER() OVER (PARTITION BY email ORDER BY created_at) as rn\")\n",
    "        print(\"        FROM staging.customers\")\n",
    "        print(\"      ) t WHERE rn > 1\")\n",
    "        print(\"    );\")\n",
    "    else:\n",
    "        print(\"‚úÖ No duplicate emails found\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DATA QUALITY ISSUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç DIAGNOSE: Data Quality Issues\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check for common data quality issues\n",
    "quality_checks = [\n",
    "    (\"NULL emails\", \"SELECT COUNT(*) FROM staging.customers WHERE email IS NULL\"),\n",
    "    (\"Invalid emails\", \"SELECT COUNT(*) FROM staging.customers WHERE email NOT LIKE '%@%.%'\"),\n",
    "    (\"Negative prices\", \"SELECT COUNT(*) FROM staging.products WHERE price < 0\"),\n",
    "    (\"Future dates\", \"SELECT COUNT(*) FROM staging.orders WHERE order_date > CURRENT_DATE\"),\n",
    "    (\"Orphaned orders\", \n",
    "     \"\"\"SELECT COUNT(*) FROM staging.orders o \n",
    "        WHERE NOT EXISTS (SELECT 1 FROM staging.customers c WHERE c.customer_id = o.customer_id)\"\"\")\n",
    "]\n",
    "\n",
    "print(\"\\nData Quality Issues:\")\n",
    "issues_found = False\n",
    "\n",
    "for check_name, query in quality_checks:\n",
    "    try:\n",
    "        result = db.read_sql(query)\n",
    "        count = result.iloc[0, 0]\n",
    "        if count > 0:\n",
    "            print(f\"  ‚ùå {check_name}: {count} records\")\n",
    "            issues_found = True\n",
    "        else:\n",
    "            print(f\"  ‚úÖ {check_name}: OK\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è {check_name}: Error - {e}\")\n",
    "\n",
    "if issues_found:\n",
    "    print(\"\\nüí° Solutions:\")\n",
    "    print(\"  1. Check data generation logic in generate_raw_data.py\")\n",
    "    print(\"  2. Review transformation logic in etl_stg.py\")\n",
    "    print(\"  3. Re-run ETL pipeline: make etl-run-full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ETL PIPELINE FAILURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç DIAGNOSE: ETL Pipeline Status\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check data flow through pipeline\n",
    "pipeline_query = \"\"\"\n",
    "SELECT \n",
    "    'RAW' as layer,\n",
    "    (SELECT COUNT(*) FROM raw.customers) as customers,\n",
    "    (SELECT COUNT(*) FROM raw.orders) as orders,\n",
    "    (SELECT COUNT(*) FROM raw.order_items) as order_items\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'STAGING' as layer,\n",
    "    (SELECT COUNT(*) FROM staging.customers) as customers,\n",
    "    (SELECT COUNT(*) FROM staging.orders) as orders,\n",
    "    (SELECT COUNT(*) FROM staging.order_items) as order_items\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'PROD' as layer,\n",
    "    (SELECT COUNT(*) FROM prod.customer_metrics) as customers,\n",
    "    (SELECT COUNT(*) FROM prod.daily_sales) as orders,\n",
    "    0 as order_items\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    pipeline_status = db.read_sql(pipeline_query)\n",
    "    display(pipeline_status)\n",
    "    \n",
    "    # Analyze pipeline\n",
    "    raw_customers = pipeline_status.loc[pipeline_status['layer'] == 'RAW', 'customers'].values[0]\n",
    "    stg_customers = pipeline_status.loc[pipeline_status['layer'] == 'STAGING', 'customers'].values[0]\n",
    "    prod_customers = pipeline_status.loc[pipeline_status['layer'] == 'PROD', 'customers'].values[0]\n",
    "    \n",
    "    print(\"\\nüìä Pipeline Analysis:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    if raw_customers == 0:\n",
    "        print(\"‚ùå RAW layer is empty\")\n",
    "        print(\"üí° Solution: Generate raw data\")\n",
    "        print(\"  python scripts/generate_raw_data.py --test-mode\")\n",
    "    elif stg_customers == 0:\n",
    "        print(\"‚ùå STAGING layer is empty\")\n",
    "        print(\"üí° Solution: Run staging ETL\")\n",
    "        print(\"  make etl-run-stg\")\n",
    "    elif prod_customers == 0:\n",
    "        print(\"‚ùå PROD layer is empty\")\n",
    "        print(\"üí° Solution: Run production ETL\")\n",
    "        print(\"  make etl-run-prod\")\n",
    "    else:\n",
    "        print(\"‚úÖ All layers have data\")\n",
    "        \n",
    "        # Check data loss\n",
    "        loss_pct = (raw_customers - stg_customers) / raw_customers * 100\n",
    "        print(f\"\\nData loss RAW ‚Üí STAGING: {loss_pct:.1f}%\")\n",
    "        \n",
    "        if loss_pct > 20:\n",
    "            print(\"‚ö†Ô∏è High data loss detected!\")\n",
    "            print(\"üí° Check data quality issues in staging transformation\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PERFORMANCE ISSUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç DIAGNOSE: Performance Issues\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check table sizes\n",
    "size_query = \"\"\"\n",
    "SELECT \n",
    "    schemaname,\n",
    "    tablename,\n",
    "    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size\n",
    "FROM pg_tables\n",
    "WHERE schemaname IN ('raw', 'staging', 'prod')\n",
    "ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    sizes = db.read_sql(size_query)\n",
    "    print(\"\\nTable Sizes:\")\n",
    "    display(sizes)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check indexes\n",
    "index_query = \"\"\"\n",
    "SELECT \n",
    "    schemaname,\n",
    "    tablename,\n",
    "    indexname,\n",
    "    indexdef\n",
    "FROM pg_indexes\n",
    "WHERE schemaname IN ('raw', 'staging', 'prod')\n",
    "ORDER BY schemaname, tablename\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    indexes = db.read_sql(index_query)\n",
    "    print(\"\\nIndexes:\")\n",
    "    display(indexes)\n",
    "    \n",
    "    if len(indexes) == 0:\n",
    "        print(\"\\n‚ö†Ô∏è No indexes found!\")\n",
    "        print(\"üí° Consider adding indexes for better performance:\")\n",
    "        print(\"  CREATE INDEX idx_orders_customer ON staging.orders(customer_id);\")\n",
    "        print(\"  CREATE INDEX idx_orders_date ON staging.orders(order_date);\")\n",
    "        print(\"  CREATE INDEX idx_order_items_order ON staging.order_items(order_id);\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RECOVERY PROCEDURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Scenario 1: Reset Entire Pipeline\n",
    "\n",
    "**When to use:** Complete data corruption or major schema changes\n",
    "\n",
    "```bash\n",
    "# 1. Drop and recreate schemas\n",
    "make db-reset\n",
    "\n",
    "# 2. Initialize schemas\n",
    "make db-init\n",
    "\n",
    "# 3. Generate fresh data\n",
    "python scripts/generate_raw_data.py --test-mode\n",
    "\n",
    "# 4. Run full pipeline\n",
    "make etl-run-full\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick reset function (use with caution!)\n",
    "def reset_layer(layer_name):\n",
    "    \"\"\"\n",
    "    Reset a specific layer by truncating all tables\n",
    "    WARNING: This will delete all data in the layer!\n",
    "    \"\"\"\n",
    "    print(f\"‚ö†Ô∏è WARNING: This will delete all data in {layer_name} layer!\")\n",
    "    print(\"Uncomment the code below to execute.\")\n",
    "    \n",
    "    # Uncomment to execute\n",
    "    # db = DatabaseConnector()\n",
    "    # \n",
    "    # # Get all tables in layer\n",
    "    # tables_query = f\"\"\"\n",
    "    # SELECT table_name \n",
    "    # FROM information_schema.tables \n",
    "    # WHERE table_schema = '{layer_name}'\n",
    "    # \"\"\"\n",
    "    # tables = db.read_sql(tables_query)['table_name'].tolist()\n",
    "    # \n",
    "    # # Truncate each table\n",
    "    # for table in tables:\n",
    "    #     truncate_query = f\"TRUNCATE {layer_name}.{table} CASCADE;\"\n",
    "    #     db.execute(truncate_query)\n",
    "    #     print(f\"  ‚úÖ Truncated {layer_name}.{table}\")\n",
    "    # \n",
    "    # print(f\"\\n‚úÖ {layer_name} layer reset complete!\")\n",
    "\n",
    "# Example usage (commented out for safety)\n",
    "# reset_layer('staging')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Scenario 2: Reload Specific Date Range\n",
    "\n",
    "**When to use:** Data issues in specific date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_date_range(start_date, end_date):\n",
    "    \"\"\"\n",
    "    Reload data for specific date range\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Reloading data from {start_date} to {end_date}\")\n",
    "    print(\"\\nSteps:\")\n",
    "    print(f\"  1. Delete from RAW: DELETE FROM raw.customers WHERE _partition_date BETWEEN '{start_date}' AND '{end_date}';\")\n",
    "    print(f\"  2. Delete from STAGING: DELETE FROM staging.customers WHERE signup_date BETWEEN '{start_date}' AND '{end_date}';\")\n",
    "    print(f\"  3. Delete from PROD: DELETE FROM prod.daily_sales WHERE order_date BETWEEN '{start_date}' AND '{end_date}';\")\n",
    "    print(f\"  4. Re-generate raw data: python scripts/generate_raw_data.py --start-date {start_date} --end-date {end_date}\")\n",
    "    print(f\"  5. Re-run ETL: make etl-run-full\")\n",
    "\n",
    "# Example\n",
    "reload_date_range('2025-01-01', '2025-01-07')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Scenario 3: Fix Specific Table\n",
    "\n",
    "**When to use:** Issues in one specific table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_table(table_name):\n",
    "    \"\"\"\n",
    "    Fix specific table by reloading from previous layer\n",
    "    \"\"\"\n",
    "    print(f\"üîß Fixing {table_name}\")\n",
    "    print(\"\\nSteps:\")\n",
    "    \n",
    "    if table_name.startswith('staging.'):\n",
    "        print(f\"  1. Truncate: TRUNCATE {table_name} CASCADE;\")\n",
    "        print(f\"  2. Reload from RAW: make etl-run-stg\")\n",
    "    elif table_name.startswith('prod.'):\n",
    "        print(f\"  1. Truncate: TRUNCATE {table_name};\")\n",
    "        print(f\"  2. Rebuild from STAGING: make etl-run-prod\")\n",
    "    else:\n",
    "        print(f\"  1. Delete raw data files\")\n",
    "        print(f\"  2. Truncate: TRUNCATE {table_name};\")\n",
    "        print(f\"  3. Re-generate: python scripts/generate_raw_data.py\")\n",
    "        print(f\"  4. Re-ingest: make etl-run-raw\")\n",
    "\n",
    "# Example\n",
    "fix_table('staging.customers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. DIAGNOSTIC QUERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç USEFUL DIAGNOSTIC QUERIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "diagnostic_queries = {\n",
    "    \"Row counts by layer\": \"\"\"\n",
    "        SELECT 'RAW' as layer, COUNT(*) FROM raw.customers\n",
    "        UNION ALL\n",
    "        SELECT 'STAGING', COUNT(*) FROM staging.customers\n",
    "        UNION ALL\n",
    "        SELECT 'PROD', COUNT(*) FROM prod.customer_metrics\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Latest ingestion time\": \"\"\"\n",
    "        SELECT MAX(_ingested_at) as latest_ingestion\n",
    "        FROM raw.customers\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Date range in data\": \"\"\"\n",
    "        SELECT \n",
    "            MIN(order_date) as first_date,\n",
    "            MAX(order_date) as last_date,\n",
    "            COUNT(DISTINCT order_date) as total_days\n",
    "        FROM staging.orders\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Revenue by layer\": \"\"\"\n",
    "        SELECT \n",
    "            'STAGING' as layer,\n",
    "            SUM(total_amount) as total_revenue\n",
    "        FROM staging.orders\n",
    "        WHERE order_status = 'completed'\n",
    "        UNION ALL\n",
    "        SELECT \n",
    "            'PROD' as layer,\n",
    "            SUM(total_revenue) as total_revenue\n",
    "        FROM prod.daily_sales\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "for query_name, query in diagnostic_queries.items():\n",
    "    print(f\"\\nüìä {query_name}:\")\n",
    "    print(\"-\" * 70)\n",
    "    try:\n",
    "        result = db.read_sql(query)\n",
    "        display(result)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. HEALTH CHECK SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üè• SYSTEM HEALTH CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "health_checks = []\n",
    "\n",
    "# 1. Database connection\n",
    "try:\n",
    "    db.read_sql(\"SELECT 1\")\n",
    "    health_checks.append((\"Database Connection\", \"‚úÖ OK\"))\n",
    "except:\n",
    "    health_checks.append((\"Database Connection\", \"‚ùå FAILED\"))\n",
    "\n",
    "# 2. Schemas exist\n",
    "try:\n",
    "    schemas = db.read_sql(\"SELECT schema_name FROM information_schema.schemata WHERE schema_name IN ('raw', 'staging', 'prod')\")['schema_name'].tolist()\n",
    "    if len(schemas) == 3:\n",
    "        health_checks.append((\"Schemas\", \"‚úÖ OK (3/3)\"))\n",
    "    else:\n",
    "        health_checks.append((\"Schemas\", f\"‚ö†Ô∏è PARTIAL ({len(schemas)}/3)\"))\n",
    "except:\n",
    "    health_checks.append((\"Schemas\", \"‚ùå FAILED\"))\n",
    "\n",
    "# 3. Data in RAW\n",
    "try:\n",
    "    count = db.read_sql(\"SELECT COUNT(*) as c FROM raw.customers\")['c'][0]\n",
    "    if count > 0:\n",
    "        health_checks.append((\"RAW Layer\", f\"‚úÖ OK ({count:,} rows)\"))\n",
    "    else:\n",
    "        health_checks.append((\"RAW Layer\", \"‚ö†Ô∏è EMPTY\"))\n",
    "except:\n",
    "    health_checks.append((\"RAW Layer\", \"‚ùå FAILED\"))\n",
    "\n",
    "# 4. Data in STAGING\n",
    "try:\n",
    "    count = db.read_sql(\"SELECT COUNT(*) as c FROM staging.customers\")['c'][0]\n",
    "    if count > 0:\n",
    "        health_checks.append((\"STAGING Layer\", f\"‚úÖ OK ({count:,} rows)\"))\n",
    "    else:\n",
    "        health_checks.append((\"STAGING Layer\", \"‚ö†Ô∏è EMPTY\"))\n",
    "except:\n",
    "    health_checks.append((\"STAGING Layer\", \"‚ùå FAILED\"))\n",
    "\n",
    "# 5. Data in PROD\n",
    "try:\n",
    "    count = db.read_sql(\"SELECT COUNT(*) as c FROM prod.daily_sales\")['c'][0]\n",
    "    if count > 0:\n",
    "        health_checks.append((\"PROD Layer\", f\"‚úÖ OK ({count:,} rows)\"))\n",
    "    else:\n",
    "        health_checks.append((\"PROD Layer\", \"‚ö†Ô∏è EMPTY\"))\n",
    "except:\n",
    "    health_checks.append((\"PROD Layer\", \"‚ùå FAILED\"))\n",
    "\n",
    "# 6. Data quality\n",
    "try:\n",
    "    dups = db.read_sql(\"SELECT COUNT(*) - COUNT(DISTINCT email) as d FROM staging.customers\")['d'][0]\n",
    "    if dups == 0:\n",
    "        health_checks.append((\"Data Quality\", \"‚úÖ OK (no duplicates)\"))\n",
    "    else:\n",
    "        health_checks.append((\"Data Quality\", f\"‚ö†Ô∏è ISSUES ({dups} duplicates)\"))\n",
    "except:\n",
    "    health_checks.append((\"Data Quality\", \"‚ùå FAILED\"))\n",
    "\n",
    "# Print results\n",
    "print(\"\\nHealth Check Results:\")\n",
    "for check, status in health_checks:\n",
    "    print(f\"  {check:.<30} {status}\")\n",
    "\n",
    "# Overall status\n",
    "failed = sum(1 for _, status in health_checks if \"‚ùå\" in status)\n",
    "warnings = sum(1 for _, status in health_checks if \"‚ö†Ô∏è\" in status)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "if failed == 0 and warnings == 0:\n",
    "    print(\"üéâ ALL SYSTEMS OPERATIONAL\")\n",
    "elif failed == 0:\n",
    "    print(f\"‚ö†Ô∏è SYSTEM OPERATIONAL WITH {warnings} WARNING(S)\")\n",
    "else:\n",
    "    print(f\"‚ùå SYSTEM ISSUES DETECTED: {failed} FAILED, {warnings} WARNING(S)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì KEY TAKEAWAYS\n",
    "\n",
    "## üîß Common Issues:\n",
    "1. **Connection Issues**: Check PostgreSQL service, .env config\n",
    "2. **Missing Tables**: Run `make db-init`\n",
    "3. **Duplicate Keys**: Review deduplication logic\n",
    "4. **Data Quality**: Check transformation rules\n",
    "5. **Performance**: Add indexes, optimize queries\n",
    "\n",
    "## üîÑ Recovery Strategies:\n",
    "1. **Full Reset**: Drop and recreate everything\n",
    "2. **Layer Reset**: Truncate and reload specific layer\n",
    "3. **Date Range**: Reload specific dates\n",
    "4. **Table Fix**: Fix individual table\n",
    "\n",
    "## üìö Useful Commands:\n",
    "```bash\n",
    "# Database\n",
    "make db-init          # Initialize schemas\n",
    "make db-reset         # Reset database\n",
    "\n",
    "# ETL\n",
    "make etl-run-full     # Run full pipeline\n",
    "make etl-run-raw      # Run RAW layer only\n",
    "make etl-run-stg      # Run STAGING layer only\n",
    "make etl-run-prod     # Run PROD layer only\n",
    "\n",
    "# Data Generation\n",
    "python scripts/generate_raw_data.py --test-mode\n",
    "python scripts/generate_raw_data.py --start-date 2025-01-01 --end-date 2025-01-31\n",
    "```\n",
    "\n",
    "## üí° Best Practices:\n",
    "1. Always backup before major changes\n",
    "2. Test on small data first\n",
    "3. Monitor logs during ETL runs\n",
    "4. Validate data after each layer\n",
    "5. Document any manual fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚úÖ Troubleshooting Guide Complete!\")\n",
    "print(\"\\nüí° If you still have issues:\")\n",
    "print(\"  1. Check logs in logs/ directory\")\n",
    "print(\"  2. Review ETL scripts in scripts/ directory\")\n",
    "print(\"  3. Check SQL schemas in sql/ directory\")\n",
    "print(\"  4. Run health check above\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}